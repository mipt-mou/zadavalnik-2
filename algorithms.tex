\section{Вероятностные методы в Computer Science}
\label{CS}

%\subsection{Рандомизированные алгоритмы}

\begin{comment}
\begin{problem}

Пять философов сидят за круглым столом. В центре стола находится чаша со
спагетти. Между каждой парой соседних философов лежит вилка. Философы чередуют размышления с приемами пищи, не отвлекаясь на второстепенные занятия. Однако
для того, чтобы вытащить спагетти из чаши и донести их до рта философу требуются
две вилки. Каждый философ может взять вилку рядом с ним (если она доступна), или положить - если он уже держит её. Если требуемая вилка занята соседом, голодный философ вынужден ждать - он не может вернуться к размышлениям, не поев. После окончания еды философ кладет обе вилки на стол.

Время одного приема пищи одним философом равномерно распределено на отрезке [0, a]. 
Время одного размышления равномерно распределено на отрезке [0, b].

Данный процесс подвержен взаимной блокировке (Dead Lock): например, если каждый возьмет по левой вилке, то начнется вечное голодание. Для избежания блокировки каждый философ кладет первую вилку, если за время t после ее взятия вторая не освободилась.

Требуется определить распределение времени t, минимизирующее среднее время ожидания после размышления и перед приемом пищи.

\end{problem}
\end{comment}

\begin{problem}
\noindent Алгоритм быстрой сортировки основан на парадигме ``разделяй и властвуй''. Выбирается из элементов массива опорный элемент, относительно которого переупорядочиваются все остальные элементы. Желательно выбрать опорный элемент близким к значению медианы, чтобы он разбивал список на две примерно равные части. Переупорядочивание элементов относительно опорного происходит так, что все переставленные элементы, лежащие левее опорного, меньше его, а те, что правее -- больше или равны опорному. Далее процедура быстрой сортировки рекурсивно применяется к левому и правому списку для их упорядочивания по отдельности.

Наихудшие входные данные для описанного алгоритма быстрой сортировки (предполагается, что в качестве опорного элемента выбирается последний элемент обрабатываемого массива) -- элементы уже упорядоченные по возрастанию. 
Откуда следует, что асимптотика времени работы быстрой сортировки в худшем случае $\Theta (n^{2} )$.

Оценить время работы алгоритма быстрой сортировки в среднем. 


\begin{ordre}
Получить рекуррентное соотношение для математического ожидания времени работы, введя индикаторную функцию позиции опорного элемента. 
Воспользоваться соотношением:
\[\begin{array}{l} {\sum _{k=1}^{n-1}k\log k \le \log \frac{n}{2} \sum _{k=1}^{\left\lceil \frac{n}{2} \right\rceil -1}k +\log n\sum _{k=\left\lceil \frac{n}{2} \right\rceil }^{n-1}k =} \\ {=\frac{n(n-1)}{2} \log n-\frac{\left\lceil \frac{n}{2} \right\rceil \left(\left\lceil \frac{n}{2} \right\rceil -1\right)}{2} \le \frac{1}{2} n^{2} \log n-\frac{n^{2} }{8} } .\end{array}\]
 
\end{ordre}

Показать неулучшаемость оценки для произвольного алгоритма сортировки. Привести способ сортировки с асимптотикой $O(n \log n)$ в худшем случае.

\end{problem}


\begin{problem}[Задача поиска $k$-ой порядковой статистики]

Рекурсивное применение процедуры, основанной на методе быстрой сортировки, позволяет быстро (в среднем) находить $k$-ую порядковую статистику. Задача вычисления порядковых статистик состоит в следующем: дан список (массив) из $n$ чисел, необходимо найти значение, которое стоит в $k$-ой позиции в отсортированном в возрастающем порядке списке. 

Модифицируем алгоритм быстрой сортировки:


 Выбираем опорный элемент. Делим список на две группы. В первой --- элементы меньше опорного, во второй --- больше либо равны.
 Если размер (число элементов) первой группы больше либо равен $k$, то к ней снова применяется эта процедура. Иначе --- вызывается процедура для второй группы.
 
Покажите, используя ту же технику, что и при анализе в среднем алгоритма быстрой сортировки, что среднее время работы такого алгоритма линейно.

\begin{ordre}

\noindent Покажите, что выполняется оценка среднего времени работы алгоритма:

\[
\Exp [T(n)]\le \Exp\left\{\sum _{k=1}^{n}T\left(\max (k-1,n-k)\right)+O(n)\right\} 
\]
\[ \le \frac{2}{n} \sum _{k=\left\lfloor \frac{n}{2} \right\rfloor }^{n-1} \Exp[T(k)] + O(n).
\]



\end{ordre}
\end{problem}



\begin{problem}[Задача о рюкзаке] 

Рассмотрим  NP-трудную задачу
\[\sum _{j=1}^{n}x_{j}  \to \max , x_{j} \in \left\{0,\; 1\right\}, j=1,...,n;\] 
\[\sum _{j=1}^{n}a_{ij} x_{j}  \le 1, i=1,\ldots,m,                         (*)\] 
где $a_{ij} \in \left\{0,\; 1\right\}$, $i=1,\ldots,m$, $j=1,\ldots,n$.

Булев вектор $\vec{x}$ длины $n$ будем называть допустимыми, если он удовлетворяет системе (*). Обозначим через $T\left(j\right)$ множество всех допустимых булевых векторов для системы (*) с $(n-j)$ нулевыми последними компонентами и через $\vec{e}_{j} $ - вектор длины $n$ с единичной $j$--ой компонентой и с остальными нулевыми компонентами.

Рассмотрим алгоритм: 1) строим множество допустимых решений $T\left(j\right)$ на основе множества $T\left(j-1\right)$, пытаясь добавить вектор $\vec{e}_{j} $ ко всем булевым векторам $T\left(j-1\right)$; 2) среди $\left|T\left(n\right)\right|$ допустимых булевых векторов ищем ``наилучший''.
\begin{enumerate}
\item Покажите, что сложность описанного алгоритма составляет ${\rm O} \left(\left|T\left(n\right)mn\right|\right)$. При каких $a_{ij} \in \left\{0,\; 1\right\}$ алгоритм будет работать экспоненциально долго?

\item Оцените сложность в среднем (математическое ожидание времени работы алгоритма), т.е. ${\rm O} \left(\Exp\left(\left|T\left(n\right)\right|\right)mn\right)$, если с.в. $\left\{a_{ij} \right\}_{i,j=1}^{m,n} $ -- независимые и одинаково распределенные по закону Бернулли \\ $\mathrm{Be}\left(p\right)$ ($mp^{2} \ge \ln n$).
\end{enumerate}

\begin{ordre}
Пусть $k>0$. Положим: $\vec{x}_{j_{1} ,...,j_{k} } $ - вектор с $k$ единицами (на позициях $\left\{j_{1} ,...,j_{k} \right\}$) и $n-k$ нулями; $p_{ki} $ - вероятность выполнения $i$-го неравенства системы (*) для $\vec{x}_{j_{1} ,...,j_{k} } $; $P_{k} $ -- вероятность того, что $\vec{x}^{k} $ -- допустимое решение (покажите, что $p_{ki} $ и $P_{k} $ не зависят от набора $\left\{j_{1} ,...,j_{k} \right\}$). Докажите, что $p_{ki} \le \left(1-p^{2} \right)^{k-1} \le e^{-p^{2} \left(k-1\right)} $, $P_{k} \le e^{-mp^{2} \left(k-1\right)} $ и $\Exp\left(\left|T\left(n\right)\right|\right)=\sum _{k=0}^{n}C_{n}^{k}  P_{k} <1+n+n\sum _{k=2}^{n}e^{\left(k-1\right)\left(\ln n-mp^{2} \right)}  $.
\end{ordre}

\end{problem}

\begin{problem}

Даны три матрицы $A,B,C$ размера $n\times n$. Требуется проверить равенство $AB=C$.

Простой детерминированный алгоритм перемножает матрицы $A$, $B$ и сравнивает результат с $C$. Время работы такого алгоритма при использовании обычного перемножения матриц составляет $O(n^{3} )$, при использовании быстрого - $O(n^{2,376} )$. Вероятностный алгоритм Фрейвалда с односторонней ошибкой проверяет равенство за время $O(n^{2} )$.

Описание вероятностного алгоритма:

\begin{enumerate}
\item \textbf{ }взять случайный вектор $x\in \left\{0,1\right\}^{n}; $

\item  вычислить $y=Bx;$

\item  вычислить $z=Ay;$

\item  вычислить $t=Cx;$

\item  если $z=t$ вернуть «да», иначе «нет».
\end{enumerate}
Покажите, что для предъявленного алгоритма выполняется 
\[\begin{array}{l} {\PR\left\{z=t \vert AB=C\right\}=1,} \\ {\PR\left\{z \neq t \vert AB\ne C\right\}\ge 1/2.} \end{array}\] 

\begin{remark} (амплификация)
Оцените вероятность ошибочного ответа на одной ненулевой строке матрицы $D = AB - C$. Как можно добиться того, чтобы вероятность ошибочного ответа стала меньше 0.01?
\end{remark}

\end{problem}


\begin{problem}
\label{derandom}
Рассмотрим задачу из класса \textit{NP-трудных} задач -- \textit{максимальная выполнимость (MAX-SAT)}: даны $m$ скобок \textit{конъюнктивной нормальной формы }(КНФ) с $n$ переменными, нужно найти значения переменных, максимизирующее число выполненных скобок.

\begin{enumerate}

\item Для \textit{приближенного} решения задачи MAX-SAT воспользуемся простейшим вероятностным алгоритмом, выбирая значения каждой переменной (0 или 1) независимо и равновероятно. Покажите, что такой алгоритм гарантирует точность ${1\mathord{\left/ {\vphantom {1 2}} \right. \kern-\nulldelimiterspace} 2} $: для всех входов $I$

\[\frac{\Exp m_{A} (I)}{m_{0} (I)} \ge \frac{1}{2} ,\] 
где $m_{0} (I)$ - оптимум, $m_{A} (I)$ - случайное значение, найденное алгоритмом. 

\item Алгоритм с лучшими оценками точности строится на основе \textit{метода вероятностного округления}. Для начала переформулируем задачу MAX-SAT в терминах задачи целочисленного линейного программирования (ЦЛП). Каждой скобке $C_{j} $ поставим в соответствие булеву переменную $z_{j} \in \left\{0,1\right\}$, которая равна 1, если скобка $C_{j} $ выполнена; каждой входной переменной $x_{i} $ сопоставляем переменную $y_{i} $, которая равна 1, если $x_{i} =1$, и равна 0 в противном случае. Обозначим $C_{j}^{+} $ индексы переменных в скобке $C_{j} $, которые входят в нее без отрицания, а через $C_{j}^{-} $ - множество индексов переменных, которые входят в скобку с отрицанием. Тогда задача MAX-SAT эквивалентна следующей задаче ЦЛП:

\[\begin{array}{l} {\sum _{j=1}^{m}z_{j}  \to \mathop{\max }\limits_{z,y} } \\ {\sum _{i\in C_{j}^{+} }y_{i} +\sum _{i\in C_{j}^{+} }(1-y_{i} )  \ge z_{j} ,\quad j=1,...,m} \\ {y_{i} ,z_{j} \in \left\{0,1\right\},\quad i=1,...,n,j=1,...,m} \end{array}\] 

Рассмотрим и решим задачу \textit{линейной релаксации} целочисленной программы:

\[\begin{array}{l} {\sum _{j=1}^{m}\hat{z}_{j}  \to \mathop{\max }\limits_{\hat{y},\hat{z}} } \\ {\sum _{i\in C_{j}^{+} }\hat{y}_{i} +\sum _{i\in C_{j}^{+} }(1-\hat{y}_{i} )  \ge \hat{z}_{j} ,\quad j=1,...,m} \\ {\hat{y}_{i} ,\hat{z}_{j} \in \left\{0,1\right\},\quad i=1,...,n,j=1,...,m} \end{array}\] 

Пусть $\hat{y}_{i} ,\; \hat{z}_{j} $ - решения задачи линейной релаксации. Ясно, что $\sum _{j=1}^{m}\hat{z}_{j}  $ является верхней оценкой числа выполненных скобок для данной КНФ.

Рассмотрим вероятностный алгоритм решения задачи максимальной выполнимости, где каждая переменная $y_{i} $ независимо принимает значения 0 или 1 уже не с равными вероятностями, а с вероятностью $\hat{y}_{i} $ принимает значение 1 (и 0 с вероятностью $1-\hat{y}_{i} $). Такой метод называется \textit{вероятностным округлением}.

Докажите, что если в скобке $C_{j} $ имеется $k$ литералов, то вероятность того, что она выполнена при вероятностном округлении, не менее

\[\left(1-\left(1-\frac{1}{k} \right)^{k} \right)\hat{z}_{j} .\] 

\begin{remark}
Тогда из того, что

\[1-\left(1-\frac{1}{k} \right)^{k} \ge 1-\frac{1}{e} >0.63\] 
для всех положительных целых $k$, получаем, что для произвольной КНФ среднее число скобок, выполненное при вероятностном округлении, не меньше $1-\frac{1}{e} >0.63$ от максимально возможного числа выполненных скобок.
\end{remark}

\item Для получения детерминированного алгоритма приближенного решения задачи MAX-SAT воспользуемся один из способов\textit{ дерандомизации -- методом условных математических ожиданий. }Введем случайную величину\textit{ }$Z(x)$, где в булевом векторе $x=(x_{1} ,...,x_{n} )$ компоненты - суть значения переменных в КНФ, назначенных вероятностным алгоритмом - являются независимыми случайными величинами, причем $\PR\left\{x_{i} =1\right\}=p_{i} $, $\PR\left\{x_{i} =0\right\}=1-p_{i} $; $Z(x)$ - число невыполненных скобок. Требуется найти булев вектор $\hat{x}$, для которого выполнено неравенство $Z(\hat{x})\le \Exp Z$. Обозначим через $Z(x|x_{1} =d_{1} ,...,x_{k} =d_{k} )$ новую случайную величину, которая получена из $Z$ фиксированием значений первых $k$ булевых переменных.

Рассмотрим покомпонентную стратегию определения искомого вектора $\hat{x}$. Для определения его первой компоненты вычисляем значения $f_{0} =\Exp Z(x|x_{1} =0)$ и $f_{1} =\Exp Z(x|x_{1} =1)$. Если $f_{0} <f_{1} $ полагаем $x_{1} =0$, иначе полагаем $x_{1} =1$. При определенной таким образом первой компоненты (обозначим ее $d_{1} $) вычисляем значение функционала $f_{0} =\Exp Z(x|x_{1} =d_{1} ,x_{2} =0)$ и $f_{1} =\Exp Z(x|x_{1} =d_{1} ,x_{2} =1)$. Если $f_{0} <f_{1} $ полагаем $x_{2} =0$, иначе полагаем $x_{2} =1$. Фиксируем вторую координату (обозначая ее $d_{2} $) и продолжаем описанный процесс до тех пор, пока не определится последняя компонента решения.

Покажите, что найденный вектор $(x_{1} =d_{1} ,\ldots ,x_{n} =d_{n} )$ будет удовлетворять требованию минимизации оценки математического ожидания:

\begin{enumerate}
\item  для этого докажите неравенство $\Exp Z\ge \Exp Z(x|x_{1} =d_{1} )$;

\item  рекуррентно получите неравенство $\Exp Z\ge \Exp Z(x|x_{1} =d_{1} ,\ldots ,x_{n} =d_{n} )$;

\item  заметьте, что $\Exp Z(x|x_{1} =d_{1} ,\ldots ,x_{n} =d_{n} )=Z(x|x_{1} =d_{1} ,\ldots ,x_{n} =d_{n} )$.
\end{enumerate}

Покажите справедливость формул:
\begin{enumerate} 

\item $\Exp Z=\sum _{j=1}^{m}\PR_{j}$, где $\PR_{j} =\PR\left\{\sum _{i\in C_{j}^{+} }x_{i} +\sum _{i\in C_{j}^{-} }(1-x_{i} )  =0\right\};$
\item в предположении, что значения первых $k$ переменных уже определены и $I_{0} $ --- множество индексов тех переменных, значения которых равно 0, а $I_{1} $ --- множество индексов тех переменных, значения которых равно 1:
$$\Exp Z(x|x_{1} =d_{1} ,\ldots ,x_{k} =d_{k} )=\sum _{j=1}^{m}\PR_{j} ^{k},$$ 
\[ \text{где}\PR_{j} ^{k} = \PR\left\{\sum _{i\in C_{j}^{+} }x_{i} +\sum _{i\in C_{j}^{-} }(1-x_{i} )  =0|x_{1} =d_{1} ,\ldots,x_{k} =d_{k} \right\}=\] 
\[\left\{\begin{array}{cc} {0,} & { \text{при }  (I_{0} \cap C_{j}^{-} ) \cup ( I_{1} \cap C_{j}^{+} ) \ne \emptyset ;} \\ {\prod_{i\in C_{j}^{+} \backslash I_{0} }(1-p_{i} ) \prod _{i\in C_{j}^{-} \backslash I_{1} }p_{i}  ,} & \text{иначе} \end{array}\right. \] 
Здесь $C_{j}^{+} $ --- множество индексов переменных в скобке $C_{j} $, которые входят в нее без отрицания,  $C_{j}^{-} $ --- множество индексов переменных, которые входят в скобку с отрицанием.

\end{enumerate}

\end{enumerate}

\end{problem}

\begin{remark}
Детали см. в  Кузюрин Н.Н., Фомин~С.А. Эффективные алгоритмы и сложность вычислений: Учебное пособие. -- М.: МФТИ, 2007.
\end{remark}




%\begin{comment}

\begin{problem}[Коммуникационная сложность, хэширование]

Требуется сравнить ("достаточно достоверно") две битовые строки $a,b$,  осуществив как можно меньше по битовых сравнений. Основная идея -- сравнивать не сами строки, а функции от них. Так сравниваются $a\; \mod\; p$ и $b\; \mod\; p$, для некоторого простого числа $p$. Для этого требуется передать $2\log p$ бит информации.

Описание алгоритма сравнения строк:

\begin{enumerate}
\item  Пусть $\left|a\right|=\left|b\right|=n$, $N=n^{2} \log  n^{2}; $ 

\item  Выбираем случайное простое число $p$из интервала $\left[2..N\right]$ ;

\item  Выдать «да», если $a\; \mod\; p=b\; \mod\; p\Leftrightarrow (a-b)\equiv 0\; \mod\; p$, иначе выдать «нет».
\end{enumerate}

\noindent  Обоснуйте выбор именно простого числа на шаге 2 и предложите способ его генерации.  

\noindent Покажите что,

\[\begin{array}{l} {\PR\left\{(a-b)\equiv 0,\mod(p)|a=b\right\}=1,} \\ {\PR\left\{(a-b)\equiv 0,\mod(p)|a\ne b\right\}=O(1/n),} \end{array}\] 
 
При этом необходимое количество переданных бит равно $O\left(\log  n\right)$.

\begin{ordre}

Воспользоваться асимптотическим законом распределения простых чисел:
\[\mathop{\lim }\limits_{n\to \infty } \frac{\pi \left(n\right)}{n/\ln n}  =1,\] 
где $\pi \left(n\right)$ - функция распределения простых чисел, равная количеству простых чисел, не превосходящих $n$.

\end{ordre}

\end{problem}

\begin{remark}
В приведенной задаче требуется проверка простоты числа. Согласно малой теореме Ферма, если $N$ - простое число и целое $a$ не делится на $N$, то  
\[a^{N-1} \equiv 1\; \mod\; N.                         \; \; \;            \left(*\right)\] 

Отсюда следует, что если при каком-то $a$ сравнение $\left(*\right)$ нарушается, то можно утверждать, что $N$ - составное. 
К сожалению,  простой вариант подбора $a$ не всегда позволяет эффективно выявить составное число. Имеются составные числа $N$, обладающие свойством $\left(*\right)$ для любого целого $a$ с условием $\left(a,N\right)=1$ ($a$ и $N$ - взаимно простые). Такие числа называются числами Кармайкла.

В 1976 г. Миллер предложил заменить проверку $\left(*\right)$ проверкой несколько иного условия. Если $N$ - простое число, то $N-1=2^{s} t$, где $t$ нечетно, то согласно малой теоремы Ферма для каждого a с условием $\left(a,N\right)=1$ хотя бы одна из скобок в произведении 
\[\left(a^{t} -1\right)\left(a^{t} +1\right)\left(a^{2t} +1\right) \ldots  \left(a^{2^{s-1} t} +1\right)=a^{N-1} -1\] 
делится на $N$. 

Пусть $N$ - нечетное составное число, $N-1=2^{s} t$, где \textbf{$t$ }нечетно. Назовем целое число $a$, $1<a<N$ «выявляющим» для $N$, если нарушается одно из двух условий:

I) $N$ не делится на $a$

II) $a^{t} \equiv 1\; \mod\; N$ или существует целое $k$, $0\le k<s$ такое, что $a^{2^{k} t} \equiv -1\; \mod\; N$.

Если $N$ составное число, то согласно теореме Рабина  существует не менее $\frac{3}{4} \left(N-1\right)$  выявляющих чисел.

\end{remark}

\begin{comment}

\begin{problem}

Пусть $f(x_{1} ,...,x_{n} )=C_{1} \vee \cdots \vee C_{m} $ - булева формула в дизъюнктивной нормальной форме (ДНФ), где каждая скобка $C_{i} $ - есть конъюнкция $L_{1} \wedge \cdots \wedge L_{k_{i} } $ $k_{i} $ литералов (литерал есть либо переменная, либо ее отрицание). Набор значений переменных $a=(a_{1} ,...,a_{n} )$ называется выполняющим для $f$, если $f(a_{1} ,...,a_{n} )=1$. Требуется найти число выполняющих наборов для данной ДНФ.

\noindent $V$ - множество всех двоичных наборов длины $n$.

\noindent $G$ - множество выполняющих наборов.


\noindent  Проведем $N$ независимых испытаний:

\noindent Выбираем случайно $v_{i} \in V$ ( в соответствии с равномерным распределением).
\noindent $y_{i} =f(v_{i} )$. Заметим, что $P\left\{y_{i} =1\right\}=\frac{\left|G\right|}{\left|V\right|} =p$.
Рассмотрим сумму независимых случайных величин $Y=\sum _{i=1}^{N}y_{i}  $. В качестве аппроксимации $\left|G\right|$ возьмем величину $\frac{Y}{N} \left|V\right|$.

\noindent Оцените необходимое число испытаний $N$ как функцию от $|V|$, $|G|$ и точности аппроксимации $\varepsilon$. 

\begin{ordre}
Докажите следующее утверждение. Пусть $X_{1} ,...,X_{n} $ - независимые случайные величины, принимающие значения 0 или 1, при этом $P\left\{X_{i} =1\right\}=p,\quad P\left\{X_{i} =0\right\}=1-p$. Тогда для $X=\sum _{i=1}^{N}X_{i}  $ и для любого $0<\delta <1$, выполнены неравенства
\[\begin{array}{l} {P\left\{X>(1+\delta )EX\right\}\le e^{-\frac{\delta ^{2} }{3} EX} } \\ {P\left\{X<(1-\delta )EX\right\}\le e^{-\frac{\delta ^{2} }{2} EX} } \end{array}\] 
\end{ordre}

\end{problem}


\begin{problem}

(Задача о покрытии). Дано конечное множество из m элементов и система его подмножеств $S_{1} ,...,S_{n} $. Требуется найти минимальную по числу подмножеств подсистему $S_{1} ,...,S_{n} $, покрывающую все множество объектов. 

\noindent Сформулируем ее в терминах булевых матриц и целочисленного линейного программирования:
\[\left\{\begin{array}{l} {cx\to \min ,} \\ {Ax\ge b,} \\ {\forall j\; x_{j} \in \{ 0,1\} .} \end{array}\right. \] 
Здесь переменные $x_{1} ,...,x_{n} $ соответствуют включению подмножеств $S_{1} ,...,S_{n} $ в решение-покрытие, матрица $A$ - матрица инцидентности, $c=(1...1)^{T} \in {\mathbb R}^{n} ,\quad b=(1...1)^{T} \in {\mathbb R}^{m} $ - векторы стоимости и ограничений.

\noindent Пусть элементы матрицы инцидентности -- независимые случайные величины с бернулевским распределением:$P\{ a_{ij} =1\} =p,$ $P\{ a_{ij} =0\} =1-p$. 

\noindent Для решения задачи применяется жадный алгоритм: на каждом шаге выбирается подмножество, максимально покрывающее еще не покрытые объекты. 

Доказать следующее утверждение. Пусть для случайной матрицы $A$, определенной выше, выполнены соотношения:
\[\begin{array}{l} {\forall \gamma >0:} \\ {\frac{\ln n}{m^{\gamma } } \mathop{\to }\limits_{n\to \infty } 0,} \\ {\frac{\ln m}{n} \mathop{\to }\limits_{n\to \infty } 0.} \end{array}\] 
Тогда для $\forall \varepsilon >0:$ $P\left\{\frac{Z}{M} \le 1+\varepsilon \right\}\mathop{\to }\limits_{n\to \infty } 1$, где Z -- решение жадного алгоритма, M -- величина минимального покрытия. 

\begin{fixme}
Добавить указание.
\end{fixme}

\end{problem}

\end{comment}

\begin{comment}
\begin{problem}
А) Пусть имеется генератор случайных чисел, в 
результате обращения к которому появляется 0 или 1 с одинаковой вероятностью 
равной 1/2 (аналог подбрасывания симметричной монеты). Пусть задано 
вещественное число $0\le p\le 1$. С помощью имеющегося генератора определить 
генератор randp, в результате обращения к которому появляется 0 или 1 с 
вероятностями $p$ и $1-p$ соответственно (незначительные отклонения 
допустимы). Оцените сложность в среднем алгоритма получения одного 
случайного числа с помощью randp (затраты определяются числом обращений $к$ 
изначально имеющемуся генератору). 

Б) Пусть имеется генератор 
случайных чисел randp (описанный выше). Известно, что $p\ne 0,\quad p\ne 1$. 
Как с помощью него сконструировать генератор, в результате обращения 
которому появляется 0 или 1 с одинаковой вероятностью 1/2. 

В) Чему равно математическое ожидание числа обращений к изначально имеющемуся 
генератору случайных чисел при построении последовательности пар до 
появления 0,1 или 1,0? Найти сложность в среднем алгоритма получения k 
``равновероятных'' нулей и единиц с помощью сконструированного генератора 
(затраты определяются количеством обращений к изначально имеющемуся 
генератору). Можно ли указать значения $p$, для которых эта сложность имеет 
минимальное и, соответственно, максимальное значение?
\end{problem}
\end{comment}

\begin{problem}[Интерактивные доказательства] 

а) (изоморфизм графов). Авдотье известен изоморфизм $\phi$ графов $G_0$ и $G_1$. Но она посылает Евлампию граф $H =\psi(G_0)$, либо $H =\psi(G_1)$, где $\psi$ -- некоторый другой изоморфизм, не равный $\phi$. Евлампий бросает симметричную монетку и просит изоморфизм либо $H : G_0$, либо $H : G_1$. В первом случае Авдотья  посылает  $\psi$, во втором -- $\psi \phi^{-1}$. Таких партий разыгрывается $N$ штук. Заметим, что в каждой новой партии Авдотья придумывает новую перестановку $\psi$   вершин графа $G_0$. Если $\phi$  -- действительно изоморфизм $G_0 : G_1$, то все проверки Евлампия будут положительны.
Покажите, что если $\phi$ -- блеф, то с вероятностью  $1 - 2^{-N}$  хотя бы одна проверка обнаружит это (та проверка, в которой Евлампий попросил $H : G_1$).

\begin{remark}
Этот пример поучителен с точки зрения ``криптографического фокуса'' -- Авдотья  убедила Евлампия в $G_0 : G_1$ так и не огласив самого изоморфизма $\phi$. Если $\phi$ –- пароль, то диалог можно вести даже в открытую, что служит примером криптосистемы с нулевым разглашением.
\end{remark}

Обоснуйте справедливость данного замечания (Евлампий не получает никакой информации об изоморфизме $\phi$).

б) (неизоморфизм графов). Теперь наделим Авдотью сверхъестественными вычислительными способностями. Для удобства переименуем игроков: ``Prover'' и ``Verifier''. Verifier выбирает случайно (равновероятно) $ i \in \lbrace 0, 1 \rbrace$ и некоторую перестановку $\pi$ вершин графа $G_i$, затем посылает граф $H =\psi(G_i)$ и требует, чтобы Prover определил $i$. Таких партий разыгрывается $N$ штук. Аналогично предыдущему примеру, $\pi$ в каждой новой партии свое. Если графы неизоморфны, то Prover всегда верно определит индекс $i$. Все тесты будут пройдены. Покажите, что иначе с вероятностью $1-2^{-N}$  Prover ошибется хотя бы один раз (хотя бы в одной партии).
 
 \begin{remark}
Заинтересовавшимся в этой теме, можно также порекомендовать посмотреть про цифровую подпись (протокол аутентификации) и электронную систему голосвания в книге Введение в криптографию под ред. В.В. Ященко. М.: МЦНМО, 2013.
\end{remark}
\end{problem}

\begin{problem}[Хеш функции]
\label{hesh_func}
Для компактного хранения данных, индексированных, как правило, ключами целочисленного или строкового форматов, используются хеш-функции, при помощи которых вычисляются  номера ячеек в таблице в зависимости от значения ключа. Пусть хэш-функция задана на множестве ключей $K$ 
\[
h: K \to \{1,\ldots,m\}.
\]  
Заведем таблицу размера $m$, в которой элемент с ключом $k$ будет размещаться по адресу $h(k)$ (во многих случаях $m \ll |K|$). Ключи с одинаковым значением $h(\cdot)$ образуют список, начинающийся с ячейки таблицы. Если всего имеется $n$ элементов, то желательно чтобы количество ключей с одинаковым хеш-значением не  превышало значительно $\alpha = n/m$ (в таком случае длины списков будут ограничены $O(1+\alpha)$). Формально данное свойство может быть сформулировано в виде двух требований:
\begin{enumerate}
\item $h(k)$ является с.в. с равномерным распределением;
\item $\forall k_1, k_2 \in K$ $h(k_1)$ и  $h(k_2)$ независимы.  
\end{enumerate}    
Покажите, что в случае выполнения требований среднее время поиска отсутствующего ключа в таблице составляет $(1+ \alpha)$, а также среднее время поиска присутствующего ключа в таблице составляет $(1+ \alpha/2)$ (где усреднение ведется как по $h(k)$, так и по множеству других присутствующих в таблице ключей). 
\end{problem}

\begin{remark}
Так как хеш-функция является детерминированной, то случайность, присутствующая в требованиях, может быть реализована посредством выбора функции $h$ из некоторого семейства $H$. $H$ называется \textit{универсальным} для множества $K$, если $\forall k_1, k_2 \in K$ вероятность выбрать функцию из $H$, в которой возникает коллизия, не превышает $1/m$. Для универсального семейства справедливы приведенные в задаче оценки времени поиска ключа в таблице. 
Примером универсального семейства $H$ для целых чисел может служить
\[
\{h_{ab}: h_{ab}(k) = ((ak +b) \; \text{mod} \; p) \; \text{mod} \;   m, \; a,b \in \{1,\ldots, p-1\}\},
\]    
где $p$ -- простое число, большее $m$.

Другой пример универсального семейства, используемого при хешировании IP адресов вида $(x_1,x_2,x_3,x_4)$, $x_i \in \{0,\ldots,255\}$ представляет собой 
множество хеш-функций
\[
\{
h_a(x_1,x_2,x_3,x_4) = (a_1 x_1 + a_2 x_2 + a_3 x_3 + a_4 x_4) \; \text{mod} \; p
\}.
\]
где $p$ -- также простое число, большее $m$.
\end{remark}

\begin{problem}[Хеширование без коллизий]
При известном заранее множестве ключей $K, |K| = n$ хеширование без коллизий может быть реализовано при помощи двухуровневой схемы. На первом уровне используется таблица размера $m = n$, для которой хеш функция выбирается случайно из универсального семейства (см. замечание к задаче \ref{hesh_func}). На втором уровне осуществляется хеширование без коллизий (вместо создания списка ключей будем использeтся вторичная хеш-таблица $T_i$, хранящая все ключи, хешированные функцией $h$ в ячейку $i$, со своей функцией $h_i$).  Покажите, что при выборе  на втором уровне таблицы размером $n_i^2$ ($n_i$ -- количество ключей в $i$-й ячейке первого уровня) вероятность возникновения коллизий менее $1/2$ (поэтому для данной ячейки всегда найдется функция $h_i$ свободная от коллизий). Докажите, что существует хеш функция $h$ для первого уровня, при которой затраты памяти составят суммарно $O(n)$.    
\end{problem}

\begin{ordre}
Получите оценку $\Exp \left(\sum_i n_i^2\right) < 2n$, воспользуйтесь неравенством Маркова.
\end{ordre}


\begin{problem}[Bloom filter]
Вероятностная структура данных Bloom filter используется для проверки принадлежности элемента множеству $S$. Она представляет собой массив $A$ длиной $m$ бит и $k$ различных хэш-функций $h_1, \ldots, h_k$, равновероятно отображающих элементы множества  $D \supseteq S$ в позиции массива $A$ ($h_i: S \to \{1, \ldots, m\}$). 
Интерфейс данной структуры включает следующие операции:

\verb|add_element(s)|: устанавливает биты $A[h_1(s)], \ldots, A[h_k(s)]$ равными 1 (изначально все биты равны 0);

\verb|contains(s)|: если биты $A[h_1(s)], \ldots, A[h_k(s)]$ равны 1, то возвращает \verb|true|, иначе \verb|false|;

Последовательно добавив все элементы множества $S$ при помощи операции \verb|add_element|, получим объект структуры данных, который при вызове метода \verb|contains(s)| всегда возвращает \verb|true| при $s \in S$, но не всегда возвращает \verb|false| при  $s \notin S$, так как соответствующие биты могли быть установлены равными 1 за счет других элементов из $S$.     

\verb|approx_size|: возвращает приближенное количество добавленных элементов
 \[
 |S| \approx  - m \frac{\ln (1 - \mathbb{I}/m)}{k},
 \] 
где $\mathbb{I}$ -- сумма элементов $A$. 

\imgh{80mm}{bf.png}{Пример структуры данных Bloom filter ($k = 3$, $m=18$): x, y, z -- принадлежат множеству, w -- не принадлежит.}


Считая события $A[i] = 1$ и $A[j] = 1$ при добавлении $n$ элементов в пустой объект структуры Bloom filter  независимыми, ввиду независимости значений хэш-функций, покажите, что:
\[
\PR(\text{contains(s)} = \text{true} \; | \; s \text{ not in } S) \approx p =  \left( 1 - e^{k n / m} \right)^k.
\]   
Откуда следует, что оптимальные значения $m$ и $k$ могут быть вычислены по формулам:
\[
k = \frac{m}{n} \ln 2, \quad m = -n \frac{\ln p}{(\ln 2)^2}.
\]
\end{problem}

\begin{remark}
Bloom filter  обладает следующими отличительными характеристиками:
\begin{enumerate}
\item При вероятности ошибки $p = 0.01$ требуется всего лишь 9.6 бит на один элемент множества.
\item  Сложность операций \verb|contains| и \verb|add_element| составляет $O(k)$, вне зависимости от $|S|$. Стоит также учесть, что $k$ операций над элементами массива могут быть выполнены параллельно. 
\item Операции объединения и пересечения двух множеств выполняются побитно (за сравнительно короткое время): 
\[
 |S_1 \cup S_2| \approx  - m \frac{\ln (1 - \mathbb{I}_{12}/m)}{k},
 \]   
 \[
 |S_1 \cap S_2| = |S_1| + |S_2| -   |S_1 \cup S_2|,
 \]
где $\mathbb{I}_{12}$ -- скалярное произведение  $A_1 $ и $A_2$. 

\item Сборщик мусора, ровно как и сериализатор могут осуществлять сравнительно быстрые операции чтения/записи/удаления структуры данных Bloom filter, т. к.  $A$ представляет собой единичный объект, состоящий из элементарных типов данных.
\end{enumerate}

Области применения: снижение количества обращений к базе данных, хранящейся на диске в случае отсутствия запрашиваемых данных (Apache Cassandra);  локальная проверка url-адресов на принадлежность списку, хранящемуся на удаленном сервере (Google Chrome); 
выявление содержимого архива (Venti archival storage system);  
\end{remark}

\begin{problem}[Jaccard similarity]
Для измерения близости (веса связи) двух множеств часто используется коэффициент Жаккара:
\[
J(S_1, S_2) = \frac{|S_1 \cap S_2|}{|S_1 \cup S_2|}.
\]
Типичными представителями таких множеств являются списки смежных вершин в графе, изображения, профили пользователей социальных сетей и прочие web-страницы. Вычисление коэффициента Жаккара позволяет выявлять дубликаты, проводить кластеризацию, восстанавливать фрагменты перечисленных выше объектов. 

Для получения попарной близости $S_1, \ldots, S_N$   множеств больших размеров ($|S_i| = s > 500$, $i \in \overline{1,N}$) возникает потребность в приближенном вычислении коэффициента Жаккара, снижающем нагрузку на сеть вычислительного кластера и временную сложность вычислений. Для этой цели достаточно для каждого множества $|S_i|$ найти минимумы $k$ хэш-функций и далее при попарном сравнении оперировать только с набором подмножеств из минимумов. В итоге, временная сложность окажется равной $O(k N^2 + k s N \log N)$, в то время как в исходной задаче сравнения без аппроксимации меры $O(s \log s  N^2)$.

Рассмотрим два варианта приближенного вычисления, базирующиеся на следующем свойстве: пусть $h$ -- хэш-функция, иньективно и равновероятно отображающая элементы множества $\cup_i S_i$ в $\mathbb{N}$, тогда 
 \[
\PR( \min \limits_{s \in S_1} h(s) = \min \limits_{s \in S_2} h(s)) = J(S_1, S_2). 
 \]         
\begin{enumerate}
\item[\textit{1 вар.}] Задействуем $k$ различных хэш-функций. В этом случае оценкой $J$ будет являться доля функций, у которых совпадают минимальные значения на обоих множествах.    
\item[\textit{2 вар.}] Обозначим за $h_{(k)}(S)$ подмножество $S$  из $k$  элементов с наименьшими значениями $h$. Тогда  $J$ можно оценить как $h_{(k)}(S_1) \cap h_{(k)}(S_2) \cap h_{(k)}(S_1 \cup S_2)$.
\end{enumerate}

Сравните временные сложности двух вариантов. Используя метод Чернова  (см. задачу \ref{chernov_th} из раздела \ref{measure}), покажите, что порядок ошибки аппроксимации в обоих вариантах $O(1/ \sqrt{k})$.

\end{problem}


\begin{problem}[EM-алгоритм]
\label{em}

Рассмотрим задачу поиска неизвестных параметров распределения при помощи метода максимального правдоподобия. В ряде случаев, где функция правдоподобия имеет вид, не допускающий удобных аналитических методов исследования, для ее упрощения удобно ввести дополнительные ``скрытые'' (латентные) переменные и воспользоваться EM-алгоритмом.
Пусть требуется найти максимум правдоподобия $L(X,\theta) = \log p(X | \theta)$ в вероятностной модели со скрытыми переменными $Z$
\[
p(X | \theta) = \int p(X, Z | \theta) dZ.
\]
Покажите, что $L(X,\theta)$ можно представить как
\[
L(X,\theta) = \int \log \frac{p(X, Z | \theta)}{q(Z)} q(Z) dZ - \int \log \frac{p(Z | X, \theta)}{q(Z)} q(Z) dZ = 
\]
\[
 = l(X, \theta, q) + \mathcal{KL}(q \Vert p(Z | X, \theta)),
\]
где $q(Z)$ -- произвольное распределение над скрытыми переменными.

Итерационная схема EM-флгоритма состоит в фиксации на шаге $t$ некоторого значения $\theta^{(t)}$ и аппроксимации в этой точке правдоподобия с помощью его нижней оценки $l(\cdot)$:
\[
q(Z) = p(Z | X, \theta^{(t)}), \quad l(X, \theta, q) \to \max_{\theta}.
\]
Expectation step: фиксируется значение параметров $\theta^{(t)}$. Оценивается распределение на скрытые переменные
\[
q(Z) = p(Z |X, \theta^{(t)}) = \frac{p(X, Z | \theta^{(t)})} {p(X|\theta^{(t)})};
\]
Maximization step: фиксируется распределение $p(Z|X, \theta)$ и выполняется поиск новых параметров
\[
\theta^{(t+1)} = \arg\max_{\theta} \Exp_q \log p(X,Z|\theta).
\]
Покажите, что 
\[
L(X,\theta^{(t+1)}) \geq L(X,\theta^{(t)}),
\]
а также при гладкости $L$
\[
\frac{\partial L}{\partial \theta} \bigg|_{\theta^{(\infty)}} = 0.
\]
\end{problem}

\begin{ordre}
Воспользуйтесь конструкцией
\[
\theta^{(t+1)} = \arg \max \limits_{\theta} \{ Q(\theta)  - b_t \mathcal{R} (\theta, \theta^{(t)})\},
\]
где $\mathcal{R} (\theta, \theta^{(t)}) \geq 0$ -- регуляризатор, в нашем случае равный $\mathcal{KL}(\cdot\Vert\cdot) $.
\end{ordre}


\begin{problem}[Pазделение смеси распределений]


%Рассмотрим задачу поиска неизвестных параметров распределения при помощи метода максимального правдоподобия. В ряде случаев, где функция правдоподобия имеет вид, не допускающий удобных аналитических методов исследования, но допускающий серьезные упрощения, если в задачу ввести дополнительные ``скрытые'' (латентные) переменные, удобно воспользоваться EM-алгоритмом.

%В общем случае ЕМ-алгоритм может быть представлен в следующем виде. Пусть $X$ -- наблюдаемые переменные (выборка), $Z$ -- скрытые переменные с вариационным распределением $q$, $\theta$ -- искомые параметры,  $L(\theta, X, Z) = p(X,Z | \theta)$, $L(\theta, X) = \int p(X,z | \theta) p(z | \theta) dz$,
%\[
%\mathcal{F}(q, \theta) = \Exp_q[\log L(\theta, X, Z)] + H_q(q)  = L
%(\theta, X) - \mathcal{KL}(q \Vert p(Z | X, \theta)) 
%\]

%\begin{enumerate}
%\item[1)] Expectation step:
%\[
%q^{(t)} = \arg \max \limits_q \mathcal{F}(q, \theta^{(t)})
%\]
%\item[2)] Maximization step:
%\[
%\theta^{(t+1)} = \arg \max \limits_{\theta} \mathcal{F}(q^{(t)}, \theta)
%\]

%\end{enumerate}

Целью разделения смеси является как восстановление плотности наблюдаемых данных $X$, так и покомпонентная их категоризация (каждый элемент выборки  $X$ принадлежит одному из распределений, входящих в осотав смеси).
Допустим, что плотность распределения в $z$-й компоненте смеси равна $p (x | z) = p (x | z, \alpha_z)$, т.е. известна с точностью до параметра $\alpha_z$. Тогда плотность $x \in X$ можно аппроксимировать смесью 
\[
p_{\theta}(x) = \sum \limits_{z = 1}^K w_i \; p (x | z),
\]
где $\theta = (w_1, \ldots, w_K, z_1, \ldots, z_K)$, $w_i \geq 0$, $\sum_i w_i = 1$, $K$ -- количество компонент смеси. Правдоподобие $X$ задается формулой 
\[
\log L(\theta, X) = \sum \limits_{j=1}^{n} \left(\log  \sum \limits_{z = 1}^K w_i p (x_j | z) \right). 
\]
Непосредственный поиск точки максимума данной функции весьма затруднителен. Для упрощения вычислений применим EM-алгоритм (см. задачу \ref{em}),  сопоставив каждому $x_j$  скрытую переменную $z_j$ -- номер компоненты смеси, породившей  $x_j$, т. е.  $x_j \sim p(x | z_j)$. 
Проверьте, что при такой модификации данных, функция правдоподобия примет вид

\[
\log L(\theta, X, Z) = \sum \limits_{j=1}^{n} \log w_{z_j} + \sum \limits_{j=1}^{n} \log  p (x_j | z_j). 
\]
Покажите, что шагами  EM-алгоритма в этом случае будут 

\begin{enumerate}
\item[1)] Expectation step:
\[
q_{j}^{(t)}(z) \propto w_{z}^{(t)} \; p (x_j | z, \alpha_z^{(t)});
\]
\item[2)] Maximization step:
\[
w_z^{(t+1)} = \frac{1}{n}   \sum \limits_{j=1}^{n} q_{j}^{(t)}(z),
\]
\[
\alpha_z^{(t+1)} = \arg \max \limits_{\alpha}  \sum \limits_{j = 1}^n q_{j}^{(t)}(z) \; p (x_j | z, \alpha). 
\]
\end{enumerate}

\end{problem}

\begin{comment}
Приведем два важных обобщения EM-алгоритма:
\begin{enumerate}
\item PP-алгоритм (см. Васильев, Методы оптимизации – М.: Факториал Пресс, 2002): 
\[
Q(\theta) \to \max
\]
\[
\theta^{(t+1)} = \arg \max \limits_{\theta} \{ Q(\theta)  - b_t \mathcal{R} (\theta, \theta^{(t)})\},
\]
где $\mathcal{R} (\theta, \theta^{(t)}) \geq 0$ -- регуляризатор, в нашем случае равный $\mathcal{KL}(q \Vert p(Z | X, \theta)) $.

\item Variational EM: 
\[
\mathcal{F}(q, \theta)  = \mathcal{F}(q_z(z) \cdot q_\theta(\theta)) = \log p(X) - \mathcal{KL}(q_z  q_\theta \Vert p(Z, \theta | X)). 
\] 

\begin{enumerate}
\item[1)] Expectation step:
\[
q_z^{(t)}(z)  \propto \mathrm{exp} \left\{ \int \log p(X, z, \theta) q_\theta^{(t)}(\theta) d \theta \right\}
\]

В случае $p(X, Z | \theta) = h(X, Z) g(\theta) e^{\phi(\theta)^T u(X, z)}$
\[
q_z^{(t)}(z)  \propto h(X, Z) e^{\phi^T u(X, z)} , \quad \phi^{(t)} = \int  \phi(\theta) \; q_\theta^{(t)}(\theta) d \theta. 
\]
\item[2)] Maximization step:
\[
q_\theta^{(t+1)}(\theta)  \propto \mathrm{exp} \left\{ \int \log p(X, z, \theta) q_z^{(t)}(z) dz \right\}.
\]

\end{enumerate}

\end{enumerate}
\end{comment}

\begin{remark}

Для более глубокого ознакомления с модификациями EM алгоритма -- медианные модификации, SEM, CEM, MCEM, SAEM, выбор начального приближения, определение числа компонент и типа смеси -- рекомендуем ознакомится с работой  В.Ю. Королева ЕМ-алгоритм, его модификации и их применение к задаче разделения смесей вероятностных распределений.  

\end{remark}


\begin{problem}[Вариационный вывод]
\label{varinf}

Рассмотрим  задачу оценки некоторой статистики $T(X)$ для распределения с плотностью $p(X)$, т.е. величины 
\[
\Exp_p T(X) = \int  T(X)p(X)dX.
\] 
Предполагается, что  $p(X)$  известно с точностью до нормировочной константы ($S_p =  \int \tilde{p}(X)dX$ является недоступным):
\[
p ( X ) = \frac{1}{S_p} \tilde{p}( X ) \propto \tilde{p}( X ). 
\]
Например, для получения оценки скрытых параметров $Z$ распределения $p(X,Z)$ можно подсчитать  $\Exp_{p(Z|X)} Z$. В этом случае в качестве недоступной для вычисления нормировочной константы выступает $p(X)$. В задачах, решаемых при помощи ЕМ-алгоритма (см. задачу \ref{em}), в качестве ненормированного распределения $\tilde{p}$ выступает совместное распределение $p(X, Z |\theta)$, недоступной нормировочной константой является неполное правдоподобие $p(X |\theta)$, а искомой статистикой $T(X,Z)$ -- величина $\log p(X, Z | \theta)$.

Одним из вариантов решения задач такого рода является нахождение приближения $q(x)$ в некотором простом семействе распределений и последующая оценка статистики как $\Exp_p T(X) \approx \Exp_q T(X)$. Для нахождения $q(X)$ решается следующая оптимизационная задача  
\[
\mathcal{KL}(q \Vert p) \to \min_q
\Leftrightarrow
\int q(X) \log \frac{\tilde{p}(X)}{q(X)}dX \to \max_q.
\]
Получите для семейства полностью факторизованных распределений $q(X) = \prod_i q_i(x_i)$, $X = (x_1,\ldots,x_n)$ итерационный метод вычисления оптимальных $q_i$: 
\[
q_i(x_i) \propto \exp \left(\int \log \big[ \tilde{p}(X) \big] \prod_{j \neq i} q_j(x_j) dx_j \right), 
\quad i = \overline{1,n}.
\]
Рассмотрите случай экспоненциального распределения 
\[
p(x_i |X_{-i})=h(x_i) e^{ \langle \theta, f(x_i) \rangle - d(\theta) },\quad
\theta = \theta(X_{-i}).
\]
Докажите, что для этого случая справедливо упрощенное  соотношение для  $q_i$:
\[
q_i(x_i) = h(x_i) e^{ \langle \Exp\theta , f(x_i) \rangle - d(\Exp\theta) } , 
\quad i = \overline{1,n}.
\] 

\end{problem}

\begin{remark}
Сравним между собой два метода приближенной оценки статистик $T(X)$: вариационный вывод и методы MCMC. В методе MCMC оценка $\Exp_p T(X)$ является тем точнее, чем больше выборок $X$ генерируется, а в пределе является точной. В вариационном выводе нет никаких гарантий на близость между $\Exp_p T(X)$ и $\Exp_q T(X)$. В итерациях вариационного вывода происходит максимизация функционала $L(q)$, являющегося нижней оценкой для  $\log S_p$, что, как правило,  обеспечивает достаточно точную оценку на значение нормировочной константы даже при существенно ограниченном семействе распределений~$q$. Время работы одной итерации вариационного вывода и одной итерации схемы MCMC, обычно, очень близки. Однако, для сходимости вариационного вывода часто достаточно несколько десятков итераций, в то время как для надежной оценки статистик MCMC требует несколько тысяч итераций.
\end{remark}





Для того чтобы описать следующий цикл задач на метод \textit{зеркального спуска} 
А.С. Немировского, нам потребуется ввести ряд определений и сформулировать 
некоторые необходимые в дальнейшем результаты (см. раздел Вспомогательные материалы). Отметим, что одну задачу, 
которая может быть отнесена к задачам этого цикла мы уже встречали (см. 
задачу \ref{bandit} раздела \ref{information}).  Далее при изложении мы будем опираться в основном на работы

Lugosi G., Cesa-Bianchi N. Prediction, learning and games. -- New York: Cambridge University Press, 2006.

Вьюгин В.В. Математические основы теории машинного обучения и прогнозирования. -- М.: МЦНМО, 2013.

%\underline {http://www.iitp.ru/upload/publications/6256/vyugin1.pdf}

Гасников А.В., Нестеров Ю.Е., Спокойный В.Г. Об эффективности одного метода рандомизации зеркального спуска в задачах онлайн оптимизации. -- М.: Автоматика и телемеханика. 2014.



\begin{problem}
Рассмотрим задачу взвешивание экспертных решений. Имеется $n$ различных 
Экспертов. Каждый Эксперт играет на рынке. Игра повторяется $N\gg 1$ раз 
(это число может быть заранее неизвестно). Пусть $l_i^k $ -- проигрыш 
Эксперта $i$ на шаге $k$ ($\left| {l_i^k } \right|\le M)$. На каждом шаге 
$k$ мы распределяем один доллар между Экспертами, согласно вектору $x^k\in 
S_n \left( 1 \right)$. Потери, которые мы при этом несем, рассчитываются по 
потерям экспертов $\left\langle {l^k,x^k} \right\rangle $. Целью является 
таким образом организовать процедуру распределения доллара на каждом шаге, 
чтобы наши суммарные потери были бы минимальны. Допускается, что потери 
экспертов $l^k$ могут зависеть еще и от текущего хода $x^k$. Легко 
проверить, что для данной постановки применима теорема 1 в детерминированном 
варианте с функциями
\[
f_k \left( {x;\xi ^k} \right)\equiv f_k \left( x \right)=\left\langle 
{l^k,x} \right\rangle .
\]
Покажите, что оценка, даваемая теоремой 1, имеет вид
\[
{\rm O}\left( {M\sqrt {\frac{\ln n}{N}} } \right).
\]

Докажите, что данную оценку нельзя улучшить. 
\end{problem}

\begin{ordre} См. S. Bubeck, N. Cesa-Bianchi. Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems. In Foundati- ons and Trends in Machine Learning, Vol 5, 2012. 
\end{ordre}

\begin{problem}
В условиях предыдущей задачи предположим, что на $k$-м шаге $i$-й эксперт 
использует стратегию $\zeta _i^k \in \Delta $ (множество $\Delta $ -- 
выпуклое), дающую потери $\lambda \left( {\omega ^k,\zeta _i^k } \right)$, 
где $\omega ^k$ -- ``ход'', возможно, враждебной ``Природы'', знающей, в том 
числе, и нашу текущую стратегию. Функция $\lambda \left( {\,\cdot \,} 
\right)$ -- выпуклая по второму аргументу и $\left| {\lambda \left( {\,\cdot 
\,} \right)} \right|\le M$. На каждом шаге мы должны выбирать свою стратегию
\[
x\mathop =\limits^{def} \sum\limits_{i=1}^n {x_i \cdot \zeta _i^k } \in 
\Delta ,
\]
дающую потери $\lambda \left( {\omega ^k,x} \right)$ так, чтобы наши 
суммарные потери были минимальны. Для данной постановки также применима 
теорема 1 в детерминированном варианте с
\[
f_k \left( {x;\xi ^k} \right)\equiv f_k \left( x \right)=\sum\limits_{i=1}^n 
{x_i \lambda \left( {\omega ^k,\zeta _i^k } \right)} \ge \lambda \left( 
{\omega ^k,x} \right).
\]
Покажите, что оценка, даваемая теоремой 1, имеет вид
\[
{\rm O}\left( {M\sqrt {\frac{\ln n}{N}} } \right).
\]
Отметим, что эта оценка для данного класса задач.

\end{problem}

\begin{ordre}
Чтобы применить теорему заметим, что функция $\lambda 
\left( {\omega ^k,\zeta } \right)$ -- выпуклая по $\zeta $ для любого 
$\omega ^k$, поэтому
\[
\sum\limits_{k=1}^N {\lambda \left( {\omega ^k,x^k} \right)} -\mathop {\min 
}\limits_{i=1,...,n} \sum\limits_{k=1}^N {\lambda \left( {\omega ^k,\zeta 
_i^k } \right)} \le \sum\limits_{k=1}^N {f_k \left( {x^k} \right)} -\mathop 
{\min }\limits_{x\in S_n \left( 1 \right)} \sum\limits_{k=1}^N {f_k \left( x 
\right)} .
\]
\end{ordre}

\begin{problem}
Предположим, что в условиях предыдущей задачи мы не можем 
гарантировать выпуклость $\lambda \left( {\,\cdot \,} \right)$ -- по второму 
аргументу. Тогда мы выбираем стратегию -- распределение вероятностей на 
множестве стратегий Экспертов, и разыгрываем случайную величину согласно 
этому распределению вероятностей. Другими словами мы просто пользуемся МЗС2 
c $f_k \left( {x;\xi ^k} \right)\equiv f_k \left( x 
\right)=\sum\limits_{i=1}^n {x_i \lambda \left( {\omega ^k,\zeta _i^k } 
\right)} $, применимость которого обосновывается теоремой 2. Получите оценки

${\rm O}\left( {M\sqrt {\frac{\ln n}{N}} } \right) $ -- в среднем;

${\rm 
O}\left( {M\sqrt {\frac{\ln \left( {n \mathord{\left/ {\vphantom {n \sigma 
}} \right. \kern-\nulldelimiterspace} \sigma } \right)}{N}} } \right)$ -- с 
вероятностью $\ge 1-\sigma $.

%Отметим, что эти оценки для данного класса задач. 
\end{problem}

\begin{problem}[Антагонистические матричные игры]

Пусть есть два игрока А и Б. Задана матрица игры $A=\left\| {a_{ij} } \right\|$, где $\left| 
{a_{ij} } \right|\le M$, $a_{ij} $ -- выигрыш игрока А (проигрыш игрока Б) в 
случае когда игрок А выбрал стратегию $i$, а игрок Б стратегию $j$. 
Отождествим себя с игроком Б. И предположим, что игра повторяется $N\gg 1$ 
раз (это число может быть заранее неизвестно). Мы находимся в условиях 
предыдущей задачи с $\lambda \left( {\omega ^k,\zeta _j^k } 
\right)=\sum\limits_{i=1}^n {\omega _i^k a_{ij} } $, то есть
\[
f_k \left( x \right)=\left\langle {\omega ^k,Ax} \right\rangle ,
\quad
x\in S_n \left( 1 \right),
\]
где $\omega ^k$ -- вектор\footnote{ Вообще говоря, зависящий от всей истории 
игры до текущего момента включительно, в частности, как-то зависящий и от 
текущей стратегии (не хода) игрока Б, заданной распределением вероятностей 
(результат текущего разыгрывания (ход Б) игроку А не известен).} со всеми 
компонентами равными 0, кроме одной компоненты, соответствующей ходу А на 
шаге $k$, равной 1. Хотя функция $f_k \left( x \right)$ определена на 
единичном симплексе, по ``правилам игры'' вектор $x^k$ имеет ровно одну 
единичную компоненту, соответствующую ходу Б на шаге $k$, остальные 
компоненты равны нулю. Обозначим цену игры

$C=\mathop {\max }\limits_{\omega \in S_n \left( 1 \right)} \mathop {\min 
}\limits_{x\in S_n \left( 1 \right)} \left\langle {\omega ,Ax} \right\rangle 
=\mathop {\min }\limits_{x\in S_n \left( 1 \right)} \mathop {\max 
}\limits_{\omega \in S_n \left( 1 \right)} \left\langle {\omega ,Ax} 
\right\rangle .$ (теорема фон Неймана о минимаксе)

Пару векторов $\left( {\omega ,x} \right)$, доставляющих решение этой 
минимаксной задачи, назовем равновесием Нэша. По определению (это 
неравенство восходит к Ханнану)
\[
\mathop {\min }\limits_{x\in S_n \left( 1 \right)} 
\frac{1}{N}\sum\limits_{k=1}^N {f_k \left( x \right)} \le C.
\]
Тогда, если мы (игрок Б) будем придерживаться рандомизированной стратегии 
МЗС2, то по теореме 2 с вероятностью $\ge 1-\sigma $ (в случае когда $N$ 
заранее известно оценку можно уточнить)
\[
\frac{1}{N}\sum\limits_{k=1}^N {f_k \left( {x^k} \right)} -\mathop {\min 
}\limits_{x\in S_n \left( 1 \right)} \frac{1}{N}\sum\limits_{k=1}^N {f_k 
\left( x \right)} \le \frac{2M}{\sqrt N }\left( {\sqrt {\ln n} +\sqrt {2\ln 
\left( {\sigma ^{-1}} \right)} } \right),
\]
т.е. с вероятностью $\ge 1-\sigma $ наши потери ограничены
\[
\frac{1}{N}\sum\limits_{k=1}^N {f_k \left( {x^k} \right)} \le 
C+\frac{2M}{\sqrt N }\left( {\sqrt {\ln n} +\sqrt {2\ln \left( {\sigma 
^{-1}} \right)} } \right).
\]
Самый плохой для нас случай (с точки зрения такой оценки) -- это когда игрок 
А тоже ``знает'' теорему 2, и действует согласно МЗС2 (точнее версии МЗС2 
для максимизации вогнутых функций на симплексе). Очевидно, что если и А и Б 
будут придерживаться МЗС2, то они сойдутся к равновесию Нэша, причем 
чрезвычайно быстро:

$\frac{8M\left( {\ln n+2\ln \left( {\sigma ^{-1}} \right)} 
\right)}{\varepsilon ^2}$ -- итераций;

${\rm O}\left( {n+M\frac{s\ln n\left( {\ln n+\ln \left( {\sigma ^{-1}} 
\right)} \right)}{\varepsilon ^2}} \right)\quad $ -- общая сложность вычислений,
где $s\le n$ -- среднее число элементов в строках и столбцах матрицы $A$. 
Докажите эти оценки.

\end{problem}


















\begin{comment}
%refactorise%
\begin{problem} [Метод зеркального спуска Немировского--Юдина] ** Рассмотрим задачу стохастической оптимизации
\[
\frac{1}{N}\sum\limits_{k=1}^N {\Exp_{\xi_k} \left[ {f_k \left( {x,\xi_k } \right)} 
\right]} \to \mathop {\min }\limits_{x\in S_n \left( 1 \right)} ,\quad S_n 
\left( 1 \right)=\left\{ {x\ge 0: \; \sum\limits_{i=1}^n {x_i =1} } 
\right\},
\]
где $\xi _k $ -- независимые случайные величины, $f_k \left( {x,\xi _k } 
\right)$:

\begin{enumerate}
\item выпуклые по $x$; 
\item $\left\| {\nabla _x f_k\left( {x,\xi_k } \right)} \right\|_\infty \le M$; 
\item  $\Exp\left[ {\nabla_x f_k \left( {x; \xi_k} \right)} \right]=\nabla _x \Exp\left[ {f_k \left( {x;\xi_k} \right)} \right].$ 
\end{enumerate}

С целью получить адаптивный алгоритм поиска оптимального $x^t$ при известных $\xi_1 \ldots \xi_t$ и уже вычисленных $x^1 \ldots x^{t-1}$ выполним линейную аппроксимацию функций $f_k \left( {x,\xi _k } 
\right)$:  

\[
\min \limits_{x\in S_n \left( 1 \right)} \sum\limits_{k=1}^t 
f_k(x) \approx \min \limits_{x \in S_n(1)} 
\sum\limits_{k=1}^t \left\{ f_k (x^{k-1})+ \left\langle 
\sum\limits_{k=1}^t \nabla f_k(x^{k-1}) , x-x^{t-1}  
\right\rangle  \right\}.
\]
Полагая при этом, что невязка $i$-й компоненты $\nabla f_k \left( x^{k-1} \right)$ равна случайной величине $\varepsilon_{k,i}$:
\[
\PR(x_t = e_j) \mathop{=}
\PR_\varepsilon \left( 
  j=\arg \max \limits_{i=1,...,n} \sum \limits_{k=1}^t 
  \left( 
    \left[ 
        -\nabla f_k \left( x^{k-1} \right) 
    \right]_i + \varepsilon_{k,i}  
  \right) 
\right),
\]
получим, при довольно общих условиях относительно с.в. $\varepsilon _{k,i} $ 
(типа i.i.d.), что если $t\gg 1$, то
\[
\begin{array}{c}
 P_\varepsilon \left( {j=\arg \mathop {\max }\limits_{i=1,...,n} 
\sum\limits_{k=1}^t {\left\{ {\left[ {-\nabla f_k \left( {x^{k-1} } 
\right)} \right]_i +\varepsilon _{k,i} } \right\}} } \right)\approx \\ 
 \approx P_\varsigma \left( {j=\arg \mathop {\max }\limits_{i=1,...,n} 
\left\{ {\sum\limits_{k=1}^t {\left[ {-\nabla f_k \left( {x^{k-1} } 
\right)} \right]_i } } \right\}+\varsigma _{t,i} } \right), \\ 
 \end{array}
\]
где с.в. $\varsigma _{t,i} $ -- i.i.d. с распределением Гумбеля 
(max-устойчивым), с параметром, зависящим от $t$: $P\left( {\varsigma _{t,i} 
<\tau } \right)=\exp \left\{ {-e^{-\tau \mathord{\left/ {\vphantom {\tau 
{\beta _t }}} \right. \kern-\nulldelimiterspace} {\beta _t }}} \right\}$, $\beta _t =\frac{M}{\sqrt {\ln n} }\sqrt {t+1}$
(см. задачу \ref{gumbel}).

Тогда
\[
E_\varsigma \left[ {x^t} \right]=-W_{\beta _t } \left( {\sum\limits_{k=1}^t 
{\nabla f_k \left( {x^{k-1} } \right)} } \right),
\]
где \[W_\beta \left( y \right)=\beta \ln \left( 
{\frac{1}{n}\sum\limits_{i=1}^n {\exp \left( {-\frac{y_i }{\beta }} \right)} 
} \right).\]

Используя результат задачи \ref{gibbs}, получите конечную формулу для независимого вычисления каждой из компонент $x^t$:

\[
x_i^t \propto \exp \left( {-\frac{1}{\beta_t 
}\sum\limits_{k=1}^{t-1} {\frac{\partial f_k \left( {x^{k-1},\xi _k } 
\right)}{\partial x_i }} } \right).
\]

\begin{remark}
Полученный алгоритм является стохастическим вариантом метода 
зеркального спуска (экспоненциального взвешивания в наших условиях).

Стоит подчеркнуть, что независимость вычислений каждой из компонент $x^t$ позволяет использовать распределенные вычисления $x^t$, предварительно разослав (broadcast) $x^{t-1}$ на каждый из рабочих узлов кластера.  
\end{remark}

Известно (см., например, работу Юдицкий А.Б., Назин А.В., Цыбаков А.Б., 
Ваятис Н. Рекуррентное агрегирование оценок методом зеркального спуска с 
усреднением // Пробл. передачи информ., 2005. Т. 41:4. стр. 78--96), что

\[
\sum\limits_{k=1}^t {\gamma _k \left\{ {E_{\xi ,x} \left[ {f_k \left( 
{x^{k-1};\xi_k} \right)} \right]-E_{\xi} \left[ {f_k \left( {x;\xi_k} 
\right)} \right]} \right\}} \le 
\]
\begin{equation}
\label{terrible}
\begin{array}{c}
 \le \sum\limits_{k=1}^t {\gamma _k \left( {x^{k-1}-x} \right)^T} \nabla _x 
E_{\xi ,x} \left[ {f_k \left( {x^{k-1};\xi_k} \right)} \right]\le \beta _t 
V\left( x \right)- \\ 
 -\sum\limits_{k=1}^t {\gamma _k \left( {x^{k-1}-x} \right)^T} \left( 
{\nabla _x f_k \left( {x^{k-1};\xi_k} \right)-\nabla _x E_{\xi ,x} \left[ 
{f_k \left( {x^{k-1};\xi_k} \right)} \right]} \right)+ \\ 
 +\sum\limits_{k=1}^t {\frac{\gamma _k^2 }{2\beta _{k-1} }\left\| {\nabla _x 
f_k \left( {x^{k-1};\xi_k} \right)} \right\|_\infty ^2,} \\ 
 \end{array}
\end{equation}
где  $V\left( x \right)=\ln n+\sum\limits_{i=1}^n {x_i \ln x_i }$.

Введем компактное обозначение для минимума целевой функции
\[
{\rm F_{min}} = \mathop {\min }\limits_{x\in S_n \left( 1 \right)} 
\frac{1}{N}\sum\limits_{k=1}^N {\Exp_{\xi_k} \left[ {f_k \left( {x,\xi _k } 
\right)} \right]}
\]

Получите следующие оценки для среднего отклонения от минимального значения 
целевой функции:

$$
\Exp_{x} \left( \frac{1}{N}\sum\limits_{k=1}^N {\Exp_{\xi_k} \left[ {f_k \left( {x^{k-1},\xi 
_k } \right)} \right]} \right) -\rm{F_{min}} \le 2M\sqrt {\frac{\ln n}{N}},
$$

a если все $f_k \equiv f$ и $\xi_k$ -- независимы и одинаково распределены, то

\[
\Exp_{x}  {\Exp_{\xi} \left[ {f\left( 
{\frac{1}{N}\sum\limits_{k=1}^N {x^{k-1}} ,\xi } \right)} \right] - {\rm F_{min}} } \le 2M\sqrt {\frac{\ln n}{N}}.  
\]

Также получите вероятностные оценки отклонения от минимального значения 
целевой функции на величину  
$\delta(\Omega) = \frac{2M}{\sqrt N } \left({\sqrt {\ln n} +\sqrt {3\Omega }  }\right)$:

\[
\PR_{x} 
\bigg[
    \frac{1}{N}\sum\limits_{k=1}^N \Exp_{\xi _k} 
    \left[ {
      f_k \left( {x^{k-1},\xi_k } \right)
    } \right] 
    - {\rm F_{min}}  \ge  
   \delta(\Omega)
\bigg] 
\le \exp \left( {-\Omega}  \right),
\]
\[
\PR_{x}
\Bigg[
  \Exp_\xi \left[{
     f\left({
        \frac{1}{N}\sum\limits_{k=1}^N {x^{k-1}} ,\xi 
     } \right)
  } \right]
- {\rm F_{min}} \ge
\delta(\Omega)
\Bigg]
\le \exp \left( {-\Omega } \right).
\]
\end{problem}

\begin{ordre}  
Для получения оценок вероятностей больших уклонений, 
воспользуйтесь п. б)  задачи \ref{sec:mirrorDescent} из раздела \ref{measure}. 
\end{ordre}  

\begin{remark} 
Важно отметить, что описанный в этой задаче адаптивный 
метод помимо приложений в статистике имеет широкие приложения в задачах 
оптимального взвешивания экспертных решений (см., например, Вьюгин В.В. Элементы 
математической теории машинного обучения. М.: МФТИ, 2010, глава 3) и в 
задачах о многоруких бандитах (см., например, \textit{Lugoshi G., Cesa-Bianchi N}. Prediction, learning and 
games. New York: Cambridge University Press, 2006, а также Juditsky A., 
Nazin A.V., Tsybakov A.B., Vayatis N. Gap-free Bounds for Stochastic 
Multi-Armed Bandit // IFAC World congress, 2008). Удивительно, что во всех 
случаях, описанный метод дает (с мультипликативной точностью до константы) 
не улучшаемые оценки (только в случае многоруких бандитов c 
мультипликативной точностью $\sim \sqrt {\ln n} )$.

Для того чтобы применять этот метод к многоруким бандитам: $f_k \left( 
{x,\xi _k } \right)=r_i^k $ с вероятностью $x_i $, $i=1,...,n$; а чтобы 
$\nabla _x \Exp\left[ {f_k \left( {x,\xi _k } \right)} \right]=a=\Exp\left[ {r^k} 
\right]$, выбирают $\nabla _x f_k \left( {x,\xi _k } \right)=(\underbrace 
{0,..,r_i^k}_i{} \mathord{\left/ {\vphantom {{r_i^k } {x_i }}} \right. 
\kern-\nulldelimiterspace} {x_i },...,0)^T$ с вероятностью $x_i $, 
$i=1,...,n$, где $r_i^k \ge 0$ -- потери (regret), которые выдает 
$i\mbox{-я}$ ручка, если её дернуть на шаге $k$. При этом вместо слагаемых

$\frac{\gamma _k^2 }{2\beta _{k-1} }\left\| {f_k \left( {x^{k-1};\xi ^k} 
\right)} \right\|_\infty ^2 $ в выражении (\ref{terrible}) необходимо писать точнее \[\gamma _k^2 
\frac{x_j^{k-1} \left( {1-x_j^{k-1} } \right)}{\beta _{k-1} }\left( {\frac{r_j^k 
}{x_j^{k-1} }} \right)^2,\]
где $j$ -- номер ручки, выбранной алгоритмом на $k$-м шаге.

Для задач оптимального рандомизированного взвешивания экспертных решений 
$\Exp_\xi \left[ {f_k \left( {x,\xi_k } \right)} \right]=\sum\limits_{i=1}^n 
{\lambda \left( {\omega _k ,\zeta _k^i } \right)x_i } $, где $\lambda \left( 
{\omega _k ,\zeta _k^i } \right)$ -- потери Эксперта $i$, выбравшего на шаге 
$k$ стратегию $\zeta _k^i $, при ходе ``сопротивляющейся Природы'' $\omega 
_k $ (на каждом шаге первым выбирают свои ходы Эксперты, потом мы, потом 
природа; но есть важный нюанс: наш ход заключается в выборе распределения 
вероятностей, которое становится известным Природе, но разыгрывание согласно 
этому распределению вероятностей происходит после того, как Природа выбрала 
свой ход.

Тонкая разница в постановке этих двух задача (``стоящая'' $\sqrt n )$, 
заключается в том, что в многоруких бандитах игрок имеет только свою историю 
дергания ручек (ему не известно, какой бы \textit{regret} принесли ему другие ручки, кабы 
он их выбрал), а в постановке взвешивания экспертных решений это все 
известно и называется потерями экспертов.

В случае решения детерминированных разреженных задач выпуклой оптимизации в 
пространствах огромных размеров, как, например, задачи о ранжировании 
web-страниц

\begin{center}
http://goo.gl/HLM6w
\end{center}
важную роль играет искусственное введение случайности (рандомизация), позволяющее найти $x.\mathrm{top}(\cdot)$, проводя вычисления с разреженным вектором $x$:

\[
x_{i\left( t \right)}^t =x_{i\left( t \right)}^{t-1} +1,\;\;x_j^t =x_j^{t-1} 
,\;j\ne i\left( t \right), 
\]
где
\[
p_i^t \propto \exp \left( {-\frac{1}{\beta _t }\sum\limits_{k=1}^{t-1} 
{\gamma _k \frac{\partial f_k \left( {x^{k-1},\xi _k } \right)}{\partial x_i }} 
} \right),\quad i=1,...,n,
\]

Для этого алгоритма $\beta _t $ и $\gamma _t $ разумнее брать постоянными:
\[
\beta _t \equiv 1,
\quad
\gamma _t \equiv M^{-1}\sqrt {{2\ln n} \mathord{\left/ {\vphantom {{2\ln n} 
N}} \right. \kern-\nulldelimiterspace} N} .
\]
При этом сохраняются все приведенные в задаче оценки, но, к сожалению, 
теряется адаптивность, то есть теперь мы должны знать заранее либо требуемую точность, либо число шагов.
\end{remark}

\end{comment}
