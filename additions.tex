\section{Вспомогательные материалы}

\subsection*{Метод зеркального спуска}

Рассмотрим задачу стохастической онлайн оптимизации\footnote{ Запись $\Exp_{\xi 
^k} \left[ {f_k \left( {x;\xi ^k} \right)} \right]$ означает, что 
математическое ожидание берется по $\xi^k$, то есть $x$ и $f_k $ понимаются 
в такой записи не случайными. Отметим, что $\xi ^k$ может зависеть от $\xi 
^1$, {\ldots}, $\xi ^{k-1}$, а распределение $\xi^k$ может зависеть от $x$.}
\begin{equation}
\label{eq1}
\frac{1}{N}\sum\limits_{k=1}^N {\Exp_{\xi ^k} \left[ {f_k \left( {x;\xi ^k} 
\right)} \right]} \to \mathop {\min }\limits_{x\in S_n \left( 1 \right)} 
,
\end{equation}
\[
S_n \left( 1 \right)=\left\{ {x\ge 0: \; \sum\limits_{i=1}^n {x_i =1} } 
\right\}
\]
при следующих условиях:

\begin{enumerate}
\item $\Exp_{\xi^k} \left[ {f_k \left( {x;\xi ^k} \right)} \right]\quad -$ выпуклые функции (по $x)$, для этого достаточно выпуклости по $x$ функций $f_k \left( {x;\xi ^k} \right)$;
\item Существует такой вектор $\nabla _x f_k \left( {x;\xi ^k} \right)$, который для компактности будем называть субградиентом, хотя последнее верно не всегда, что
\[
\Exp_{\xi ^k} \left( {\left. {\nabla _x f_k \left( {x;\xi ^k} \right)-\nabla _x 
\Exp_{\xi ^k} \left[ {f_k \left( {x;\xi ^k} \right)} \right]} \right|\Xi 
^{k-1}} \right)\equiv 0,
\]
где $\Xi ^{k-1}$ -- $\sigma $-алгебра, порожденная с.в. 
$\xi ^1, \ldots ,\xi ^{k-1}$. Далее мы будем использовать 
обозначения обычного градиента для векторов, которые мы назвали здесь 
субградиентами. 
%В частности, если мы имеем дело с обычным субградиентом, то 
%запись $\nabla _x f_k \left( {x;\xi ^k} \right)$ в вычислительном контексте 
%(например, в итерационной процедуре МЗС, описанной ниже) означает какой-то 
%его элемент (не важно какой именно), а если в контексте проверки условий 
%(например, в условии 3 ниже), то $\nabla _x f_k \left( {x;\xi ^k} \right)$ 
%пробегает все элементы субградиента (говорят также, субдифференциала);

\item $\left\| {\nabla _x f_k \left( {x;\xi ^k} \right)} \right\|_\infty \le M\quad -$ (равномерно, с вероятностью 1) ограниченный субградиент. Для справедливости части утверждений достаточно требовать одно из следующих (более слабых) условий:
\end{enumerate}
\begin{center}
 $1) \Exp_{\xi ^k} \left[ {\left\| {\nabla _x f_k \left( {x;\xi ^k} \right)} 
\right\|_\infty ^2 } \right]\le M^2$; $2) \Exp_{\xi ^k} \left[ {\left. {\exp 
\left( {\frac{\left\| {\nabla _x f_k \left( {x;\xi ^k} \right)} 
\right\|_\infty ^2 }{M^2}} \right)} \right|\Xi ^{k-1}} \right]\le \exp 
\left( 1 \right)$.
\end{center}

Онлайновость постановки задачи допускает, что на каждом шаге $k$ функция 
$f_k $ может подбираться из рассматриваемого класса функций враждебно по 
отношению к используемому нами методу генерации последовательности $\left\{ 
{x^k} \right\}$. В частности, $f_k $ может зависеть от $\left\{ {x^1,\xi 
^1;...;x^{k-1},\xi ^{k-1};x^k} \right\}$, если выбор $x^k$ осуществляется 
исходя только из информации $\left\{ {x^1,\xi ^1;...;x^{k-1},\xi ^{k-1}} 
\right\}$, т.е. без дополнительной рандомизации. Ситуация с дополнительной 
рандомизацией при выборе $x^k$ рассматривается в следующем пункте.

Для решения задачи (\ref{eq1}) воспользуемся адаптивным методом зеркального спуска (точнее двойственных усреднений). Положим $x_i^1 =1 \mathord{\left/ 
{\vphantom {1 n}} \right. \kern-\nulldelimiterspace} n$, $i=1,...,n$. Пусть 
$t=1,...,N-1$.

\underline {Алгоритм МЗС1-адаптивный}

\[
x_i^{t+1} \propto \exp \left( {-\frac{1}{\beta _{t+1} }\sum\limits_{k=1}^t 
{\frac{\partial f_k \left( {x^k;\xi ^k} \right)}{\partial x_i }} } 
\right),\quad i=\overline{1,n},\; \beta _t 
=\frac{M\sqrt t }{\sqrt {\ln n} }.
\]

\begin{theorem}
Пусть справедливы условия а, б, в.1, тогда
\[
\mbox{ }\frac{1}{N}\sum\limits_{k=1}^N {\Exp\left[ {f_k \left( {x^k;\xi ^k} 
\right)} \right]} -\mathop {\min }\limits_{x\in S_n \left( 1 \right)} 
\frac{1}{N}\sum\limits_{k=1}^N {\Exp \left[ {f_k \left( {x;\xi ^k} 
\right)} \right]} \le 2M\sqrt {\frac{\ln n}{N}} .
\]
Если $f_k = f$, а $\left\{ {\xi ^k} \right\}$ -- независимы и одинаково распределены, как $\xi$, то
\[
\Exp\left[ {f\left( {\frac{1}{N}\sum\limits_{k=1}^N {x^k} ;\xi } \right)} 
\right]-\mathop {\min }\limits_{x\in S_n \left( 1 \right)} \Exp \left[ 
{f\left( {x;\xi } \right)} \right]\le 2M\sqrt {\frac{\ln n}{N}} .
\]
Пусть справедливы условия а, б, в, тогда\footnote{ Запись\par ``$\PR_{x^1,...,x^N} \left\{ 
{\frac{1}{N}\sum\limits_{k=1}^N {\Exp_{\xi ^k} \left[ {f_k \left( {x^k;\xi ^k} 
\right)} \right]} -...} \right.$''\par означает, что внутри фигурных скобок мы 
считаем математическое ожидание по $\xi ^k$ при фиксированных $x^k$.} при $\Omega \ge 0$
\[
\PR_{x^1,...,x^N} \left\{ {\frac{1}{N}\sum\limits_{k=1}^N {\Exp_{\xi ^k} \left[ 
{f_k \left( {x^k;\xi ^k} \right)} \right]} -\mathop {\min }\limits_{x\in S_n 
\left( 1 \right)} \frac{1}{N}\sum\limits_{k=1}^N {\Exp \left[ {f_k 
\left( {x;\xi ^k} \right)} \right]} \ge } \right.
\]
\[
\left. {\frac{2M}{\sqrt N }\left( {\sqrt {\ln n} +\sqrt {8\Omega } } 
\right)} \right\} \le \exp \left( {-\Omega } \right).
\]
Если $f_k = f$, а $\left\{ {\xi ^k} \right\}$ -- независимы и одинаково распределены, как $\xi$, то
\[
\PR_{x^1,...,x^N} \left\{ {\Exp_\xi \left[ {f\left( 
{\frac{1}{N}\sum\limits_{k=1}^N {x^k} ;\xi } \right)} \right]-\mathop {\min 
}\limits_{x\in S_n \left( 1 \right)} \Exp \left[ {f\left( {x;\xi } \right)} 
\right]} \right.
\]
\[
\left. {\ge \frac{2M}{\sqrt N }\left( {\sqrt {\ln n} +\sqrt {8\Omega } } 
\right)} \right\} \le \exp \left( {-\Omega } \right).
\]
\end{theorem}

\textbf{Замечание.} Для задач стохастической выпуклой оптимизации (см., 
например, Shapiro A., Dentcheva D., Ruszczynski A. Lecture on stochastic programming. Modeling and theory. MPS-SIAM series on Optimization, 2009), в которых $\left\{ {\xi ^k} \right\}$ -- 
независимые случайные величины, можно привести оценку вероятностей больших 
уклонений с точностью до констант аналогичную оценке, приведенной в теореме 
и для случая, когда вместо условия в) предполагается условие в.2 при этом в 
правой части неравенства под вероятностью вместо $\sqrt \Omega $ необходимо 
писать $\Omega $ (с точностью до констант).

Снова рассмотрим постановку задачи стохастической онлайн оптимизации (\ref{eq1}). Но на этот раз будем допускать, что метод генерирования последовательности 
$\left\{ {x^k} \right\}$ может допускать (внешнюю, дополнительную) 
рандомизацию. Т.е.  вместо   Также как и раньше онлайновость постановки задачи допускает, 
что на каждом шаге $k$ функция $f_k $ может подбираться из рассматриваемого 
класса функций враждебно по отношению к используемому нами методу генерации 
последовательности $\left\{ {x^k} \right\}$. В частности, $f_k $ и $\xi ^k$ 
могут зависеть от $\left\{ {x^1,\xi ^1;...;x^{k-1},\xi ^{k-1}} \right\}$, и 
даже от распределения вероятностей $p^k$ (многорукие бандиты), согласно 
которому осуществляется выбор $x^k$. Чтобы можно было работать с таким 
классом задач, нам придется наложить дополнительное условие:


г) На каждом шаге генерирование случайной величины $x^k$ согласно распределению вероятностей $p^k$ осуществляется независимо ни от чего.

Положим $p_i^1 =x_i^1 =1 \mathord{\left/ {\vphantom {1 n}} \right. 
\kern-\nulldelimiterspace} n$, $i=1,...,n$. Пусть $t=1,...,N-1$.

$ $

\underline {Алгоритм МЗС2-адаптивный}

\noindent Согласно распределению вероятностей

\[
p_i^{t+1} \propto \exp \left( {-\frac{1}{\beta _{t+1} }\sum\limits_{k=1}^t 
{\frac{\partial f_k \left( {x^k;\xi ^k} \right)}{\partial x_i }} } 
\right), \quad i=\overline{1,n},\;
\beta _t =\frac{M\sqrt t }{\sqrt {\ln n} },
\]
получаем случайную величину 
$i\left( {t+1} \right)$, т.е. \\ $x_{i\left( {t+1} \right)}^{t+1} =1,\;\;x_j^{t+1} 
=0,\;j\ne i\left( {t+1} \right).$



\underline {Алгоритм МЗС2-неадаптивный (заранее известно $N$)}

\noindent Согласно распределению вероятностей

\[
p_i^{t+1} \propto \exp \left( {-\frac{1}{\beta _{t+1} }\sum\limits_{k=1}^t 
{\gamma _k \frac{\partial f_k \left( {x^k;\xi ^k} \right)}{\partial x_i }} } 
\right),\quad i=\overline{1,n},
\]
\[
\gamma _k \equiv M^{-1}\sqrt {{2\ln n} \mathord{\left/ {\vphantom {{2\ln n} 
N}} \right. \kern-\nulldelimiterspace} N} ,
\quad
\beta _t \equiv 1,
\]
получаем случайную величину 
$i\left( {t+1} \right)$, т.е. \\ $x_{i\left( {t+1} \right)}^{t+1} =1,\;\;x_j^{t+1} 
=0,\;j\ne i\left( {t+1} \right).$

Аппроксимируя
\[
\mathop {\min }
\limits_{x\in S_n \left( 1 \right)} 
\frac{1}{N}
\sum_{k=1}^t 
f_k \left( x \right) \approx \mathop {\min} \limits_{x\in S_n \left( 1 \right)}  
\frac{1}{N}\sum\limits_{k=1}^t 
\left\{ 
  f_k \left( x^k  \right)+
  \left\langle \nabla f_k \left( {x^k } 
   \right),x-x^k  \right\rangle  
\right\} ,
\]
получим
\[
\PR\left( {x_j^{t+1} =1;\;x_i^{t+1} =0,\;i\ne j} \right)\mathop =
\]
\[ 
= \PR_\varsigma \left( {j=\arg \mathop {\max }\limits_{i=1,...,n} \left\{ 
{\sum\limits_{k=1}^t {\left[ {-\nabla f_k \left( {x^k } \right)} \right]_i 
+\varsigma _{t,i} } } \right\}} \right),
\]
где $\varsigma _{t,i} $ -- независимые одинаково распределенные случайные 
величины по закону Гумбеля с параметром $\beta _{t+1} $, характеризующим 
среднеквадратичное отклонение $\varsigma _{t,i} $ (см. задачу \ref{gumbel} раздела 
\ref{hard}):\footnote{ Поскольку случайные величины $\varsigma _{t,i} $ получаются в 
результате суммирования $t$ слагаемых (невязок в аппроксимации выпуклой 
функции линейными минорантами), то можно ожидать, что среднеквадратичное 
отклонение $\varsigma _{t,i} $ имеет порядок $\sqrt t $. Условие одинаковой 
распределенности $\varsigma _{t,i} $, которое не понятно за счет чего может 
иметь место, в действительности, не очень здесь и нужно. Достаточно, чтобы 
$E\left[ {\varsigma _{t,i} -\varsigma _{t,j} } \right]=o\left( {\sqrt t } 
\right)$ и $\mbox{Var}\left[ {\varsigma _{t,i} } \right]\sim \sqrt t $. 
Более того, если, с некоторой натяжкой, наряду с независимостью $\left\{ 
{\varsigma _{t,i} } \right\}_{i=1}^n $ также считать, что при формировании 
$\varsigma _{t,i} $ суммируются независимые слагаемые, то можно ожидать, что 
$\varsigma _{t,i} $ нормальные случайные величины (центральная предельная 
теорема). Следовательно, в контексте последующих операций при $n\gg 1$, 
$\varsigma _{t,i} $ могут быть заменены на случайные величины, 
распределенные по закону Гумбеля с соответствующими среднеквадратичными 
отклонениями. }
\[
P\left( {\varsigma _{t,i} <\tau } \right)=\exp \left\{ {-e^{-\tau 
\mathord{\left/ {\vphantom {\tau {\beta _{t+1} }}} \right. 
\kern-\nulldelimiterspace} {\beta _{t+1} }}} \right\}.
\]
Тогда (см. задачу \ref{gibbs} раздела \ref{hard})
\[
E_\varsigma \left[ {x^{t+1}} \right]=\nabla W_{\beta _{t+1} } \left( 
{-\sum\limits_{k=1}^t {\nabla f_k \left( {x^k } \right)} } \right).
\]
Распределение Гумбеля возникает именно в таком контексте совсем не случайно, 
и связано это с тем, что оно max-устойчиво. 
Другими словами, при некоторых довольно общих предположениях (типа Крамера) 
$\varsigma _{t,i} $ могут быть распределенными произвольно, тем не менее, 
при $n\gg 1$ с хорошей точностью можно считать, что мы в итоге имеем дело с 
соответствующим распределением Гумбеля. Более того, если задаться вопросом: 
а какое распределение ``наиболее подходит'' для $\varsigma _{t,i} $, чтобы в 
случае ``враждебной Природы'' (то есть в минимаксном смысле) иметь наилучшие 
оценки, то ответом будет: симметричное показательное распределение 
(Лапласа), которое ведет себя в интересном для анализа диапазоне подобно 
распределению Гумбеля, но в отличие от распределения Лапласа, в случае 
Гумбеля мы явно можем посчитать интересующие нас вероятности. Как правило, 
такого рода задачи явно не решаются, и распределение Гумбеля является 
приятным исключением, для которого есть явные формулы. 

К сожалению, не делая относительно функций $f_k \left( {x;\xi ^k} \right)$ 
дополнительно никаких предположений, не удается доказать для МЗС2 аналог 
теоремы 1. Чтобы можно было сформулировать такой аналог, мы вынуждены будем 
предполагать, что $f_k \left( {x;\xi ^k} \right)$ -- линейные функции по $x$ 
(можно обобщить и на сублинейные). С одной стороны это существенно сужает 
класс задач, к которым применим МЗС2. С другой стороны, как будет 
продемонстрировано в следующем пункте, даже такой узкий класс функций за 
счет ``онлайновости'' позволяет применять МЗС2 к довольно широкому кругу 
задач. Для того чтобы лучше чувствовалась преемственность методов и 
доказательств их сходимости, далее мы по-прежнему будем использовать общие 
обозначения $f_k \left( {x;\xi ^k} \right)$, не подчеркивая в формулах 
линейность.

\begin{theorem}

Пусть справедливы условия а, б, в.1, г и $f_k \left( {x;\xi ^k} \right)$ -- линейные функции по $x$, тогда
\[
\frac{1}{N}\sum\limits_{k=1}^N {\Exp\left[ {f_k \left( {x^k;\xi ^k} \right)} 
\right]} -\mathop {\min }\limits_{x\in S_n \left( 1 \right)} 
\frac{1}{N}\sum\limits_{k=1}^N {\Exp\left[ {f_k \left( {x;\xi ^k} \right)} 
\right]} \le 2M\sqrt {\frac{\ln n}{N}} .
\]
Для неадаптивного метода ``2''-у перед $M$ можно занести под знак корня.
Кроме того если справедливы условия а, б, в, г, при  $\Omega \ge 0$
\[
\PR_{x^1,...,x^N} \left\{ {\frac{1}{N}\sum\limits_{k=1}^N {\Exp_{\xi ^k} \left[ 
{f_k \left( {x^k;\xi ^k} \right)} \right]} -\mathop {\min }\limits_{x\in S_n 
\left( 1 \right)} \frac{1}{N}\sum\limits_{k=1}^N {\Exp_{\xi ^k} \left[ {f_k 
\left( {x;\xi ^k} \right)} \right]} \ge } 
\right.
\]
\[
\left. {\frac{2M}{\sqrt N 
}\left( {\sqrt {\ln n} +\sqrt {18\Omega } } \right)} \right\} \le \exp 
\left( {-\Omega } \right).
\]
Если $f_k \left( {x;\xi ^k} \right) = f_k \left( x \right)$ , то это неравенство можно уточнить
\[
\frac{2M}{\sqrt N }\left( {\sqrt {\ln n} +\sqrt {18\Omega } } \right)\to 
\frac{2M}{\sqrt N }\left( {\sqrt {\ln n} +\sqrt {2\Omega } } \right),
\]
при этом же условии для неадаптивного метода можно еще больше уточнить:
\[
\frac{2M}{\sqrt N }\left( {\sqrt {\ln n} +\sqrt {2\Omega } } \right)\to 
\frac{\sqrt 2 M}{\sqrt N }\left( {\sqrt {\ln n} +2\sqrt \Omega } \right).
\]

\end{theorem}