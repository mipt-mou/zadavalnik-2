\begin{comment}
Введем следующие обозначения, используемые в задачах на тему классификация (см. http://www.machinelearning.ru/):

${\mathbb X }$ - генеральная выборка (матрица объект-признак) размера $L$ (число объектов).

$X$ - наблюдаемоя (обучающая) выборка размера $l$. У объектов из данной выборки известны значения классов.

$\overline{X}$ - тестовая выборка размера $L - l$. У объектов из данной выборки значения классов не известны и их необходимо предсказать, проанализировав $X$.

$a:{\mathbb X } \rightarrow \{0,1\}$ - алгоритм классификации, где множество значений - есть множество индексов классов.

$I:{\mathbb X } \rightarrow \{0,1\} $ - индикатор ошибки, для заданного алгоритма.

\begin{problem}
Докажите, что при $n(a, {\mathbb X }) = m$ число ошибок в наблюдаемой подвыборке $n(a,X)$
подчиняется гипергеометрическому распределению:
\[
P(n(a,X) = s) = h^{l,m}_L (s) = \frac{C^s_m C^{l-s}_{L-m}}{C^l_L},
\]

\noindent где $s$ принимает значения от $s_0 = \max\{0,m - k\}$ до $s_1 = \min\{l,m\}$.
\end{problem}

\begin{problem}
Пусть алгоритм $a$ допускает $m$ ошибок на генеральной выборке:
$n(a, {\mathbb X }) = m$. Покажите, что в этом случае для любого $\epsilon \in [0, 1]$ справедливы равенства:
\[
Q\epsilon(a,{\mathbb X }) = P(\nu(a, \overline{X} ) - \nu(a,X) \geq \epsilon) =                     H^{l,m}_L(\frac{l}{L}(m - \epsilon k))
\]

\[
R\epsilon(a,{\mathbb X }) = P(\nu(a, \overline{X} ) \geq \epsilon) =                     H^{l,m}_L((m - \epsilon k))
\]
\[
C(a,{\mathbb X }) = \Exp\nu(a, \overline{X} ) = \frac{m}{L},
\]

\noindent где $\nu(\cdot)$ - частота ошибок алгоритма,
\[
H^{l,m}_L(z) = \overset{\lfloor z \rfloor}{\underset{s=s_0}{\sum}}h^{l,m}_L (s)
\]
\end{problem}

\begin{problem}
Для любых $\mu$ - метод обучения (выбора алгоритма по обучающей выборке), ${\mathbb X }$, $I$, $A_e$ - множество векторов ошибок, порождаемых множеством алгоритмов $A$ и $\epsilon \in [0, 1]$ требуется доказать следующие оценки:

\[
Q\epsilon(\mu, {\mathbb X }) \leq |A_e| \underset{m = 1, \ldots, L}{\max} H^{l,m}_L(\frac{l}{L}(m - \epsilon k)),
\]

\[
Q\epsilon(\mu, {\mathbb X }) \leq |A_e|\; \frac{3}{2} e^{-\epsilon^2 l}, \; l = k
\]

\end{problem}

\end{problem}



\end{comment}

\begin{remark}
Классическая постановка задачи \textit{обучения распознаванию образов} с двумя классами объектов. Изучается некоторое множество объектов $\omega \in \Omega $, каждый из которых обладает $n$ измеряемыми свойствами, выраженными действительными числами $x_{i} (\omega )\in {\rm {\mathbb R}}$, $i=1,...,n$. Совокупность результатов этих измерений будем называть вектором действительных признаков объекта $x(\omega )=\left(x_{1} (\omega )\cdots x_{n} (\omega )\right)^{T} \in {\rm {\mathbb R}}^{n} $. \underbar{}

Допустим, что все множество объектов $\omega \in \Omega $ разбито на два класса индикаторной функцией $y(\omega ):\; \Omega \to \{ -1,\; 1\} $, вообще говоря, неизвестной наблюдателю. Целью наблюдателя является определение класса предъявленного объекта $y(\omega )\in \{ -1,\; 1\} $, зная лишь доступный для непосредственного наблюдения вектор признаков $x(\omega )\in {\rm {\mathbb R}}^{n} $. Иными словами, желание наблюдателя сводится к построению дискриминантной функции $\hat{y}(x):\; {\rm {\mathbb R}}^{n} \to \{ -1,\; 1\} $. В качестве исходной информации для выбора дискриминантной функции будем рассматривать обучающую совокупность объектов, представленных и векторам их признаков $x_{j} =x(\omega _{j} )\in {\rm {\mathbb R}}^{n} $, и фактическими значениями индикаторной функции класса $y_{j} =y(\omega _{j} )\in \{ -1,\; 1\} $. Таким образом, обучающая совокупность представлена конечным множеством пар $(X,Y)=\left\{(x_{j} ,y_{j} ),\; j=1,...,N\right\}$. 
\end{remark}


\begin{problem}[Байесовская интерпретация Метода опорных векторов~SVM]

Рассмотрим следующую модель наблюдения. Пусть в ${\rm {\mathbb R}}^{n} $ определена некоторая гиперплоскость $a^{T} x+b$ с направляющим вектором $a\in {\rm {\mathbb R}}^{n} $ и параметром сдвига $b\in {\rm {\mathbb R}}$, а также пара несобственных плотностей распределения вероятностей $\phi (x|y;\; a,b,c)$, $x,a\in {\rm {\mathbb R}}^{n} $, $b,c\in {\rm {\mathbb R}}$, $y=\pm 1$ (см. замечание к задаче \ref{van_tris}), сконцентрированных преимущественно по разные стороны от этой гиперплоскости (параметр $c$ считается известным): 

\[
\phi (x|y;\; a,b,c)=\exp \left[-c\left(1-y(a^{T} x+b)\right)_{+} \right],
\]
где $(x)_{+} = \max(0, x)$. 


Направляющий вектор $a\in {\rm {\mathbb R}}^{n} $ будем рассматривать как случайный, распределенный с некоторой известной плотностью $\Psi (a)$. Никаких априорных предположений о значении случайного сдвига гиперплоскости $b\in {\rm {\mathbb R}}$ приниматься будет, так что совместное распределение $\Psi (a,b)$ будет рассматриваться как несобственное: 

\[
\Psi (a,b)\propto \Psi (a).  
\] 
Далее, пусть обучающая совокупность \[(X,Y) = \left\{(x_{j} ,y_{j} ), \; j=1,...,N\right\}\] есть результат многократных случайных независимых реализаций распределений $\phi (x|y=1;\; a,b,c)$ и $\phi (x|y=-1;\; a,b,c)$, всякий раз с известным индексом $y=\pm 1$ принадлежности очередного объекта к одному их классов. 

а) Запишите апостериорное распределение параметров разделяющей гиперплоскости после наблюдения обучающей совокупности (согласно формуле Байеса). Покажите, что с точностью до множителя, не зависящего от параметров гиперплоскости

\[\PR(a,b\, |\, X,Y)\propto \Psi (a)\Phi (X\, |\, Y;a,b),\] 
\[\Phi (X|Y;\; a,b,c)=\prod _{j=1}^{N}\phi (x_{j} |y_{j} ;\; a,b,c).\]

Обучение естественно понимать как вычисление байесовской оценки параметров разделяющей гиперплоскости: 

\[(\hat{a},\hat{b}\, |\, X,Y;c)=\mathop{\arg \max }\limits_{a\in {\rm {\mathbb R}}^{n} ,\; b\in {\rm {\mathbb R}}} \PR(a,b\, |\, X,Y).\] 

Покажите, что критерий обучения можно записать через следующую задачу оптимизации:

\[
-\ln \Psi (a)+ c \sum_{j} \left(1-y_i(a^{T} x_i+b)\right)_{+} \to \min_{a,b},
\] 
или как задачу квадратичного программирования

\[\left\{\begin{array}{l} {-\ln \Psi (a)+c\sum _{j=1}^{N}\delta _{j}  \to \min (a,b,\delta _{1} ,,...,\delta _{N} ),} \\ {y_{j} (a^{T} x_{j} +b)\ge 1-\delta _{j} ,\; \delta _{j} \ge 0,\; j=1,...,N.} \end{array}\right. \] 

б) Приняв дополнительное предположение об априорных вероятностях принадлежности случайно появляющегося объекта одному либо другому классу: 

\[q(1)=P\left(y(\omega )=1\right), q(-1)=P\left(y(\omega )=-1\right), q(1)+q(-1)=1, \] 
запишите апостериорную вероятность принадлежности объекта $\omega \in \Omega $ с вектором признаков $x\in {\rm {\mathbb R}}$ классу $y=1$ и $y=-1$: $p(y=1\, |\, x;\; a,b,c)$ и $p(y=-1\, |\, x;\; a,b,c)$.

в) Приняв дополнительное предположение, что априорной плотности распределения направляющего вектора разделяющей гиперплоскости является нормальным с нулевым математическим ожиданием и независимыми компонентами $a=(a_{1} ,...,a_{n} )$, характеризующимися одинаковыми дисперсиями $r_{1} =...=r_{n} =r$: 

\[\Psi (a)=\prod _{i=1}^{n}\frac{1}{r^{{1\mathord{\left/ {\vphantom {1 2}} \right. \kern-\nulldelimiterspace} 2} } (2\pi )^{{1\mathord{\left/ {\vphantom {1 2}} \right. \kern-\nulldelimiterspace} 2} } } \exp \left(-\frac{1}{2r} a_{i}^{2} \right),\] 
запишите критерий обучения.

г) Классическая детерминированная постановка задачи SVM имеет наглядное объяснение для выбора параметров гиперплоскости. Пусть поступила обучающая совокупность $\left\{(x_{j} ,y_{j} ),\; j=1,...,N\right\}$. Представляется естественной эвристическая идея выбрать такую разделяющую гиперплоскость $(a,b)$, которая правильно разделяет объекты двух классов: $y_{j} (a^{T} x_{j} +b)>0$ для всех $j=1,...,N$. 

Допустим, что для предъявленной обучающей совокупности разделяющая гиперплоскость существует. Но в этом случае существует континуум разделяющих гиперплоскостей. Идея В.Н. Вапника заключается в выборе той из них, которая обеспечивает наибольший «зазор» между гиперплоскостью и ближайшими точками обучающей совокупности как одного, таки другого класса $y_{j} (a^{T} x_{j} +b)\ge \varepsilon >0$. Правда, величина зазора $\varepsilon $ условна, и определяется еще и нормой направляющего вектора, поэтому задача формулируется в виде задачи условной оптимизации: 

\[y_{j} (a^{T} x_{j} +b)\ge \varepsilon \to \max_{a,b},\; j=1,\ldots,N, \; a^{T} a=1.  \] 

Такая концепция обучения названа концепцией оптимальной разделяющей гиперплоскости. 

Или эквивалентная формулировка задачи поиска оптимальной разделяющей гиперплоскости: 

\[\left\{\begin{array}{l} {a^{T} a\to \min ,} \\ {y_{j} (a^{T} x_{j} +b)\ge 1,\; j=1,...,N.} \end{array}\right. \] 

Однако предъявленная обучающая совокупность может оказаться линейно неразделимой, и поставленная оптимизационная задача не будет иметь решения. В качестве еще одной эвристики В.Н. Вапник предложил в качестве компромисса «разрешить» некоторым точкам обучающей совокупности располагаться с «неправильной» стороны разделяющей гиперплоскости, потребовав, чтобы такой дефект был минимальным: 

\[\left\{\begin{array}{l} {J(a,b,\delta _{1} ,...,\delta _{N} )=a^{T} a+C\sum _{j=1}^{N}\delta _{j}  \to \min ,} \\ {y_{j} (a^{T} x_{j} +b)\ge 1-\delta _{j} ,\; \delta _{j} \ge 0,\; j=1,...,N,} \end{array}\right. 
\] 
где $C>0$ -- некоторый коэффициент, согласующий два, вообще говоря, взаимно противоречивых требования -- обеспечить как можно меньшее значение нормы направляющего вектора и как можно меньшую ошибку классификации в пределах обучающей совокупности. Сравните классическую постановку задачи SVM с вероятностной (из п. в), чему соответствует в вероятностной интерпретации задачи параметр $C$.

Записав двойственную задачу оптимизации и найдя множители Лагранжа, выпишите значение направляющего вектора оптимальной гиперплоскости. Обратите внимание, что направляющий вектор оптимальной гиперплоскости выражается как линейная комбинация векторов признаков только части объектов обучения, для которых множитель Лагранжа ненулевой, т.е. лежащих за границей поверхности разделяющей полосы, образованной разделяющей гиперплоскостью и обладающей шириной обратно пропорциональной $\Vert a\Vert$ (это подмножество объектов обучения и называется «опорным» откуда и происходит название метода обучения -- «метод опорных векторов» или «support vector machine»).
\end{problem}


\begin{problem}
Выбирая алгоритм классификации $a: \mathbb{X} \rightarrow \{0,1\}, a \in A$ ($A$ - некоторое семейство алгоритмов) при помощи обучающей (заранее известной) выборки $X$ ($|X| = l$), исследователей инетересует оценка частоты ошибок $a$ на будущих данных $\overline{X}$: $\nu(a, \overline{X})$, причем $\mathbb{X} =  X \sqcup \overline{X}$ Для того, чтобы было возможно получить эту оценку, необходимо сделать предположение о том, что элементы генеральной выборки $\mathbb{X}$ ($X \in \mathbb{X}$, $\overline{X} \in \mathbb{X}$) появляются в случайном порядке. Причем все $L!$ перестановок ($|\mathbb{X}| = L$) равновозможны. Иногда условие ослабляют и равновозможными считаются все $C_L^l$ разбиений. Пусть алгоритм $a$ допускает на генеральной совокупности $m$ ошибок: $n(a, \mathbb{X}) = m$, докажите, что в этом случае число ошибок в наблюдаемой подвыборке $n(a, X)$ подчиняется гипергеометрическому распределению:
\begin{center}
$P(n(a, X) = s) = \frac{C_m^s C_{L-m}^{l-s}}{C_L^l}$,
\end{center}
где $s\in \left\{s_0 = \max(0, m-k),..., \min(l,m)\right\}$.
\end{problem}

\begin{problem}
В условиях предыдущей задачи определим \textit{функционалы обобщающей способности} алогритма:
\begin{enumerate}
\item вероятность переобучения $Q_{\epsilon}(a, \mathbb{X}) = P(\nu(a, \overline{X}) - \nu(a,X) \geq \epsilon)$;
\item вероятность высокой частоты ошибок на контрольной выборке $R_{\epsilon}(a, \mathbb{X}) = P(\nu(a. \overline{X})\geq \epsilon)$;
\item средняя частота ошибок на скользящем контроле $C(a, \mathbb{X}) = \mathbb{E}\nu(a, \overline{X})$.
\end{enumerate}
Докажите, что если $n(a, \mathbb{X}) = m$, то $\forall \epsilon \in [0, 1]$: $C(a,\mathbb{X}) = \frac{m}{L}$, $Q_{\epsilon}(a, \mathbb{X}) = H_L^{l,m}$, $R_{\epsilon}(a, \mathbb{X}) = H_L^{l,m}$, где $H_L^{l,m}(z) = \sum_{s=s_0}^{\lfloor z \rfloor} h_L^{l, m}(s)$, а $h_L^{l, m}(s) =  \frac{C_m^s C_{L-m}^{l-s}}{C_L^l}$.
\end{problem}

\begin{problem} Пусть $\mu$ - метод выбора алгоритма по обучающей выборке $X$ (метод обучения), $I: \mathbb{X} \rightarrow \{0, 1\}$ - индикатор ошибки для заданного алгоритма, $A_{\epsilon}$ - множество векторов ошибок, порождаемых множеством алгоритмов $A$. Доказать, что $\forall \epsilon \in [0, 1]$:
\begin{center}
$Q(\mu, \mathbb{X}) \leq |A_{\epsilon}| \max_{m = 1,...,L} H_L^{l,m}(s_m(\epsilon))$, где $s_m(\epsilon) = \frac{l}{L}(m - k\epsilon)$.
\end{center}

\begin{ordre}
Для получения верхних оценок вероятности переобучения, не зависящих от метода, часто используется \textit{принцип равномерной сходимости}
\begin{center}
$Q(\mu, \mathbb{X}) \leq P( \max_{a \in A} ( \nu (a, \overline{X}) - \nu (a,X) )\geq \epsilon)$.
\end{center}
\end{ordre}
\end{problem}


\begin{problem}[Оценка вероятности переобучения]
В статистической теории переобучения центральным объектом анализа является задача минимизации математического ожидания функции штрафа:
\[\tag{1}
M(\alpha) = \Exp \lambda(x, \alpha) = \int \lambda(x, \alpha) dF(x) \to \min,
\]
где $\alpha \in \Omega$ -- набор параметров метода обучения, $F(x)$ -- функция распределения выборки, 
$0\leq \lambda(x, \alpha) \leq \Lambda$ -- некоторая функция, измеримая $\forall \alpha \in \Omega$ относительно меры $F(x)$.  
  
Ввиду того, что в большинстве практических задач $F(x)$ неизвестна, $M(\alpha)$  приближается эмпирическим риском: 
\[\tag{2}
M_{l}(\alpha) = \frac{1}{l}\sum \limits_{i=1}^l \lambda(X_i, \alpha) \to \min,
\] 
где $\{X_i\}_{i=1}^{l}$ -- выборка из распределения 
$F(x)$.

В задаче классификации обычно в качестве $M(\alpha)$  берется вероятность неправильной классификации с помощью алгоритма $a(f, \alpha): \mathbb{F} \to \mathbb{Y}$, где $x = (f,y)$, $f$ -- множество признаков, $y$ -- индекс класса, при этом 
\[
\lambda(x, \alpha) = I[a(f, \alpha) \neq y],
\] 
\[
M(\alpha) = \PR(A_\alpha) = \PR(a(f, \alpha) \neq y).
\]
В свою очередь $M_{l}(\alpha)$ равна частоте события  $A_\alpha$ при заданной выборке.

В качестве меры близости между оптимальными значениями параметров $\alpha_{min}$, $\alpha^*$  в задачах (1) и (2) естественно взять 
\[
M(\alpha^*) - M(\alpha_{min}) \leq 2 \sup\limits_{\alpha} |M(\alpha) - M_l(\alpha)|.  
\]    
Таким образом, возникает вопрос: имеет ли место равномерная сходимость  $M_{l}(\alpha)$ к $M(\alpha)$ 
по заданной системе событий $S$ (или же по параметру $\alpha$  задающему систему событий). В случае задачи классификации $S = \{A_\alpha\}$ и близость оптимальных параметров означает близость частот к вероятностям системы $S$. 

Применив ц.п.т., покажите, что для конечной системы $S$  $M_{l}(\alpha)$ равномерно сходится к $M(\alpha)$.

Основная идея, на которой строятся условия равномерной сходимости для бесконечной системы $S$, состоит в разбиении $S$ на конечное число классов эквивалентности так, что в каждом классе события неотличимы относительно выборки. Для понимания применимости данной замены, проверьте, что близость $M_{l}(\alpha)$ к $M(\alpha)$
равносильна сходимости  $M_{l}(\alpha)$ на обучающей и тестовой выборках, а именно: 
\[
\PR \bigg\{\sup \limits_{\alpha \in \Omega} |M(\alpha) - M_l(\alpha)| > \varepsilon \bigg\} \leq 2 \PR \bigg\{\sup \limits_{\alpha \in \Omega} |M_l(\alpha) - M_{l,2l}(\alpha)| > \frac{\varepsilon}{2}  \bigg\} 
\] 
при $l > 2/ \varepsilon$.

Рассмотрим систему событий $S$ более общего вида $A(\alpha, c) = \{x: \lambda(x, \alpha) \geq c\}$ для всевозможных значений $\alpha \in \Omega$ и $c$. Обозначим за $\Delta^S(x_1,...., x_l)$ -- число классов  эквивалентности системы $S$. Введем функцию роста $m^S(l) = \max \Delta^S(x_1,..., x_l)$, где максимум берется по всем последовательностям $(x_1,...,x_l)$ длины $l$. Покажите, что 
\[\tag{3}
\PR \bigg\{\sup \limits_{\alpha \in \Omega} |M(\alpha) - M_l(\alpha)| > \varepsilon \bigg\} \leq 6m^S(2l)\exp\bigg\{-\frac{\varepsilon^2 (l -1)}{4\Lambda^2}\bigg\}.
\]
При помощи данной оценки докажите теорему Гливенко (см. задачу \ref{Fn}), взяв $S = \{x: x \leq \alpha\}$, а также ее обобщение на n-мерный случай, где  $S = \{x:  \langle x, \alpha \rangle \geq 0 \}, \; \alpha \neq 0$.
\end{problem}
\begin{remark} Для любой системы событий $S$ имеет место
\begin{center}
$m^S(l) = 2^l$ или 
\end{center}
\begin{center}
$m^S(l) \leq \sum \limits_{i=0}^{n-1}C_l^i$ 
т.е. $\exists n_0 \in \mathbb{N}: m^S(l) = O(l^{n_0})$.
\end{center}
Минимально возможное значение $n_0$ принято называть \textit{размерностью Вапника-Червоненкиса} (VC-размерность). Однако А.Я.Червоненкис предлагает называть её \textit{комбинаторной размерностью} $S$. Так, например, для множества всевозможных решающих правил в пространстве размерности $n$ комбинаторная размерность $n_0 = n + 1$. Если $m^S(l) = 2^l$, то говорят, что комбинаторная размерность бесконечна. Для рассматриваемого в задаче случая достаточным условием конечности комбинаторной размерности, как следствие равномерной сходимости с ростом объема выборки $M_{l}(\alpha)$ к  $M(\alpha)$, является то, что $\Omega$ -- компакт, $\lambda(x, \alpha)$ непрерывна по $\alpha$, $|\lambda(x, \alpha)| < K(x)$, где $\int K(x)dx < \infty$.

Наряду с достаточным условием (3) критерием равномерной сходимости является условие
\[
\lim \limits_{l \to \infty} \frac{H^S(l)}{l} = 0,
\] 
где $H^S(l) = \Exp \log_2 \Delta^S(x_1,...., x_l)$ -- энтропия системы $S$ относительно выборок длины $l$.

Отметим, что оценка (3) в большинстве случаев является чрезмерно завышенной (на несколько порядков) и поэтому не может быть использована для подсчета достаточного размера обучающей выборки на практике.     
\end{remark}


\begin{comment}
НАПОМИНАЮ ПРО ЭНТРОПИЙНЫЕ ОЦЕНКИ СНИЗУ (Концовки Лекций 11 и 12 Червоненкиса) - это полезно в виде замечания добавить!

НАПОМИНАЮ ПРО И КОНЦЕНТРАЦИЮ МЕРЫ ДЛЯ ПЕРЕСТАНОВОК И ЕЁ ИСПОЛЬЗОВАНИЕ ДЛЯ АСИМПТОТИЧЕСКОГО ОЦЕНИВАНИЯ ПЕРЕОБУЧЕНИЯ


CЛЕДУЮЩИЕ ЗАДАЧИ НУЖДАЮТСЯ В ПЕРЕФОРМУЛИРОВКИ, И ЛОГИЧНО ВСЕ "ТРИ КИТА СПОКОЙНОГО", И ВСЕ ЧТО ИЗ СЕКЕЯ, ВКЛЮЧАЯ ТО ЧТО НИЖЕ, ВСТАВИТЬ ПЕРЕД КОМБИНАТОРНОЙ ТЕОРИЕ ОБУЧЕНИЯ

\begin{problem}[Оптимальные оценки]
Назовем оценку $T_n^*$ оптимальной в классе всех оценок $\{\theta_n^*\}$, если она удовлетворяет следующему неравенству: $\mathbb{E}_{\theta}w(T_n^*, \theta) \leq \mathbb{E}_{\theta}w(\theta_n^*, \theta)$, для любого $\theta \in \Theta$, где $w(\cdot, \cdot)$ - функция потерь. Несложно видеть, что такая оценка не может быть построена. Р.Фишер предложил рассматривать только те асимптотически нормальные оценки, для которых равномерно в $\Theta$ минимальна дисперсия предельного нормального закона. Эффективность таких оценок принято считать равной единице. Оказывается, что можно привести оценку такую оценку, что найдется $\theta \in \Theta$, для которой эффективность превысит еденицу. Приведетие пример.
\begin{ordre}
Одним из возможных является \textit{пример Ходжеса-ЛеКама}, в котором строится \textit{суперэффективная оценка} для квадратичной функции потерь $\{T_n\}$:\\ 
$\lim_{n \to \infty} \mathbb{E}_{\theta}[\sqrt{n}(T_n - \theta)]^2 \leq I^{-1}(\theta)$, $\theta \in \Theta$ и существует $\theta$, при которой выполнено строгое неравенство.
Пусть $X_1,...,X_n$ - независимые в совокупности нормально распределенные случайные величины, $X_i \in N(0, \theta)$, $\theta \in \mathbb{R}$. Из оценки параметра $\theta$ методом максимального правдоподобия $\tilde{\theta} = \frac{1}{n}\sum_{i = 1}^n X_i$ может быть получена суперэффективная оценка. 
\end{ordre}
\end{problem}

\begin{problem}[Парадокс Стейна]
Пусть $\vec{X} \in N(\vec{\theta},I)$ нормально распределенный случайный вектор, $\vec{\theta} \in \mathbb{R}^n$, $n\geq 3$. Оказывается, что полученная при помощи минимизации квадратичного риска $L(\tilde{\theta}, \vec{\theta}) = \sum_{i=1}^n|\theta_i - \tilde{\theta}_i|^2$ оценка $\tilde{\theta} = \vec{X}$ может быть улучшена. Приведите пример такой оценки.
\begin{ordre}
На оценке, предложенной Стейном и Джеймсом $\theta^* = (1 - \frac{n-2}{||X||^2})\vec{X}$ действительно достигается меньшее значение квадратичного риска. Докажите это.
Парадокс является следствием того, что в неравенстве Рао-Крамера мы отказываемся от условия $b(\theta) \equiv 0$ и, если $b'(\theta) < 0$, то $\mathbb{E}(\tilde{\theta} - \theta)$ может убывать значительно быстрее, чем дисперсия несмещенной оценки с минимальной диспресией.
\end{ordre}
\end{problem}


НЕ ПОНЯЛ ПРИЧЕМ ЗДЕСЬ Джеймс - ЕСЛИ СТАТЬЯ 1956 года Стейна?
\end{comment}


\begin{problem}
\label{emp_err}
Дана {\it обучающая выборка} $X =\{(X_i,Y_i)\}_{i=1}^l$, состоящая из $l$ независимых пар $(X_i,Y_i)$ из распределения $\mathbb{P}$ и некоторая {\it функция потерь} $\lambda \colon \mathbb{Y}^2\to [0,1]$, которая характеризует величину потерь при отнесении объекта класса $y\in\mathbb{Y}$ к классу $y'\in\mathbb{Y}$.
Рассматривается задача поиска алгоритма  $a(x)$, минимизирующего {\it средний риск}:
\begin{equation}
\label{eq:rm}
M(a)\equiv\mathrm{\mathbb{E}_{\mathbb{P}}}\bigl[ \lambda\bigl(Y, a(X)\bigr) \bigr] \to \min_{a \in A}.
\end{equation}
Поскольку распределение $\mathbb{P}$ неизвестно, задачу \eqref{eq:rm} часто заменяют задачей минимизации {\it эмпирического риска}:
\begin{equation}
\label{eq:erm}
M_l(a)\equiv \frac{1}{l}\sum_{i=1}^l \lambda \bigl(Y_i, h(X_i)\bigr) \to \min_{a \in A}.
\end{equation}
Таким образом встает вопрос о соотношении величин $M(a^*)$ и $M_l(a^*)$, где $a^*$ -- решение задачи \eqref{eq:erm}.

Пусть $\mathbb{Y}=\{-1,+1\}$,  $\lambda(y,y')=\mathrm{Int}(y\neq y')$ и $A = \{a_1,\dots, a_N\}$.
Докажите, что $\forall \delta > 0$ выполнено:
\[
\mathbb{P} \left( M(a^*) \leqslant M_l(a^*) + \sqrt{\frac{\log_2{\frac{N}{\delta}}}{2l}} \right)  \geq 1-\delta.
\]
\end{problem}

\begin{ordre}
Получите оценку для $\sup_{a\in A}\bigl( M(a) - M_l(a)\bigr)$ с помощью неравенства Хефдинга из раздела \ref{measure} и неравенства Буля: $\mathbb{P}\{A\cup B\}\leqslant \mathbb{P}\{A\} + \mathbb{P}\{B\}$.
\end{ordre}

\begin{remark}
Эта оценка без изменений обобщается на произвольное множество ответов $\mathbb{Y}$ и любую функцию потерь $\lambda \colon \mathbb{Y}^2\to[0,1]$.
В том числе она может использоваться в задачах регрессии ($\mathbb{Y} = \mathbb{R}$) с квадратичной функцией потерь $\lambda(y,y') = (y - y')^2$.
\end{remark}

\begin{problem}
В постановке задачи \ref{emp_err} предположим дополнительно, что существует  алгоритм $\hat{a}\colon \mathbb{X}\to\mathbb{Y} \in A$, такой что $\mathbb{P}\{Y = \hat{a}(X)\} = 1$. Такую упрощенную постановку принято называть реализуемым случаем без шума (noise-free realizable setting).

Докажите, что $\forall \delta > 0$ выполнено:
\[
\mathbb{P} \left(
M(a^*) \leqslant M_l(a^*)+ C\cdot\frac{\log\frac{N}{\delta}}{l}
\right)
\geq 1-\delta,
\]
где $C$ "--- некоторая универсальная константа.

\end{problem}


\begin{ordre}
Покажите, что $\forall \delta > 0$ и $\forall a \in A$ выполнено:
\[
\mathbb{P} \left(
M(a) \leqslant M_l(a) + \sqrt{\frac{2M(a)\log\frac{N}{\delta}}{l}} + \frac{2\log\frac{N}{\delta}}{3l}
\right)
\geq 1-\delta.
\]
Для этого воспользуйтесь неравенством Бернштейна из раздела \ref{measure} вместе с неравенством Буля.

\end{ordre}


\begin{problem}[Радемахеровская сложность]
{\it Радемахеровской сложностью} и {\it условной радемахеровской сложностью} множества $A$ при фиксированной функции потерь $\lambda$ назовем соответственно
\[
\mathcal{R}(\lambda,  A) = 
\mathbb{E}\left[ \sup_{a\in A}
\frac{1}{l}\sum_{i=1}^l \sigma_i \lambda\bigl(Y_i,a(X_i)\bigr)
\right],
\]
\[
\mathcal{R}_l(\lambda,  A) = 
\mathbb{E}\left[ \sup_{a \in A}
\frac{1}{l}\sum_{i=1}^l \sigma_i \lambda\bigl(Y_i,a(X_i)\bigr)\Big| X
\right],
\]
где $\sigma_1,\dots,\sigma_l$ "--- последовательность независимых радемахеровских случайных величин, принимающих значения $+1$ и $-1$ с вероятностями $1/2$. Математические ожидания берутся по всем случайным величинам.

Докажите, что $\forall \delta > 0$ выполнено:
\begin{equation}
\label{eq:radem}
\mathbb{P} \left(
M(a^*) \leqslant M_l(a^*) + 2\mathcal{R}(\lambda,  A) + \sqrt{\frac{\log \frac{1}{\delta}}{2l}}
\right)
\geq 1-\delta,
\end{equation}

\begin{equation}
\label{eq:condradem}
\mathbb{P} \left(
M(a^*) \leqslant M_l(a^*) + 2\mathcal{R}_n(\lambda,  A) + 3\sqrt{\frac{\log \frac{2}{\delta}}{2n}}
\right)
\geq 1-\delta.
\end{equation}

\end{problem}

\begin{ordre}

\begin{enumerate}

\item Докажите неравенство {\it симметризации}:
\[
\mathbb{E}\left[\sup_{a\in A}\bigl(M(a)  - M_l(a) \bigr)\right]
\leqslant
2\,\mathcal{R}(\lambda,  A).
\]
Для этого введите независимую копию обучающей выборки \\ $\{(X'_i,Y'_i)\}_{i=1}^l$;

\item Воспользуйтесь два раза неравенством ограниченных разностей для случайных величин $\sup_{a \in A}\bigl(M(a) - M_l(a)\bigr)$ и
$\mathcal{R}_l(\lambda,  A)$;

\item Объедините все результаты с помощью неравенства Буля.

\end{enumerate}

\end{ordre}

\fixme{
неравенством ограниченных разностей - не па эвидан
}

\begin{remark}
Обратим внимание, что оценки \eqref{eq:radem} и \eqref{eq:condradem} справедливы для любого класса $ A$, в том числе несчетного.
Случай задачи классификации и бинарной функции потерь был изучен ранее, и для него справедлива оценка  Вапника--Червоненкиса, которая использует другую комбинаторную меру сложности семейства $ A$, известную как {\it  размерность Вапника--Червоненкиса}.
Помимо того, оценка \eqref{eq:condradem}, в отличие от оценки \eqref{eq:radem}, полностью вычислима по обучающей выборке.
\end{remark}

\begin{problem}[Неравенство Талаграна]
На декартовом произведении $\mathbb{X}\times\mathbb{Y}$  задана вероятностная мера
$\mathbb{P}$,  $\{(X_i,Y_i)\}_{i=1}^n$ "--- i.i.d обучающая выборка из $\mathbb{P}$, {\it минимизатор эмпирического риска} $a^* = \arg\min_{a\in A} M_l(a)$. Наша цель -- оценить отличие среднего риска алгоритма  $a^*$ от  минимального среднего риска. 
Для этого введем понятие {\it избыточного риска}: 
\[\mathcal{E}(a^*) = M(a^*) - \min_{a\in A} M(a).\]

Получите следующую верхнюю границу, с которой работать удобнее нежели с самим избыточным риском:   
\begin{gather*}
\mathcal{E}(a^*) 
\leqslant{}\sup_{a_1, a_2 \in A} \bigl( (M - M_l)(a_1 - a_2)\bigr)
\leqslant{}\\
\tag{1}
{}\leqslant 2\sup_{a\in A} \bigl( (M(a) - M_l(a)\bigr).
\end{gather*}

Этим путем были получены ключевые в Теории Статистического Обучения (ТСО) оценки Вапника--Червоненкиса, позже "--- оценки, основанные на Радемахеровских сложностях.

Проверьте, что если множество алгоритмов $A$, в некотором смысле, не слишком ``сложно'' (например, имеет конечную VC"=размерность), то выражение в (1) имеет порядок $O(1/\sqrt{n})$.

Для конкретного распределения $\mathbb{P}$, скорее всего, только очень маленькая часть $A$ подходит для решения задачи минимизации $M(a)$. По этой причине оптимальнее брать $\sup$ по множеству:
\[
A(\delta) = \{a\in A \colon \mathcal{E}(a) \leqslant \delta\}.
\]
Приходим к следующему результату:
\[
\delta^* = \mathcal{E}(a^*) \leqslant 
\sup_{a_1, a_2 \in A(\delta^*)} \bigl( (M - M_l)(a_1-a_2)\bigr).
\]

Используя неравенство Буске (одна из версий неравенства Талаграна), докажите, что с вероятностью не меньше $1 - e^{-t}$ справедливо следующее:
\[
\tag{2}
\sup_{a_1, a_2 \in A(\delta)} \bigl( (M - M_l)(a_1- a_2)\bigr)
\leqslant
\phi_l(\delta) + \sqrt{
2\frac{t}{l}\bigl( D^2(\delta) + 2\phi_l(\delta)\bigr)
}
+\frac{t}{2l},
\]
где
\[
D(\delta) = \sup_{a_1, a_2 \in A(\delta)} \sqrt{M\bigl( (a_1 - a_2)^2\bigr)}, \] \[
\phi_l(\delta) = \mathbb{E} \left[\sup_{a_1, a_2 \in A(\delta)} \left| (M - M_l)(a_1 - a_2) \right|\right].
\]

\begin{remark}
Неравенство (2) дает оценку порядка $o(1/\sqrt{n})$.
Установление точного порядка зависит от конкретного выбора функции потерь и условий, накладываемых на распределение $\mathbb{P}$. В частности, часто в качестве такого условия берут Tsybakov's low noise condition. 
\end{remark}
\end{problem}