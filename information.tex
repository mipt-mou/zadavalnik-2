\section{Теория информации и кодирование}
\label{information}

\begin{comment}
\subsection{Основные определения}
Пусть $X$ - дискретная случайная величина, принимающая значения из конечного множества (алфавита) $A = \{a_1,..., a_{|A|}\}$. $P = \{\mathbb{P}\{X = a_i\} = p_{a_i}\}$ - вероятностное распределение $X$ на $A$.

\textit{Условной энтропией} двух с.в. $X \in P$ и $Y \in Q$ называется $H(P|Q) = \sum_{a \in A} \mathbb{P}(Y = a)H(P|Y=a)$, где $H(P|Y=a) = \sum_{a' \in A} \frac{\mathbb{P}(X = a', Y = a)}{\mathbb{P}(Y = a)} \log \frac{\mathbb{P}(X = a', Y = a)}{\mathbb{P}(Y = a)}$. Условная энтропия характеризует ту среднюю степень неопределнности, содержащейся в $X$, если имеется некоторая информация об $Y$.

\begin{definition} \textit{Словом} в алфавите $A$ будем называть реализацию последовательности случайных величин $X_1,...,X_n..$: $w = (x_1...x_n..)$, $x_i \in A$.
\end{definition}

\begin{definition} 
\textit{Энтропией} $H(X)$ случайной величины $X$ распределенной по закону $P$ называется:
\begin{center}
$H(X) = - \sum_{a \in A} p_a\log p_a$, где $\log = \log_2$.
\end{center}
Иногда вместо $H(X)$  используется запись $H(P)$.
Энтропия измеряется в битах и интерпретируется как мера неопределенности или информационного содержания случайной величины. Чем она больше, тем больше неопределенность. В качестве иллюстрации, читателю предлагается решить первую задачу.
\end{definition}
В случае нескольких случайных величин можно определить два тесно связанных понятия: \textit{условной энтропии} и \textit{совместной информации}.
\begin{definition}
\textit{Условной энтропией} двух с.в. $X$ и $Y$ называется $H(X|Y) = \sum_{a \in A} \mathbb{P}(Y = a)H(X|Y=a)$, где $H(X|Y=a) = \sum_{a' \in A} \frac{\mathbb{P}(X = a', Y = a)}{\mathbb{P}(Y = a)} \log \frac{\mathbb{P}(X = a', Y = a)}{\mathbb{P}(Y = a)}$. Условная энтропия характеризует ту среднюю степень неопределнности, содержащейся в $X$, если имеется некоторая информация об $Y$.
\end{definition}

\begin{definition}
\textit{Совместная информация} $I(X,Y) = H(X) - H(X|Y)$ определяет то, сколько информации об $X$ содержится в $Y$.
\end{definition}

\begin{definition}
\textit{Относительной энтропией} случайных величин $X \backsim P$ и $Y \backsim Q$ на множестве $A$ (или расстоянием Кульбака-Лейблера между ними) называется
\begin{center}
$KL(P||Q) = \sum_{a \in A} p_a \log \frac{p_a}{q_a}$. 
\end{center}
В статистике эта величина определяет то, насколько "неэффективно" использование распределения $Q$ для аппроксимации распределедния $P$, или как много дополнительных бит мы заплатим за такую аппроксимацию.
\end{definition}

\begin{definition}
\textit{Кодом} слова $w \in A^{len(w)}$ в алфавите $\Sigma$ называется отображение $C(w) : w \rightarrow \sigma$, $\sigma \in B^{len(\sigma)}$. $len(\sigma)$ - длина кодового слова.
\end{definition}

\subsection{Задачи}


ЧТО ТАКОЕ РЫЧАЖНЫЕ ВЕСЫ? ПОЧЕМУ В УСЛОВИИ ЗАДАЧИ СТОИТ СЛОВО ВОЗМОЖНО? В ЧЕМ ЗАКЛЮЧАЕТСЯ ВЗВЕШИВАНИЕ?

25 ПОТОМУ ЧТО ФАЛЬШИВОЙ МОНЕТЫ МОЖЕТ НЕ БЫТЬ?

"потому чтокаждый из 25-и исходов может быть закодирован двоичным словомуказанной длины." - и что?

см. две статьи в КВАНТе и то как об этом написано у Щепина в совместной книге с Верещагиным
\end{comment}

\begin{problem}
Имеется 12 монет, из них ровно одна фальшивая, которая может быть как легче, так и тяжелее настоящих.   Предложите алгоритм взвешиваний на чашечных весах без гирь, выявляющий за 3 взвешивания подделку, а также определяющий является ли фальшивая монетка тяжелее или легче настоящих.
\begin{ordre}
Любая из 12 монет может быть фальшивой, при этом быть легче, либо тяжелее настоящих. Для решения требуется с помощью взвешиваний  получить $\log_24$ бит информации (по Хартли), так как каждый  исходов может быть закодирован двоичным словом длины 24. С другой стороны, каждое взвешивание имеет три возможных исхода: выше левая чаша весов, выше правая или они в равновесии. То есть каждое взвешивание дает не более $\log3$ бит информации. Соответственно, требуемое число взвешеваний не может быть мееньше $\frac{\log24}{\log3} = 3$. 

Для решения этой задачи и последующих полезно ознакомиться с книгой Н.К. Верещагин, Е.В. Щепин Информация, кодирование и предсказание. -- М.:  МЦНМО, 2012, а также см. метод Дайсона в статье Г. Шестопала ``Как обнаружить фальшивую монетку'' в журнале Квант 1979, номер 10.
\end{ordre}
\end{problem}

\begin{problem}
Патриций решил устроить праздник и для этого приготовил 240 бочек вина. Однако к нему пробрался недоброжелатель, который подсыпал яд в одну из бочек. Про яд известно, что человек, его выпивший, умирает в течение (не «через»!) 24 часов. До праздника осталось два дня, то есть 48 часов. У патриция есть пять собак, которыми он готов пожертвовать, чтобы узнать в какой именно бочке яд.
Как патрицию вычислить отравленную бочку?
\end{problem}

\begin{problem}[Цена информации] Имеется неизвестное число от $1$ до $n$, $n \geq 2$. Разрешается задавать любые вопросы с ответами ДА/НЕТ. При этом при ответе ДА игрок платит 1 рубль, а при ответе НЕТ - 2 рубля. Сколько необходимо и достаточно заплатить для отгадывания числа?
\end{problem}


\begin{problem}[Аксиоматическое определение энтропии]
Рассмотрим множество функций, заданных на единичном симплексе. Докажите, что существует единственная (с точностью до множителя) функция, удовлетворяющая нижеперечисленным требованиям. Она имеет вид $H(P) = -\sum_{i=1}^n p_i \log p_i$ и используется в качестве количественной характеристики меры неопределенности.
\begin{enumerate}
\item Значение функции $H(P)$ не меняется при перестановке чисел ${p_{a_1},..., p_{a_n}}$,
\item $H(P)$ непрерывная функция,
\item выполняется равенство
\begin{center}
$H(p_1,...,p_n) = H(p_1 + p_2, p_3,..., p_n) + (p_1 + p_2) H(\frac{p_1}{p_1+p_2},\frac{p_2}{p_1+p_2} )$.
\end{center}
То есть неопределенность в исходе опыта не зависит от того, осуществляется ли выбор среди всех возможных альтернатив одномоментно или в несколько этапов.
\end{enumerate}
\end{problem}

\begin{problem}
\begin{enumerate}
\item Ф.М. Достоевский решил изменить своим привычкам и отправился на скачки. У него есть предварительные (априорные) данные о том, какие шансы на победу имеет каждая из восьми лошадей-участниц: $(\frac{1}{2}, \frac{1}{4}, \frac{1}{8}, \frac{1}{16}, \frac{1}{64}, \frac{1}{64}, \frac{1}{64}, \frac{1}{64})$. Оцените энтропию, которая содержится в таких данных. 
\item Сравните результат со случаем, когда все исходы равновероятны. Какое из двух респределений содержит больше информации?
\item Докажите в общем случае, что из всех дискретных распределений на конечном множестве $A$, наибольшей энтропией обладает равномерное.
\end{enumerate}
\end{problem}

\begin{comment}
\begin{problem}
ТО КАК СЕЙЧАС ПЛОХО, см. 

http://dcam.mipt.ru/students/study/stohanaliz-files/ Задание по теории вероятнсотей 2010

Эта таблица ничуть не помогает в восприятии:(

В таблице приведен прогноз погоды в г. Долгопрудный:
$p$ - вероятность наличия/отсутствия осадков, $q$ - вероятность того, что прогноз окажется верным.
\begin{table}[h]
\caption{Прогноз погоды}
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
 &$p_{rain}$  &$p_{fine}$ & $q_{rain}$ & $q_{fine}$  \\
\hline
$15$ июня & $0.4$ & $0.6$ & $3/5$ & $4/5$\\
\hline
$15$ октября & $0.8$ & $0.2$ & $9/10$ & $1/2$\\
\hline
\end{tabular}
\end{center}
\end{table}
В какой из указанных двух дней прогноз дает нам больше информации о реальной погоде?
\end{problem}
\end{comment}
\begin{problem}
Пусть для некоторого населенного пункта вероятность того, что 15 июня будет дождь, равна 0.4, а вероятность того, что дождя не будет, равна 0.6. Для этого же пункта вероятность дождя 15 октября равна 0.8, а вероятность отсутствия осадков равна 0.2. Предположим, что определенный метод прогноза погоды 15 июня оказывается верным в $3/5$ всех тех случаев, когда предсказывается дождь и в $4/5$ тех случаев, в которых прогнозируется отсутствие осадков. Применительно к погоде на 15 октября этот метод оказывается правильным в 9 из 10 случаев, когда предсказывается дождь, и в половине случаев, когда предсказывается его отсутствие. В какой из указанных двух дней прогноз дает нам больше информации о реальной погоде?
\end{problem}


\begin{problem}[Задача о шляпах. Тодд Эберт, 1998]

Трех игроков отводят в комнату, где на них надевают (случайно и независимо) белые и черные шляпы. Каждый видит 
цвет других шляп и должен написать на бумажке одно из трех слов: <<белый>>, <<черный>>, <<пас>> 
(не советуясь с другими и не показывая им свою бумажку). Команда выигрывает, если хотя бы один из игроков назвал правильный 
цвет своей шляпы и ни один не назвал неправильного. Как им сговориться, чтобы увеличить шансы? Оптимальна ли предложенная Вами стратегия?
Решите эту же задачу, если игроков $n=2^m -1$ $( m\in {\mathbb N} )$. 
\end{problem}

\begin{ordre}
Воспользуйтесь кодом Хэмминга.
Докажем для случая трех игроков, что вероятность выигрыша не может превышать $3/4$. 

Единственная информация, которой владеет $i$-й игрок --- это цвета шляп двух других. Поэтому стратегия для $i$-го игрока должна зависеть 
только от этих двух цветов. В каждом случае имется три варианта ответа для игрока: $0$, $1$ или <<пас>>, т.е. всего $3^{12}$ различных 
стратегий. Поскольку есть $8$ вариантов расположения шляп на игроках, более выгодная стратегия должна обеспечивать выигрыш в $7$ вариантах. 
Тогда один из игроков должен угадать свой цвет в $3$ ситуациях. Значит, имеются для него ответы $\alpha_{i_1 j_1}$, 
$\alpha_{i_2 j_2}$, не являющиеся пасами. Но тогда в ситуациях $\overline{\alpha_{i_1 j_1}} i_1 j_1$ и 
$\overline{\alpha_{i_2 j_2}} i_2 j_2$ он ошибется, что противоречит предположению о $7$ выигрышных ситуациях. 

Таким образом, максимальная вероятность выигрыша не превышает $3/4$.
Обобщите это рассуждение на случай $n = 2^m - 1$.
\end{ordre}

\begin{problem}[Граница Эдгара Гилберта]
Для обеспечения помехоустойчивости кода при передачи информации, вместо исходного $k$-буквенного сообщения, передается $n$-буквенное ($n>k$). Возникает вопрос, при каких значениях параметров $q = |\Sigma|$ - размер алфавита, $k$, $n$, $l$ существует код $F: \Sigma^k \rightarrow \Sigma^n$, исправляющий  $l$ ошибок, и как его построить? 
Достаточное условие существования кода дает так называемая \textit{граница Гилберта}, которая описана ниже. 

Пространство кодовых слов содержит всего $q^n$ элементов. 
Назовем шаром радиуса $e$ с центром в слове $x\in \Sigma^n$ множество слов, отличающихся от $x$ не более чем в $l$ позициях.
Будем выбирать кодовые слова одно за другим произвольным образом, следя за тем, чтобы расстояния (по Хэммингу) между ними были больше $2l$. В какой-то момент пространство окажется полностью покрытым шарами, каждый из которых состоит из $V_q(2l, n)$ элементов, где $V_q(2l, n)$ - объем шара радиуса $2l$. Тогда при выполнении условия:
\[
(q^k - 1) V_q(2e, n) < q^n
\] 
существует код с параметрами  $q$, $k$, $n$, $l$.
Используя формулу Стирлинга ($k! \approx (k/е)^k$), оцените число элементов в шаре $V_q(2l, n)$. В случае $q=2$ последняя оценка легко получается применением оценок больших уклонений биномиальной случайно величины.

С помощью идеи {\it случайного кодирования} можно посторить код, с точностью до двух битов, реализующий границу Гилберта с большой вероятностью. Для этого случайно и независимо выбираются $N$ кодовых слов $\xi_1, \ldots, \xi_N$ в пространстве $\Sigma^n$. 

Найдите достаточное условие на параметры  $q$, $N$, $n$, $e$, при котором среди сгенерированных кодовых слов менее половины в среднем (усреднение по выбору случайного кода) будут иметь ``ближайшего соседа''  на расстоянии, не превышающем  $2l$.

\begin{remark}
В контексте это задачи полезно познакомиться с брошюрой Ромащенко А., Румянцев А., Шень А. Заметки по теории кодирования. -- М.: МЦНМО, 2011.
\end{remark}

\end{problem} 


\begin{problem}
Пусть $\{X_i\}_{i=1}^n$ -- независимые в совокупности одинаково распределенные случайные величины с распределением $P = \{p_{a}\}$, на конечном множестве $A = \{a\}$. Доказать, что 
\begin{center}
$-\frac{1}{n} \sum_{i = 1}^n \log \biggl ( P(X_i) \biggr ) \xrightarrow[n \to \infty]{\mathbb{P}} H(P)$
\end{center}
Или, другими словами, для любых $\delta, \varepsilon > 0$ найдется $n_0$ такое что для всех $n \geq n_0$:
\begin{center}
$\mathbb{P}(|-\frac{1}{n} \sum_{i = 1}^n \log P(X_i) - H(P)| < \delta) > 1-\varepsilon$
\end{center}

\begin{ordre}
Воспользутесь тем, что $-\mathbb{E} \log P(X) = H(P)$.
\end{ordre}
\end{problem}

\begin{remark} При достаточно больших значениях $n$ можно определить множество \textit{типичных последовательностей} или \textit{слов}, энтропия которых близка к истинной энтропии распределения $P$. Вероятность появления слова $w$
\begin{center}
$p_w = p_{x_1}...p_{x_n} = 2^{-n (-\frac{1}{n} \sum_{i = 1}^n \log P(X_i))}$.
\end{center}
\textit{Множеством $\delta$-типичных $n$-буквенных слов} в модели, где буквы появляются независимо, назовем $T_{\delta}^{(n)}$:
$$
T_{\delta}^{(n)} = \{w: 2^{-n(H(P) + \delta)} < p_w < 2^{-n(H(P) - \delta)} \}
$$
\end{remark}

\begin{problem}[Асимптотическая равнораспределенность]
Докажите, что:
\begin{enumerate}
\item мощность множества типичных слов ограничено: \[|T_{\delta}^{(n)}| \leq 2^{n(H(X) + \delta)};\]
\item $|T_{\delta}^{(n)}| \geq (1-\varepsilon)2^{n(H(X) + \delta)}$ для достаточно больших $n$;
\item вероятность нетипичности $w$: $\mathbb{P}\{w \not\in T_{\delta}^{(n)} \} \to 0, \; n\to\infty$.
\end{enumerate}
\end{problem}

\begin{remark} Идея о типичных последовательностях лежит в основе кодирования. Например, $\delta$-типичные $n$-буквенные слова кодируются при помощи двоичных последовательностей длины $n(H(X) + \delta)$, нетипичные отбрасываются или представляются одним и тем же добавочным символом. Очевидно, что при декодировании (восстановлении) вероятность ошибки не превысит $\varepsilon$.
\end{remark}
\begin{comment}
\begin{problem} Рассмотрите связь между доказательством принципа асимптотической равнораспределенности и эквивалентностью (для больших систем)энтропий Больцмана и Гиббса.
\end{problem}
\end{comment}

\begin{comment}
\begin{problem}
Всего существует $2^{n\log m}$ $n$-буквенных случайных текстов над алфавитом $(x_1,\ldots,x_m)$. Для их кодирования понадобиться $n\log m$ бит. Предполагая, что все буквы появляются независимо друг от друга по закону $(p_1,\ldots,p_m)$, который в общем случае не является равномерным, предложите лучший способ кодирования, основанный 
на законе больших чисел.
\begin{ordre}
Воспользуйтесь результатами, полученными в предыдущих задачах.
\end{ordre}
\end{problem}

\begin{remark}
Необходимым и достаточным условием существования префиксных кодов является неравенство Крафта -- Макмиллана. 
Пусть $n_1,..., n_k$ - длины кодовых слов, тогда $\sum_{i=1}^k 2^{-n_i} \leq 1$.
Тогда задача о построении кода минимальной средней длины может быть сформулирована следующим образом:
задан вектор $(p_1,...,p_k)$, $\sum p_i = 1$. Найти $n_1,..., n_k$, $n_i > 0$, для которых выполнено
неравенство Крафта -- Макмиллана, а $\sum_{i=1}^k p_in_i \rightarrow \min$, где минимум берется по всем возможным наборам
$n_1,..., n_k$.
Решением этой оптимизационной задачи является \textit{код Хаффмена}. 

\end{remark}
\end{comment}

\begin{problem}
Рассмотрим $n$-буквенные слова, порождаемые следующей моделью: появление каждой буквы $x_i \in A$ ($|A| = m$ - алфавит) не зависит от контекста и подчиняется закону распределения $P = \{p_a, a \in A\}$. Всего существует $2^{n\log m}$ таких слов, поэтому каждое из них может быть закодировано при помощи $n\log m$ бит информации. Если же предполагать, что распределение букв $P$ не является равномерным, то найдется лучший способ кодирования. Предложите такой способ.
\begin{comment}
\begin{ordre}
Воспользуйтесь результатами, полученными в предыдущих задачах.
\end{ordre}
\end{comment}
\end{problem}
\begin{comment}
НЕ ОЧЕНЬ УДАЧНОЕ ЗАМЕЧАНИЕ!

НУЖНО ПОЯСНИТЬ, ЧТО ТАКОЕ КОД?
В ЧЕМ РАЗЛИЧИЕ И ЧТО ТАКОЕ КОДЫ ХАФФМАНА И ШЕННОНА-ФАНО?
"Этот подход продемонстрирован в следующих задачах" - ЭТО НЕ ТАК!
\end{comment}
\begin{remark}
Во многих приложениях код должен быть не только однозначно декодируем, но и \textit{оптимален} в смысле минимальности средней длины. Необходимым (а в  случае префиксных кодов и достаточным) условием для выполнения первого требования является \textit{неравенство Крафта -- Макмиллана}: пусть $l(a)$ - длина кодового слова для буквы $a \in A$, тогда $\sum_{a \in A} 2^{-l(a)} \leq 1$.

Среди всех кодов, удовлетворяющих неравенству Крафта -- Макмиллана найдется код минимальной средней длины: $\sum_{a \in A} p_{a}l(a)\rightarrow \min$, при условии  $\sum_{a \in A} 2^{-l(a)} \leq 1$.  Для этого можно, например, воспользоваться методом множителей Лагранжа: $$
\sum_{a \in A} p_{a}l(a) + \lambda (\sum_{a \in A} 2^{-l(a)} - 1)\rightarrow \min.
$$ Решением является набор $\{l_{opt}(a) = -\log{p(a)}, a \in A\}$, а величина $-\log{p(a)}$ также называется \textit{собственной информацией}. Тогда минимальная средняя длина кода $-\sum_{a \in A}p(a)\log{p(a)} = H(P)$ это ни что иное, как энтропия.
\end{remark}

\begin{problem}[Оценка энтропии марковской цепи]
Приведем еще одну модель генерирования слов над алфавитом $A = \{a_1,...,a_m\}$. Текст моделируется стационарной конечной цепью Маркова, порождающей слова вида  $X_1,\ldots,X_n \in \{A\}^{n}$.   
Вероятность появления $j$-й буквы зависит только от того, какая буква стоит перед ней: $\PR(X_k = a_j| X_{k-1} = a_i) = p_{ij} > 0$. Стационарность означает, что $\forall k: \; P(X_k = a_j) = p_j$, причем, пользуясь формулой условной вероятности, $p_j$ можно представить как $p_j = \sum_{i=1}^m p_i p_{ij}$. 

Энтропия $X_k$ при фиксированной $(k-1)$-й букве определяется как $H(X_k|X_{k-1} = a_i) = -\sum_{j = 1}^m p_{ij}\log{p_{ij}}$, а условная энтропия $X_k$ при условии, что $X_{k-1}$ станет известным перед генерацией $X_{k-1}$, определяется  как $H = H(X_k|X_{k-1}) = -\sum_{i=1}^m p_i \sum_{j=1}^m p_{ij}\log{p_{ij}}$. Энтропия всей цепочки случайных величин в силу марковского свойства равна
\[
H(X_1,..., X_n) = H(X_1) + H(X_2|X_{1}) + \ldots + H(X_n|X_{n-1}) \sim n H.
\]

Пусть $W_n = (X_1,..., X_n)$ -- некоторое слово, порождаемое описанной моделью, докажите что:
\begin{enumerate}

\item $\frac{-\log \PR(W_n)}{n} \overset{p}{\longrightarrow} H$, т.е. все слова $W_n$  могут быть разбиты на два множества: для первого множества \textit{типичных} слов $|\frac{-\log P(W_n)}{n} - H| < \delta(n)$, для второго -- сумма вероятностей элементов сходится к 0 при $n\rightarrow \infty$;

\item Обозначим через $M_\alpha(n)$ максимальное количество значений $W_n$ с суммарной вероятностью не более $\alpha$. Докажите, что
\[
\forall \alpha: \;  \frac{M_\alpha(n)}{n} \to H, \quad n \to \infty.
\]

\end{enumerate}
\end{problem}

\begin{remark}
Пусть длина алфавита $m = 2^N$, а длина слова -- $n$. Пункт б) утверждает, что найдется код, с помощью которого с высокой вероятностью исходное сообщение $W_n$ может быть передано в $N/H \geq 1$ раз более коротким сообщением, чем при кодировании при помощи двоичных слов, когда каждому слову $W_n$ ставится в однозначное соответствие двоичная цепочка длины $2^n = 2^{nN}$.
\end{remark}


\begin{comment}
Написать о роли условной энтропии в оценке энтропии марковского источника. Эмпирические оценки распределения МЦ и "штраф" за аппроксимацию, равный расстояюни КЛ.

НАМНОГО ПОДРОБНЕЕ СТОИТ РАСПИСАТЬ ЗАДАЧУ про Оценка энтропии марковской цепи

ЕСЛИ ЧЕСТНО, Я ПЛОХО ПОНИМАЮ СМЫСЛ СЛЕДУЮЩЕЙ ЗАДАЧИ:(

\begin{problem}[Оценка энтропии русского языка]
Рассмотрим алфавит $A$ состоящий из $34$х букв: $33$ буквы русского алфавита и пробел. На каждом шаге игроку необходимо угадать следующую букву текста, которая будет открыта, при условии, что он видит все буквы, открытые ранее. За каждую правильную догадку игрок получает $34$ рубля. Предложите стратегию, позволяющую оценить снизу энтропию русского языка.
\begin{ordre} Необходимо на каждом шаге выбирать ту букву, появление которой наиболее вероятно с учетом предыдущей информации. Тогда на шаге $n$ выигрыш может быть записан как $S_n = (34)^n \hat{p}(X_1...X_n)$, где $\hat{p}(X_1,...,X_n) = \sum_{a \in A} \hat{p}(a|x_{n-1}...x_1)$. Показать, что $\mathbb{E}\frac{1}{n}S_n \leq \log(34) + H(X)$, $H(X)$ - энтропия русского языка.
\end{ordre}   
\end{problem}


\begin{problem}[Основная теорема теории кодирования Шеннона]
Пусть буква $X$ --- дискретная с.в., принимающая значения из алфавита $(x_1,\ldots,x_m)$ с вероятностями $(p_1,\ldots,p_m)$. 
Имеется случайный текст из $n$ букв $X$ (предполагается, что буквы в тексте независимы друг от друга). Общее количество таких 
текстов $2^{n\log m}$. Поэтому можно закодировать все эти слова, используя $n\log m$ бит. Однако, используя то обстоятельство, что 
$(p_1,\ldots,p_m)$ --- в общем случае неравномерное распределение, предложите лучший способ кодирования, основанный 
на усиленном законе больших чисел.
\end{problem}

\begin{ordre}
Пусть $\Omega=\{ \omega:\; \omega=(X_1,X_2,\ldots, X_n),\, X_i\in 1,2,\ldots,m\}$ --- пространство элементарных исходов. 
Вероятность появления слова $\omega=(X_1,X_2,\ldots, X_n)$ равна $p(\omega)=p_{X_1}\cdot\ldots\cdot p_{X_n}$. По теореме Колмогорова об 
у.з.б.ч. 
$$
-\frac{1}{n}\log p(\omega)=-\frac{1}{n}\sum\limits_{i=1}^{n}\log p_{X_i} \xrightarrow{\text{ п.н. }} 
-{\mathbb E}p(\omega)=-\sum\limits_{i=1}^{m}p_i\log p_i=H(p) 
$$
В частности, $\frac{S_n}{n}\xrightarrow{P}H(p)$, где $S_n=-\log p(\omega)=-\sum\limits_{i=1}^{n}\log p_{X_i}$. Это можно записать в виде 

$$
{\mathbb P}\Bigl( \Bigl| \frac{S_n-nH(p)}{n}\Bigr|>\delta \Bigr)  \xrightarrow{n\to\infty} 0 
$$
\end{ordre}
\end{comment}

\begin{comment}
\begin{remark}
О связи совместная информации $I(x,y)$ и пропускной способности канала.  
\end{remark}

\begin{problem} \textit{Случайные коды.}
\end{problem}


\begin{remark} Стоит ли писать что на этом подходе основывается построение кодов оптимальной длины?
\end{remark}

\end{comment}

\begin{problem}[Неравенство Пинскера] 
\label{KL}
\textit{Относительной энтропией} двух распределений $P$ и $Q$ на множестве $A$ (или \textit{расстоянием Кульбака--Лейблера} между ними) называется
$\mathcal{KL}(P||Q) = \sum_{a \in A} p_a \log \frac{p_a}{q_a}$. 
\textit{Расстоянием по вариации} между двумя распределениями называется $||\mathbb{P}_1 - \mathbb{P}_2||_1 = \sum_{a \in A} |\mathbb{P}_1(a) - \mathbb{P}_2(a)|$. Доказать, что между ним и расстоянием Кульбака--Лейблера справедливо следующее соотношение:
$$\mathcal{KL}(\mathbb{P}_1||\mathbb{P}_2) \geq \frac{1}{2\ln2} ||\mathbb{P}_1 - \mathbb{P}_2||_1^2.$$
\end{problem}

\begin{problem}
%[Критическая размерность] 

 В уездном городе \textit{M} хотят опросить население с целью восстановления матрицы трудовых корреспонденций. Город разделен на $l\gg 1$ районов. Таким образом, число различных пар (место жительство)--(место работы) равно $m = l^2$. Именно эти пропорции (какая доля людей в городе $p_k$ соответствует корреспонденции с номером $k$): $p_k$, $k = 1,...,m$ и нужно определить, опрашивая случайно выбранных жителей города о том где они живут и работают. Используя формулу Стирлинга (для мультиномиального распределения) и неравенство Пинскера, определите какое количество людей достаточно опросить (постарайтесь оценить это число как можно точнее -- опросы стоят денег), чтобы имело место неравенство 
 $$\PR\left(\sum _{k=1}^{m}\left|\frac{n_{k} }{n} -p_{k} \right| \ge 0.05\right) = \PR\left(\frac{1}{n}\|\vec {n} - n \vec{p} \|_{1} \ge 0.05\right)\le 0.05,$$
Покажите, что справедливо аналогичное неравенство для $l_2$ нормы:
 \[
 \PR\left(\frac{1}{n}\|\vec {n} - n \vec{p} \|_{2} \ge \sqrt{\frac{8x}{n}}\right)
\le e^{-x}. 
 \]
 \begin{ordre}
 В первом неравенстве воспользуйтесь неравенствами концентрации меры из раздела \ref{measure}. Для второго неравенства докажите неравенство Хефдинга в Гильбертовом пространстве: пусть $X_1,\ldots,X_n$ -- независимые случайные вектора, причем $\Exp X_i = 0$, $\Vert X_i \Vert_2 < c/2$, $v = nc^2/4$, тогда при $t \ge \sqrt{v}$
 \[
 \PR\left(\left\Vert \sum_{i=1}^{n} X_i \right\Vert > t
 \right)  \le e^{-(t - \sqrt{v}) / 2v}.
 \]    
 Воспользуйтесь неравенством МакДиармида  и следующим свойством нормы
 \[
 \Exp \left\Vert \sum_{i=1}^{n} X_i \right\Vert \le  \sqrt{  \sum_{i=1}^{n} \Exp \Vert X_i \Vert^2   }.
 \]
 \end{ordre}

\begin{comment}
б) (Спокойный--Клочков) * Решите эту же задачу используя другие нормы: $l_2$, $l_\infty$, расстояние $KL$, используя другую нормировку $\left|n_{k}/(p_{k}n) - 1 \right|$, используя другие неравенства: Бернштейна, Буске, Спокойного (см. раздел 6); используя многомерную ц.п.т. и оценки скорости сходимости в ней (В.В. Сенатов). Рассматрите разные случаи: когда все $p_k$ приблизительно одного порядка и когда это не так. Сравните ответы. Как зависит минимально возможное значение $n = \sum _{k=1}^{m}n_{k}$ от $m$? В каком случае достаточно, чтобы $n\gg m^{3/2}$, в каком случае этого условия не достаточно? Чем чревато нарушение этого условия и что можно сделать, если мы не можем его обеспечить? Всегда ли достаточно, чтобы $n \gg m^3$? 
\end{comment}
\end{problem}


\begin{problem}
Для двух монеток, симметричной ($p = \frac{1}{2}$) и неправильной ($q = \frac{1 +\varepsilon}{2}$), требуется выяснить какая из них является симметричной. Покажите, что для выявления симметричной монетки потребуется $\Omega (1 / \varepsilon^2)$ бросаний. Рассмотрите также случай $p = \varepsilon$, $q = 2 \varepsilon$. В каком случае потребуется больше бросаний? 

\end{problem}

\begin{ordre}
Пусть $f(X_1,\ldots, X_n) \in [0, M]$ является некоторой статистикой, которую можно подсчитать для каждой из монеток и по разности значений идентифицировать монетки. Покажите, что 
\[
\bigg\vert \Exp_{ \mathbb{Q} }[ f(X_1,\ldots, X_n) ] - \Exp_{ \mathbb{P} }[ f(Y_1,\ldots, Y_n) ]  \bigg\vert 
\leq  M \Vert \mathbb{Q} - \mathbb{P} \Vert_1,
\]   
где $(X_1,\ldots, X_n) \in \mathbb{Q}$, $(Y_1,\ldots, Y_n) \in \mathbb{P}$. Далее, для оценки $ \Vert \mathbb{Q} - \mathbb{P} \Vert_1$ можно воспользоваться неравенством Пинскера (см. задачу \ref{KL}), а также цепным правилом для $\mathcal{KL}$ дивергениции (см. указание к задаче \ref{bandit_lower}).
\end{ordre}



\begin{comment}
\begin{remark} Необходимость построения нижних оценок возникает тогда, когда случайная последовательность генерируется одним из распределений $\{P_i\}$ (неизвестно каким), а наилучшая стратегия игрока зависит от вида истинного $P_k \in \{P_i\}$, о котором нет никакой информации  . 
\end{remark}


\begin{problem}[Многорукие бандиты--1]
\end{problem}

ЗАДАЧА ПРО МНОГОРУКИХ БАНДИТОВ НА ОЦЕНКУ СНИЗУ ПО 5-й ЛЕКЦИИИ Yishay Mansour (в рамках курса Advanced Topics in Machine Learning and Algorithmic Game Theory), в которой в двух местах есть опечатка вместо + там стоит -
\end{comment}
\begin{problem}$^*$
Есть $N$ ручек, с каждой из которых связана вероятность успеха $p_k$ (дергая $k$-ю ручку с вероятностью $p_k$ мы получим 1, а с вероятностью $1-p_k$ ничего).
Таким образом, выигрыш игрока при выборе $k$-й ручки есть $r_k \in \text{Be}(p_k)$.    
Вероятности $p_k$ не известны игроку. Игрок намеревается выполнить $T \gg N$ дерганий ручек. При выборе ручки на новом шаге можно использовать всю предысторию. Предложите стратегию, ``максимально близкую'' в среднем по размеру выигрыша к величине $T \max \limits_k p_k$.

\begin{remark} См. монографию Lugoshi G., Cesa-Bianchi N. Prediction, learning and games. New York: Cambridge University Press, 2006, а также S. Bubeck, N. Cesa-Bianchi. Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems. In Foundations and Trends in Machine Learning, Vol 5, 2012. 
\end{remark}
\end{problem}


\begin{problem}[Многорукие бандиты]
\label{bandit}
В модели \textit{стохастического многорукого бандита} имеется $K$ распределений $f_1, \ldots,f_K$ на множестве $\Omega = [0, 1]$, каждое из которых является распределением выигрыша в зависимости от выбора действия $a \in \{1,\ldots,K\}$. 
Игра повторяется $T \gg K$ раундов, причем на $t$-м раунде
\begin{enumerate}
\item игрок выбирает действие $A(t)$, исходя из результатов предыдущих раундов;
\item среда генерирует выигрыш $r_t$ из распределения  $f_{A(t)}$, независимо от предыдущих раундов.
\end{enumerate}   

Пусть $m_k = \Exp_{f_k} (r)$, $m^{*} = \max_{k} m_k$, $\triangle_k = m^{*} - m_k$  
Цель игрока состоит в минимизации следующей функции потерь
\[
\overline{R}_T = T m^{*} - \sum_{t=1}^{T} \Exp m_{A(t)} = \sum_{k=1}^{K} \triangle_k \Exp A_k(T),
\]
где $A_k(T)$ -- количество действий с номером $k$ за $T$ раундов.
Опишем используемую  стратегию игры (то есть алгоритм выбора действий~$A$).
В каждом раунде будем вычислять доверительные интервалы для оценок $\widehat{m}_1, \ldots, \widehat{m}_K$ и выбирать в следующем раунде действие, соответствующее максимальному значению верхней границы доверительного интервала.

Используя неравенство Маркова в экспоненциальной форме (метод Чернова), покажите 
что согласно описанной стратегии 
\[
A(t+1) \in \mathop{\arg\max} \limits_{k \in \{1,\ldots,K\}} \left[ 
\widehat{m}_{k, A_k(t)} + g^{-1} \left(\frac{\alpha \ln t}{ A(t)} \right)
\right],
\]  
при уровне значимости для доверительного интервала $1 / t^{\alpha}$, где $g$ определяется из условия 
\[
\Exp e^{\lambda |r - \Exp_{f_k} (r)|} \leq  \psi(\lambda), 
\quad
g(x) = \psi^{*}(x) = \sup_{\lambda} (\lambda x - \psi(\lambda)).  
\] 

\end{problem}


\begin{problem}
Докажите верхнюю оценку для стратегии, предложенной в задаче 
\ref{bandit}:
\[
\overline{R}_T \leq \sum_{k: \triangle_k > 0} \left(
\frac{\alpha \triangle_k}{ g(\triangle_k / 2) } \ln (T	) + \frac{\alpha}{\alpha - 2}
\right).
\]
Также установите для случая $f_k = \Be(p_k)$, что для любой стратегии $A$, при выполнении условия $\forall k: \triangle_k > 0 \to \Exp A_k(T) = o(T^{\gamma})$, $\gamma > 0$, справедлива нижняя оценка
\[
\mathop{\underline{\lim}}\limits_{T \to \infty} \frac{\overline{R}_T}{\ln(T)} \geq 
\sum_{k: \triangle_k > 0} 
\frac{\triangle_k}{\KL(m_k, m^{*}) }.
\]
\end{problem}

\begin{ordre} См. S. Bubeck, N. Cesa-Bianchi. Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems. In Foundations and Trends in Machine Learning, Vol 5, 2012. 
\end{ordre}


\begin{remark}
Последняя оценка говорит об асимптотической неулучшаемости стратегии из задачи \ref{bandit}. Для сравнения приведенных оценок можно воспользоваться неравенством
\[
\KL(m_k, m^{*}) = \KL(p_k, p^{*})  \leq \frac{(p_k - p^{*})^2}{p^{*} (1- p^{*})} =   \frac{\triangle_k^2}{p^{*} (1- p^{*})}.
\] 
\end{remark}


\begin{problem}
\label{bandit_lower}
Пусть параметры распределений $f_k = \Be(p_k)$:  $p_1,\ldots,p_K$ в модели из задачи \ref{bandit} принадлежат распределению $F$. Докажите, что сушествует такое $F$, что для любого детерминированного алгоритма игрока $A$ выполнено неравенство
\[
T \max \limits_k p_k - \sum_{t = 1}^{T} \Exp(r_t | A) \geq \frac{1}{20} \min(\sqrt{KT}, T) = \Omega (\sqrt{KT}).
\]
\end{problem}

\begin{ordre}
В качестве $F$ возьмите следуюшее распределение: все $p_k = 0.5$, за исключением одного $p_i = 0.5 + \varepsilon$, $i$ выбрано случайно равновероятно из множества $\{1,\ldots,K\}$. Установите тождество
\[
\Exp(r_t | A) = \frac{1}{2} + \frac{\varepsilon}{K} \sum_{j=1}^K \sum_{r} \big[A(t,r) = j \big] \PR(r|A, j=i),
\]
где $r \in \{0,1\}^T$, $[\cdot]$ -- индикаторная функция. Далее, воспользуйтесь 
соотношением
\[
\sum_{r} \big[A(t,r) = j \big] \PR_i(r) \leq \sum_{r} \big[A(t,r) = j \big] \PR_u(r) + \Vert \PR_i - \PR_u \Vert_1,
\]
где $\PR_i(r) = \PR(r|A, j=i)$, $\PR_u(r)$ -- распределение выигрышей при $p_1 = \ldots = p_K = 0.5$. Воспользуйтесь неравенством Пинскера и цепным разложением величины 
\[
\mathcal{KL}(p(x,y),q(x,y)) = \mathcal{KL}(p(x),q(x)) + \sum_{x} p(x) \mathcal{KL}(p(y|x),q(y|x)). 
\]

\end{ordre}


\begin{problem}
Пусть казино делает $n$ бросаний, используя распределение вероятностей на бинарных словах длины $n$ -- $p\left(x\right)$, где $x \in \left\{0,1\right\}^{n} $, известное игроку. При этом казино производит выплаты так, как если бы оно использовало распределение $q\left(x\right)$ (то есть выигранная ставка на 0, после выпадения последовательности исходов $x$ увеличивается в $\frac{q\left(x\right)}{q\left(x :+ 0\right)} $ раз, выигранная ставка на 1 -- увеличивается в $\frac{q\left(x\right)}{q\left(x :+ 1\right)} $ раз). Докажите, что у игрока есть стратегия, логарифм значения капитала которой равен расстоянию Кульбака--Лейблера (см. задачу \ref{KL})  между распределениями $p$ и $q$.
\end{problem}

\begin{comment}
ЗАМЕЧАНИЕ ВЕРЕЩАГИН И ЩЕПИН
\begin{problem} \textit{Неравенство больших уклонений}
Рассмотрим последовательность незовисимых одинаково распределенных случайных величин $X_1, ..., X_n$, $X_i~Be(q)$. Доказать, что $-\frac{1}{n}\log \mathbb{P}(\frac{1}{n}\sum X_i \geq p) \rightarrow p\log \frac{p}{q} + (1-p)\log \frac{1-p}{1-q} = KL((p, 1-p)||(q, 1-q))$.
\begin{ordre}
\end{ordre}
\end{problem}

\begin{remark} 
Этот результат является следствием теоремы Санова, которая позволяет оценивать вероятности больших уклонений.. Приведем ниже её упрощенную формулировку.\\
\textit{Теорема Санова}
\end{remark}

\end{comment}


\begin{problem} [Теорема Шеннона о пропускной способности канала с шумом] 

Канал (поток) связи с шумом описывается матрицей переходных вероятностей $p(Y|X)$, где $X$ и $Y$ -- случайные величины с распределениями $P$ (на множестве $A$ -- входной алфавит) и $Q$ (на множестве $B$ -- выходной алфавит) соответственно. Другими словами, $p(y|x)$ -- вероятность прочитать  символ $y$ из потока при условии, что в него был записан символ $x$.
Определим \textit{Шенноновское количество информации} как $I(P, Q) = H(P) + H(Q) - H(P,Q)$. Пропускной способностью такого канала называется величина $C= \max_{\{p_x\}}I(P;Q)$.
Пусть по каналу передаются слова $w_1,...,w_N$ длины $n$.
Разобьем множество кодовых слов в алфавите $Y$ на непересекающиеся области $V_0,...,V_N$. Если принятое слово $y \in V_j$, $j = \overline{1,N}$, то принимается решение о том, что было послано слово $w_j$. Если $y \in V_0$ то никакое определенное решение не принято. Введем \textit{среднюю вероятность ошибки} $$\overline{P}_{\varepsilon}(W, V) = \frac{1}{N} \sum_{i=1}^n (1 - p(V_i|w_i)).$$
Пусть $p_{\varepsilon}(n, N) = \min_{W, V} \overline{P}_{\varepsilon}(W, V)$, докажите что:
\begin{enumerate}
\item[а)] $p_{\varepsilon}(n, 2^{nR}) \rightarrow 0$, $R < C$;
\item[б)] $p_{\varepsilon}(n, 2^{nR}) \not\rightarrow 0$, $R > C$;
\item[в)] $p_{\varepsilon}(n, 2^{nR}) \rightarrow 1$, $R > C$,
\end{enumerate}
где $R = \frac{\log N}{n}$ -- скорость передачи.
\begin{remark}
Величина $H(P,Q) = -\sum_{a\in A} p(a)\sum_{b \in B}p(a, b)\log p(a, b)$ называется \textit{совместной энтропией} двух случайных величин.
\end{remark}
\end{problem} 
\begin{comment}
В ЗАМЕЧАНИИ ХОТЕЛОСЬ БЫ ВИДЕТЬ И СВЯЗЬ С ЗАДАЧЕЙ ПРО ГРАНИЦУ ГИЛБЕРТА, ГДЕ ИСПОЛЬЗУЕТС ЯСЛУЧАЙНОЕ КОДИРОВАНИЕ
ПОЛЕЗНО ЗДЕСЬ ЖЕ ОБЫГРАТЬ СТР, 133 теорему 10.11 Верещагин Щепин

Вообще стоит подробнее в заадчах осветить главу 11 этой книги!
\end{comment}


