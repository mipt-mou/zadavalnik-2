\section{Метод Монте-Карло}
\label{MK}

\begin{problem}
На плоскости дано ограниченное измеримое по Лебегу множество $S$. Требуется найти площадь (меру Лебега) этого множества с заданной точностью 
$\varepsilon$. 

Поскольку по условию множество ограничено, то вокруг него можно описать квадрат со стороной $a$. Выберем декартову систему координат 
в одной из вершин квадрата с осями, параллельными сторонам квадрата. Рассмотрим $n$  независимых с.в. $\{ X_k\}_{k=1}^{n}$,  имеющих 
одинаковое равномерное распределение в этом квадрате, т.е. $X_k\in R([0,a]^2)$. Введем с.в. 
$$
Y_k=I(X_k\in S)=\begin{cases}
1,\quad X_k\in S\\
0, \quad X_k\notin S
\end{cases} 
$$
Тогда $\{ Y_k\}_{k=1}^{n}$ --- независимые одинаково распределенные с.в.. Ясно, что $Y_k\in\Be(p(S))$. Следовательно, по у.з.б.ч. 
$$
\frac{Y_1+\ldots+Y_n}{n} \xrightarrow{\text{ п.н. }} {\mathbb E}(Y_1)=p(S)=\frac{\mu(S)}{a^2} \quad \text{ при  } n\to\infty . 
$$
Оцените сверху следующую вероятность
$$
{\mathbb P}\Bigl( \Bigl| \frac{Y_1+\ldots+Y_n}{n}-\frac{\mu(S)}{a^2}\Bigr|>\delta \Bigr) . 
$$
\end{problem}

\begin{ordre}
См. раздел 5, а также часть 2.
\end{ordre}


\begin{problem}[Вычисление значения интеграла]
\begin{enumerate}
\item Требуется вычислить с заданной точностью $\varepsilon $ и с заданной доверительной вероятностью $\gamma $ абсолютно сходящийся интеграл 
\[
J=\int _{\left[0,\; 1\right]^{m} }f\left({x}\right)d{x}.
\] Считайте, что $\forall \; \; {x}\in \left[0,\; 1\right]^{m} \to \left|f\left({x}\right)\right|\le 1$.

\begin{remark}
Введем случайный $m$-вектор ${X}\in R\left(\left[0,\; 1\right]^{m} \right)$ и с.в. $\xi =f\left({X}\right)$. Тогда $\Exp\xi =\int _{\left[0,\; 1\right]^{m} }f\left({x}\right)d{x} =J$. Поэтому получаем оценку интеграла $\bar{J}_{n} =\frac{1}{n} \sum _{k=1}^{n}f\left({x}^{k} \right) $, где ${x}^{k} $, $k=1,...,n$ -- повторная выборка значений случайного вектора ${X}$ (т.е. все ${x}^{k} $, $k=1,...,n$ -- независимы и одинаково распределены: также как и вектор $\vec{X}$). В задаче требуется оценить сверху число $n$ ($n\gg m$), начиная с которого $\mathbb{P}\left(\left|J-\bar{J}_{n} \right|\le \varepsilon \right)\ge \gamma $.
\end{remark}



\item Решите задачу из п. а) при дополнительном предположении липшецевости функции $f\left({x}\right)$, разбив единичный куб на $n=N^{m} $ одинаковых кубиков со стороной ${1\mathord{\left/ {\vphantom {1 N}} \right. \kern-\nulldelimiterspace} N} $, и используя оценку $\bar{J}_{n} =\frac{1}{n} \sum _{k=1}^{n}f\left({x}^{k} \right) $, где ${x}^{k} $ -- имеет равномерное распределение в \textit{k}-м кубике.

\item (метод включения особенности в плотность). Решите задачу п. а) не предполагая, что $f\left({x}\right)$ -- ограниченная функция на единичном кубе. Предложите способы уменьшения дисперсии полученной оценки интеграла. Как можно использовать информацию об особенностях функции $f\left({x}\right)$?

\end{enumerate}

\end{problem}
\begin{ordre}
См. книгу Соболь И.М. Численный метод Монте-Карло, М.: Наука, 1973, а также \cite{lagutin}.
\end{ordre}

\begin{problem}Стандартный способ моделирования с.в. -- {\it метод обратной функции}. Покажите, что если с.в. $\eta $ равномерно распределена на отрезке $\left[0,1\right]$, то с.в. $\xi =F^{-1} \left(\eta \right)$ имеет функцию распределения $F\left(x\right)$. Предполагается, что $F\left(x\right)$ непрерывна и строго монотонна. Как выглядит формула для моделирования с.в. из показательного распределения с функцией распределения $F\left(x\right)=\left(1-e^{-\lambda x} \right)I\left(x\geq 0\right)$?
\end{problem}

\begin{problem}
Пусть с.в. $\eta _{1} $, $\eta _{2} $ имеют равномерное распределение на отрезке $\left[0,1\right]$. Докажите, что с.в. $X$ и $Y$: $X=\sqrt{-2\ln \eta _{1} } \cos \left(2\pi \eta _{2} \right)$, $Y=\sqrt{-2\ln \eta _{1} } \sin \left(2\pi \eta _{2} \right)$ -- независимые и одинаково распределенные: стандартно нормально ${\rm {\mathcal N}}\left(0,1\right)$.

\begin{ordre}
Покажите, что
\[f_{XY} (x,y)=\frac{1}{\sqrt{2\pi } } e^{-\frac{x^{2} }{2} } \frac{1}{\sqrt{2\pi } } e^{-\frac{y^{2} }{2} } =\frac{1}{2\pi } e^{-\frac{x^{2} +y^{2} }{2} } .\] 
Перейдите к полярным координатам, не забыв о якобиане замены переменных.
\end{ordre}

\end{problem}


\begin{problem}

Если $X$ -- с.в., имеющая стандартное нормальное распределение, то $X^{-2} $ имеет устойчивую плотность (см. замечание к задаче \ref{bluzd_ust}, раздела 5):
\[\frac{1}{\sqrt{2\pi } } e^{-\frac{1}{2x} } x^{-\frac{3}{2} }, \quad x>0.\] 
Используя это, покажите, что если $X$ и $Y$ -- независимые нормально распределенные с.в. с нулевым математическим ожиданием и дисперсиями $\sigma _{1}^{2} $ и $\sigma _{2}^{2} $, то величина $Z=\frac{XY}{\sqrt{X^{2} +Y^{2} } } $ нормально распределена с дисперсией $\sigma _{3}^{2} $, такой, что $\frac{1}{\sigma _{3}^{2} } =\frac{1}{\sigma _{1}^{2} } +\frac{1}{\sigma _{2}^{2} } $.

\end{problem}


\begin{problem}[Теорема Бернштейна \cite{28}, \cite{21} т.2] 
\begin{enumerate}

\item С помощью неравенства Чебышёва установите следующий результат из анализа: 

\[
\forall \; \; f\in C\left[0,1\right]\to \left\| f_{n} -f\right\| _{C\left[0,1\right]} \xrightarrow[{n\to \infty }]{} 0,
\] 

\[
f_{n} \left(x\right)=\sum_{k=0}^{n}f\left(\frac{k}{n} \right) C_{n}^{k} x^{k} \left(1-x\right)^{n-k}. 
\]

\item Исходя из предыдущей задачи и п. а) предложите способ генерирования распределения с.в. $\xi $, имеющей плотность $f_{\xi } \left(x\right)$ с финитным носителем, для определенности, пусть носителем будет отрезок $\left[0,1\right]$.

\end{enumerate}

\end{problem}

\begin{problem}[Метод фон Неймана] 

Пусть с.в. $\xi $ распределена на отрезке $\left[a,b\right]$, причем ее плотность распределения ограничена: $\mathop{\max }\limits_{x\in \left[a,b\right]} f_{\xi } (x) < C$. Пусть с.в. $\eta _{1} $, $\eta _{2} $, \dots  -- независимы и равномерно распределены на $\left[0,1\right]$, $X_{i} =a+\left(b-a\right)\eta _{2i-1} $, $Y_{i} =C\eta _{2i} $, $i=1,2,...$, т.е. пары $\left(X_{i} ,Y_{i} \right)$ независимы и равномерно распределены в прямоугольнике $\left[a,b\right]\times \left[0,C\right]$. Обозначим через $\nu $ номер первой точки с координатами $\left(X_{i} ,Y_{i} \right)$, попавшей под график плотности $f_{\xi } (x)$, т.е. $\nu =\min \left\{i:\: Y_{i} \le f_{\xi } (X_{i} )\right\}$. Положим $X_{\nu } =\sum _{n=1}^{\infty }X_{n} I\left(\nu =n\right) $.

\begin{enumerate}
\item Покажите, что с.в. $X_{\nu } $ распределена так же как $\xi $.

\item Сколько в среднем точек $\left(X_{i} ,Y_{i} \right)$ потребуется «вбросить» в прямоугольник $\left[a,b\right]\times \left[0,C\right]$ для получения одного значения $\xi $?

\item Предложите модификацию рассмотренного метода для генерации дискретной случайной величины, принимающей значения $\lbrace 1, 2, ... , k \rbrace$ с одинаковой вероятностью, имея в распоряжении монету (генератор бинарной случайной величины).   
\end{enumerate}
\end{problem}

\begin{problem}
Как с помощью с.в. $\xi $, равномерно распределенной на отрезке $\left[0,1\right]$ ($\xi \in R\left[0,1\right]$), и симметричной монетки построить с.в. $X$, имеющую плотность распределения $f_{X} (x)=\frac{1}{4} \left(\frac{1}{\sqrt{x} } +\frac{1}{\sqrt{1-x} } \right)$, $x\in \left[0,1\right]$?
\end{problem}

\begin{problem}

Пусть $\xi $ распределена на $\left[0,1\right]$ с плотностью $f_{\xi } (x)$, представимой в виде степенного ряда $\sum _{k=0}^{\infty }a_{k} x^{k}  $ с $a_{k} \ge 0$. Положим $p_{k} ={a_{k} \mathord{\left/ {\vphantom {a_{k}  (k+1)}} \right. \kern-\nulldelimiterspace} (k+1)} $. Тогда $f_{\xi } (x)=\sum _{k=0}^{\infty }p_{k} \cdot (k+1)x^{k}  $. Примените \textit{метод суперпозиции} для моделирования с.в. $\xi $.

\begin{ordre}
\textit{Метод суперпозиции}:

\begin{enumerate}
\item Разыгрывается значение дискретной с.в., принимающей значения $k=0,1,2,...$ с вероятностями $p_{k} $;

\item Моделируется с.в. с функцией распределения $F_{k} (x) = x^{k+1}$, $x\in [0,1]$ (например, методом обратной функции).
\end{enumerate}

\end{ordre}

\end{problem}


\begin{problem}
Покажите, что в случае общего положения невозможно ``приготовить'' распределение дискретной с.в., принимающей $n$ значений менее чем за $O(n)$ операций. Однако единожды приготовив его, можно далее генерировать распределение этой с.в. за $O(\log n)$ операций. Более того, если потребуется перегенерировать распределение немного отличной с.в. (например, в которой поменялось несколько вероятностей исходов и, как следствие, нормировочный множитель всего распределения), то это также можно сделать за $O(\log n)$. Покажите, что если случайная величина принимает, например, разные значения, но с одинаковыми вероятностями, то ``приготовить распределение''  такой с.в. можно и за $O(\log n)$ операций. 
\end{problem}

\begin{problem}[Алгоритм Кнута–Яо]
С помощью бросаний симметричной монетки требуется сгенерировать распределение заданной дискретной с.в., принимающей конечное число значений. Обобщите описанную ниже схему на общий случай. Предположим, что нам нужно сгенерировать распределение с.в., принимающей три значения 1, 2, 3 с равными вероятностями 1/3. Действуем таким образом. Два раза кидаем монетку: если выпало 00, то считаем, что выпало значение 1, если 01, то 2, если 11, то 3. Если 10, то еще два раза кидаем монетку и повторяем рассуждения. Покажите, что можно сгенерировать распределение дискретной с.в., принимающей, вообще говоря, с разными вероятностями $n$ значений в среднем с помощью не более чем $\log_2 (n - 1) + 2$ подбрасываний симметричной монетки.
\end{problem}
\begin{remark}
См. Ермаков С.М. Метод Монте-Карло в вычислительной математике, М.: Бином, 2009.
\end{remark}

\begin{problem}[Метод Уокера]
Пусть случайная величина $\xi$ принимает значения $1, \ldots, n$ с вероятностями $p_1, \ldots, p_n$. Докажите, что генерация $\xi$ может быть осуществлена при помощи смеси дискретных распределений с двумя исходами и случайной величины $\xi'$, принимающей значения $1, \ldots, n$ с одинаковыми вероятностями. 
\end{problem}

\begin{ordre}
\begin{enumerate}
\item Покажите, что $\exists i, j \neq i: \;  p_i \leq 1/n, \; p_i + p_j > 1/n;$
\item Зафиксируем пару $i, j$ с указанным свойством и определим случайную величину $\eta^{(1)}$ с двумя исходами:
\[
\mathbb{P}(\eta^{(1)}=i)=q_i^{(1)} = n p_i, \; \mathbb{P}(\eta^{(1)}=j)=q_j^{(1)} = 1 - n p_i, \; q_k^{(1)} = 0.   
\]
Покажите, что $\xi$ может быть представлена в виде смеси $\eta^{1}$ и случайной величины $\zeta^{(1)}$, принимающей значения из множества $\{1, \ldots, n\} \backslash i$. 
\item Повторите рассуждения пункта б) для с.в. $\zeta^{(1)}$ и таким образом получите%, выразите $p_1, \ldots, p_n$ через  $ \{q_i^{(j)}\}$: 
\[
p_l = \frac{1}{n} \sum \limits_{j=1}^n q_l^{(j)}.
\]
\end{enumerate}

См. также  Ермаков С.М. Метод Монте-Карло в вычислительной математике, М.: Бином, 2009.
\end{ordre}

\begin{problem}[Теорема Пайка] \label{paika} 
Пусть ${x}_{k} ,\; k=1,...,n$ -- независимые равномерно распределенные на отрезке $\left[0,1\right]$ с.в. Упорядочим эти с.в., введя обозначения
\[\mathop{\min }\limits_{k=1,...,n} {x}_{\left(k\right)} ={x}_{\left(1\right)} \le ...\le {x}_{\left(n\right)} =\mathop{\max }\limits_{k=1,...,n} {x}_{\left(k\right)} .\] 
Пусть ${\rm e}_{k} ,\; k=1,...,n+1$ -- независимые показательно распределенные с.в. 
\[\mathbb{P}\left({\rm e}_{k} >t\right)=e^{-t} ,\; t\ge 0.\] 
Покажите, что
\[\left({x}_{\left(1\right)} {\rm ,}\; {x}_{\left(2\right)} {\rm ,...,}\; {x}_{\left(n\right)} \right) \mathop{=}\limits^{d} \left(\frac{{\rm e}_{1} }{\sum _{k=1}^{n+1}{\rm e}_{k}  } {\rm ,}\; \frac{{\rm e}_{1} +{\rm e}_{2} }{\sum _{k=1}^{n+1}{\rm e}_{k}  } {\rm ...,}\; \frac{\sum _{k=1}^{n}{\rm e}_{k}  }{\sum _{k=1}^{n+1}{\rm e}_{k}  } \right) \] 
\end{problem}

\begin{remark}
См. Лагутин М.Б. Наглядная математическая статистика. - М.: Бином, 2009 и Кендалл М., Моран П. Геометрические вероятности. – М.: Наука, 1972.
\end{remark}

\begin{problem}[Генерация равномерного распределения]

а) Пусть ${X}$ ---  $n$-мерный вектор с независимыми одинаково распределенными компонентами с распределением $N(0,1)$. Покажите, что ${\xi} = \frac{{X}}{\|{X}\|_2}$ имеет  равномерное распределение на единичной сфере в $\mathbb{R}^n$. 

б) Пусть ${X}$ вектор с независимыми одинаково распределенными компонентами с распределением Лапласа (т.е. с плотностью $p(x)=\frac{1}{2}\exp(-|x|)$). Какое распределение имеет вектор  ${\xi} = \frac{{X}}{\|{X}\|_1}$? 
\end{problem}




\begin{problem}\Star [Сдвиг Бернулли]
Рассмотрим динамическую систему (ДС): $x\to\{2x\}$ ($\{\cdot \}$ -- дробная часть числа), преобразования отрезка $X = [0,1]$ в себя. Покажите, что для почти всех (по равномерной мере Лебега на отрезке $X$) точек старта, полученная c помощью ДС последовательность точек, будет ``квазислучайной'' (схожей с последовательностью независимых одинаково распределенных на отрезке $X$ с.в.), то есть для неё, например, справедливы з.б.ч. и ц.п.т.
\begin{ordre}
См. Niederreiter H. Random number generation and Quasi-Monte-Carlo methods, 1992.
\end{ordre}
\begin{remark}
В контексте этой задачи рекомендуется ознакомиться также с понятиями непредсказуемой последовательности, типичной последовательности, случайной (сложной) по Колмогорову последовательности, например, по книге Верещагин Н.К., Успенский В.А., Шень А. Колмогоровская сложность и алгоритмическая случайность. -- М.: МЦНМО, 2013. Задача отражает то обстоятельство, что случайность ``переносится'' из начальных данных (даже простым в смысле Колмогорова алгоритмом). Рассматриваемая динамическая система последовательно считывает числа после запятой в двоичной записи начальной точки. Почти все числа из отрезка $[0,1]$ сложны (несжимаемы) в смысле Колмогорова, правда для подавлющего большинства конкретных чисел это утверждение не может быть доказано. Студенты ФУПМ имеют возможность познакомиться с Колмогоровской сложностью и алгоритмичексими вопросами теории вероятностей по курсу (книги) Вьюгин В.В. Колмогоровская сложность и алгоритмическая случайность, М.: МФТИ, 2012. В частности, рекомендуется ознакомиться с сложностным доказательством закона повторного логарифма и эргодической теоремы Биркгофа(-Хинчина), а также с  вопросами эффективности эргодичексой теоремы. 

Другая причина появления случайности -- это поведение динамических систем (например, рассмотренной) в условиях небольших внешних возмущений. Яркое обыгрывание этого направления имеется в конце книги Опойцев В.И. Нелинейная системостатика, М.: Наука, 1986 и в цикле недавних работ В.А. Малышева и А.А. Лыкова. Вообще между динамическими системами и случайными (марковскими) процессами имеется глубокая взаимообогащающая связь. См., например, конструкцию Улама в книге Бланк М.Л. Устойчивость и локализация в хаотической динамике, М.: МЦНМО, 2001 и Синай Я.Г. Как математики изучают хаос // Математическое просвещение. Вып. 5. 2001. С.32-46. Интересные взгляды на эту науку также имеются в книгах Вентцель А.Д., Фрейдлин М.И. Флуктуации в динамических системах под воздействием малых случайных возмущений. -- М.: Наука, 1979, \cite{333}, \cite{101}.

Ну а самое главное, что в приложениях нам нужна не ``первозданная случайность''. Нам нужно лишь выполнение для полученной квазислучайной последовательности неких тестов типа з.б.ч., ц.п.т. с такими же оценками скорости сходимости (или не сильно худшими). Однако оказывается, что псевдослучайная последовательность может даже увеличить скорость сходимости, по сравнению с первозданно случайным случаем. Обнаружено это было более 40 лет назад (сюда можно отнести метод выбора узлов Холтона--Соболя, обобщающий идею задачи 2 п. б) настоящего раздела), но по-прежнему активно используется на практике (см. Соболь И.М. Численные методы Монте-Карло. -- М.: Наука, 1973), и позволяет вместо $~1/\sqrt{n}$ получать точность $~n^{\varepsilon - 1}$ cо сколь угодно малым $\varepsilon > 0$.


В заключение хочется отметить, что в последние десятилетия очень бурно развивается область Theoretical Computer Sсience связанная с изучением генераторов псевдослучайных чисел (в том числе в связи с проблемой $P \ne NP$), см.  
Разборов А.А. Theoretical Computer Sсience: взгляд математика, 2013

\url{people.cs.uchicago.edu/~razborov/files/computerra.pdf}

\end{remark}
\end{problem}

\begin{comment}
\begin{problem}[MCMC]
\label{mcmc}
Для оценки статистик распределения с неизвестной нормировочной константой (см. задачу \ref{varinf} из раздела \ref{CS}) 
\[
p(X) = \frac{1}{S_p} \tilde{p}(X)
\]   
методы Монте Карло требуют возможность генерации выборки из этого распределения
\[
X_1,\ldots,X_N \sim p(X).
\]
Впоследствии, выборка может быть использована для оценки статистик
\[
\Exp_p T(X) \approx \frac{1}{N} \sum_{i = 1}^N T(X_i).
\]
Принцип работы метода MCMC  состоит в использовании некоторой марковской цепи с априорным распределением $p_0(X)$ и вероятностями перехода в момент времени $n:$ $q_n(X_{n+1}|X_n)$. Генерация выборки происходит таким образом:
\[
X_1 \sim p_0(X),  X_2 \sim q_1(X|X_1), \ldots, X_N \sim q_{N-1}(X|X_{N-1}). 
\]
Заметим, что при таком подходе генерируемая выборка не является набором независимых случайных величин, который может быть получен за счет прореживания $X_1,\ldots,X_N $, взяв каждый $m$-й $X$.

Рассмотрим вопрос выбора однородной вероятности перехода $q_n(X_{n+1}|X_n) = q(X_{n+1}|X_n)$, требуя принадлежность генерируемой выборки интересующему нас распределению $p(T)$. Необходимым требованием в таком случае является инвариантность $p(T)$ относительно переходов марковской цепи, т.е.
\[
p(X)=\int q(X|Y) p(Y)dY \Leftarrow q(X|Y) p(Y) = q(Y|X) p(X).
\]
Марковская цепь может иметь более одного инвариантного распределения. 
Пусть $\pi(X)$ –- ее инвариантное распределение. Тогда марковская цепь называется \textit{эргодичной}, если
\[
\forall p_0(X): \; p_n(X) = \int q(X|Y) p_{n-1}(Y)dY \to \pi(X). 
\]
Очевидно, что эргодичная марковская цепь имеет только одно инвариантное распределение. Докажите, что достаточным условием эргодичности однородной марковской цепи является  свойство:
\[
\forall X, \forall Y: \pi(Y) > 0 \Rightarrow q(Y|X) > 0. 
\]

\end{problem}
\end{comment}

\begin{problem}[PageRank] 
Ориентированный граф $G=\left\langle {V,E} \right\rangle$  сети Интернет представляется в виде набора web-страниц $V$ и ссылок между ними: запись $\left( {i,j} \right)\in E$ означает, что на $i$-й странице имеется ссылка на $j$-ю страницу. 

\begin{enumerate}

\item По web-графу случайно блуждает пользователь. За один такт 
времени пользователь, находящийся на web-странице с номером $i$, с вероятностью $p_{ij} $ переходит по ссылке на web-страницу $j$. 
Пусть из любой web-страницы можно по ссылкам перейти на любую другую 
web-страницу (условие неразложимости) графа $G$. Проверьте, что при бесконечно долгом блуждании доля времени, которую пользователь проведет на web-странице с номером $k$ есть $p_k $, где $ 
{p} =\left( {p_1 ,...,p_n } \right)^T $, ${p}^T = {p}^T P$, 
$P=\left\| {p_{ij} } \right\|_{i,j=1}^{n,n} $, $\sum_k p_k = 1$ (решение единственно,  ввиду неразложимости $P$). Обратим внимание, что ответ не зависит от того, с какой вершины стартует пользователь.

\item В условиях предыдущего пункта, пустим независимо блуждать по web-графу $N$ 
пользователей ($N\gg \left| V \right| \gg 1)$. Пусть $n_i \left( t \right)$ -- число посетителей web-страницы $i$ в момент времени $t$. Считая стохастическую матрицу $P$ неразложимой (см. п. а)) и апериодической (см. замечание), покажите, 
что
\[
\exists \;\;\lambda _{0.99} >0,\;T_G>0:\;\;\;\forall \;\;t\ge 
T_G
\]
\[
\PR\left( {\left. {\left| {\frac{n_k \left( t \right)}{N}-p_k } \right|\le 
\frac{\lambda _{0.99} }{\sqrt N }} \right)\ge 0.99} \right.,
\]
где $ {p}^T= {p}^T P$ (решение единственно). Обратим внимание, что ответ не зависит от того, с каких вершин стартуют пользователи.

\end{enumerate}

\end{problem}

\begin{remark}
\begin{comment}
Матрица $P^n = \Vert p_{ij}^{(n)} \Vert$ является матрицей переходных вероятностей за $n$ шагов. Число
\[
    d(j) = \text{НОД} \left\{n : p_{jj}^{(n)} > 0 \right\},
\]
где $\gcd$ обозначает наибольший общий делитель, называется \textit{периодом} вершины $j$. Если $d(j) = 1$, то  вершина называется апериодической. Для неразложимой $P$ периоды у всех вершин совпадают. Соответственно,  стохастическая матрица $P$ называется апериодической, если $\forall j: \; d(j) = 1$.
\end{comment}
Если организовать случайные блуждая, следуя п. б) 
(отметим, что эти блуждания хорошо распараллеливаются по числу 
блуждающих пользователей), то при определенных условиях можно получить решение задачи  ${p}^T = {p}^TP$ значительно быстрее, чем, скажем, ${\rm O}\left( {n^2} \right)$. Такой способ численного поиска вектора $ {p}$ основан на методе Markov 
chain Monte Carlo. %(см. задачу \ref{mcmc})%. 
Детали и ссылки см., например, в работе Гасников А.В., Дмитриев Д.Ю. Об эффективных рандомизированных алгоритмах поиска вектора PageRank // ЖВМиМФ. Т. 55. № 3. 2015 -- arXiv:1410.3120. Упомянем также недавнюю работу Belloni A., Chernozhukov V. On the Computational Complexity of MCMC-based Estimators in Large Samples. -- arXiv:0704.2167, 2012, содержащую строгие результаты об эффективной вычислимости байесовских оценок. Если 
использовать неравенства концентрации меры и иметь оценки на спектральную 
щель матрицы $P$ (см. \cite{44,240}), то приведенный в п. 
б) результат можно также сделать  более строгим, а именно точнее 
оценивать скорость сходимости и плотность концентрации (см.  задачу 19 раздела \ref{macrosystems}).
\end{remark}

\begin{comment}

\begin{problem}

В руки опытных криптографов попалось закодированное письмо (10 000 символов). Чтобы это письмо прочитать нужно его декодировать. Для этого берется стохастическая матрица переходных вероятностей $P$
(линейный размер которой определяется числом возможных символов (букв, знаков препинания и т.п.) в языке на котором до шифрования было написано письмо – этот язык известен и далее будет называться базовым). $P_{ij}$ отвечает за вероятность появления символа с номером $j$ сразу после символа под номером $i$. Такая матрица может быть идентифицирована с помощью статистического анализа  большого текста, скажем,  Войны и мира Л.Н. Толстого.

Пусть способ (де)шифрования определяется некоторой неизвестной функцией $\overline{f}$ – преобразование (перестановка) множества кодовых букв во множество символов базового языка.
В качестве, “начального приближения” выбирается какая-то функция $f$, например,
полученная исходя из легко осуществимого частотного анализа. Далее рассчитывается вероятность выпадения полученного закодированного текста, сгенерированного при заданной функции $f$  (правдоподобие выборки).

Случайно  выбираются два аргумента у функции $f$ и значения функции при этих аргументах меняются местами. Если в результате правдоподобие возрастает, то замена аргументов фиксируется, иначе бросается монетка с вероятностью выпадения орла равной отношению правдоподобий. 

Объясните, почему предложенный алгоритм ``сходится'' именно к $\overline{f}$ ? Оцените скорость сходимости.


\end{problem}



\subsection{ Markov chain Monte Carlo }

\begin{problem}
Чтобы построить однородный дискретный марковский процесс с конечным числом состояний, имеющий заданную инвариантную (стационарную) меру $\pi$ , переходные вероятности ищутся в следующим виде: $p_{ij} = p_{ij}^0 a_{ij}$ , 
$i \neq j$; $p_{ii} = 1 - \sum p_{ij}$ , где $p^0$ – некоторая затравочная матрица, которую будем далее предполагать симметричной. Покажите, что матрица $p$ имеет инвариантную (стационарную) меру  $\pi$, если
\[
\frac{a_{ij}}{a_{ji}} = \frac{\pi_{j} p^0_{ji}}{\pi_{i} p^0_{ij}} = \frac{\pi_{j}}{\pi_{i}} 
\]

Чтобы найти  $a_{ij}$ достаточно найти функцию F: $\mathbb{R_+} \rightarrow [0,1]$ такую, что

\[
\frac{F(z)}{F(1/z)} = z; a_{ij}  \leftarrow F( \frac{\pi_{j}}{\pi_{i}} )
\]

Пример функции $F(z) = \min(z,1)$ определяет алгоритм Метрополиса. 

\end{problem}

\begin{problem}

В руки опытных криптографов попалось закодированное письмо (10 000 символов). Чтобы это письмо прочитать нужно его декодировать. Для этого берется стохастическая матрица переходных вероятностей $P$
(линейный размер которой определяется числом возможных символов (букв, знаков препинания и т.п.) в языке на котором до шифрования было написано письмо – этот язык известен и далее будет называться базовым). $P_{ij}$ отвечает за вероятность появления символа с номером j сразу после символа под номером i . Такая матрица может быть идентифицирована с помощью статистического анализ  большого текста, скажем,  Войны и мира Л.Н. Толстого.

Пусть способ (де)шифрования определяется некоторой неизвестной функцией $\overline{f}$ – преобразование (перестановка) множества кодовых букв во множество символов базового языка.
В качестве, “начального приближения” выбирается какая-то функция f , например,
полученная исходя из легко осуществимого частотного анализа. Далее рассчитывается вероятность выпадения полученного закодированного текста, сгенерированного при заданной функции f  (правдоподобие выборки).

Случайно  выбираются два аргумента у функции f и значения функции при этих аргументах меняются местами. Если в результате правдоподобие возрастает, то замена аргументов фиксируется, иначе бросается монетка с вероятность выпадения орла равной отношению правдоподобий. 

Объясните, почему предложенный алгоритм “сходится” именно к $\overline{f}$ ? Почему сходимость оказывается такой быстрой (0.01 сек. на современном PC)?


\end{problem}

\subsection{Gibbs Sampler}

\subsection{Variational Inference}

\end{comment}




\begin{problem}\Star\,\,\Star (Markov Chain Monte Carlo Revolution и состоятельность
оценок максимального правдоподобия; P. Diaconis)
\label{cript}
В руки опытных криптографов попалось закодированное письмо (10~000 символов). Чтобы это 
письмо прочитать нужно его декодировать. Для этого берется стохастическая 
матрица переходных вероятностей $P=\left\| {p_{ij} } \right\|$ (линейный 
размер которой определяется числом возможных символов (букв, знаков 
препинания и т.п.) в языке на котором до шифрования было написано письмо -- 
этот язык известен и далее будет называться базовым), в которой $p_{ij} $ -- 
отвечает за вероятность появления символа с номером $j$ сразу после символа 
под номером $i$. Такая матрица может быть идентифицирована с помощью 
статистического анализ какого-нибудь большого текста, скажем, ``Войны и 
мира'' Л.Н.~Толстого.

Пускай способ (де)шифрования (подстановочный шифр) определяется некоторой, 
неизвестной, дешифрующей функцией $\bar {f}$ -- преобразование 
(перестановка) множества кодовых букв во множество символов базового языка.

В качестве, ``начального приближения'' выбирается какая-то функция $f$, 
например, полученная исходя из легко осуществимого частотного анализа. Далее 
рассчитывается вероятность выпадения полученного закодированного текста 
$ {x}$, сгенерированного при заданной функции $f$ (функция 
правдоподобия):

\[%\tag{*}
L\left( {{x};f} \right)=\prod\limits_k {p_{f\left( {x_k } 
\right),f\left( {x_{k+1} } \right)} } . 
\]

Случайно выбираются два аргумента у функции $f$ и значения функции при этих 
аргументах меняются местами. Если в результате получилась такая $f^\ast $, 
что $L\left( { {x};f^\ast } \right)\ge L\left( { {x};f} \right)$, то 
$f:=f^\ast $, иначе независимо бросается монетка с вероятностью выпадения 
орла $p={L\left( { {x};f^\ast } \right)} \mathord{\left/ {\vphantom 
{{L\left( {\vec {x};f^\ast } \right)} {L\left( { {x};f} \right)}}} 
\right. \kern-\nulldelimiterspace} {L\left( { {x};f} \right)}$, и если 
выпадает орёл, то $f:=f^\ast $, иначе $f:=f$. Далее процедура повторяется (в 
качестве $f$ выбирается функция, полученная на предыдущем шаге).

Объясните, почему предложенный алгоритм после некоторого числа итераций с 
большой вероятностью и с хорошей точностью восстанавливает дешифрующую 
функцию $\bar {f}$? Почему сходимость оказывается такой быстрой (0.01 сек. на современном PC)?

\end{problem}

\begin{ordre}
Описанный в задаче пример взят из обзора \textit{Diaconis P.} The Markov 
chain Monte Carlo revolution // Bulletin (New Series) of the AMS. 2009. V. 
49. № 2. P. 179--205. Детали того, что будет написано далее можно почерпнуть 
из работ \textit{Jerrum M}., \textit{Sinclair A}. The Markov chain Monte Carlo method: an approach to 
approximate counting and integration // Approximation Algorithms for NP-hard 
Problems / D.S. Hochbaum ed. Boston: PWS Publishing, 1996. P. 482--520; \textit{Joulin A., Ollivier Y.} Curvature, concentration and 
error estimates for Markov chain Monte Carlo // Ann. Prob. 2010. V. 38. № 6. 
P. 2418\textbf{--}2442; \textit{Paulin D.} Concentration inequalities for Markov chains by 
Marton couplings // e-print, \underline {arXiv:1212.2015v2}, 2013, см. также \cite{240}.

Для того чтобы построить однородный дискретный марковский процесс с конечным 
числом состояний, имеющий наперед заданную инвариантную (стационарную) меру 
$\pi $, переходные вероятности ищутся в следующем виде: $p_{ij} =p_{ij}^0 
b_{ij} $, $i\ne j$; $p_{ii} =1-\sum\limits_{j:\;\;j\ne i} {p_{ij} } $, где 
$p_{ij}^0 $ -- некоторая ``затравочная'' матрица, которую будем далее 
предполагать симметричной. Легко проверить, что матрица $p_{ij} $ имеет 
инвариантную (стационарную) меру $\pi $, если при $p_{ij}^0 >0$
\[
\frac{b_{ij} }{b_{ji} }=\frac{\pi _j p_{ji}^0 }{\pi _i p_{ij}^0 }=\frac{\pi 
_j }{\pi _i }.
\]
Чтобы найти $b_{ij} $, достаточно найти функцию $F:\;{\rm R}_+ \to \left[ 
{0,1} \right]$ такую, что

$\frac{F\left( z \right)}{F\left( {1 \mathord{\left/ {\vphantom {1 z}} 
\right. \kern-\nulldelimiterspace} z} \right)}=z$ и $$b_{ij} =F\left( 
{\frac{\pi _j p_{ji}^0 }{\pi _i p_{ij}^0 }} \right)=F\left( {\frac{\pi _j 
}{\pi _i }} \right).$$

Пожалуй, самый известный пример (именно он и использовался в задаче) такой 
функции $\tilde {F}\left( z \right)=\min \left\{ {z,1} \right\}$ -- алгоритм 
Хастингса--Метрополиса. Заметим, что для любой такой функции $F\left( z 
\right)$ имеем $F\left( z \right)\le \tilde {F}\left( z \right)$. Другой 
пример дает функция $F\left( z \right)=z \mathord{\left/ {\vphantom {z 
{\left( {1+z} \right)}}} \right. \kern-\nulldelimiterspace} {\left( {1+z} 
\right)}$. Заметим также, что $p_{ij}^0 $ обычно выбирается равным $p_{ij}^0 
=1 \mathord{\left/ {\vphantom {1 M}} \right. \kern-\nulldelimiterspace} M_i 
$, где $M_i $ число ``соседних'' состояний у $i$, или
\[
p_{ij}^0 =1 \mathord{\left/ {\vphantom {1 {\left( {2M} \right)}}} \right. 
\kern-\nulldelimiterspace} {\left( {2M} \right)},
\quad
i\ne j;
\quad
p_{ii}^0 =1 \mathord{\left/ {\vphantom {1 2}} \right. 
\kern-\nulldelimiterspace} 2,
\quad
i\ne j.
\]
При больших значениях времени $t$, согласно эргодической теореме, имеем, что 
распределение вероятностей близко к стационарному $\pi $. Действительно, при 
описанных выше условиях имеет место условие детального баланса (марковские 
цепи, для которых это условие выполняется, иногда называют обратимыми):
\[
\pi _i p_{ij} =\pi _j p_{ji} ,\;i,j=1,...,n,
\]
из которого сразу следует инвариантность меры $\pi $, т.е.
\[
\sum\limits_i {\pi _i p_{ij} } =\pi _j \sum\limits_i {p_{ji} } =\pi _j 
,\;j=1,...,n.
\]
Основное применение замеченного факта состоит в наблюдении, что время выхода 
марковского процесса на стационарную меру (mixing time) во многих случаях 
оказывается удивительно малым.\footnote{ Более того, задача поиска такого 
симметричного случайного блуждания на графе (с равномерной инвариантной 
мерой в виду симметричности) заданной структуры, которое имеет 
``наименьшее'' mixing time (другими словами, наибольшую спектральную щель), 
сводится к задаче полуопределенного программирования, которая, как известно, 
полиномиально (от числа вершин этого графа) разрешима.} При том, что 
выполнение одного шага по времени случайного блуждания по графу, отвечающему 
рассматриваемой марковской цепи, как следует из алгоритма Кнута--Яо (см. задачу 11), также 
может быть быстро сделано. Таким образом, довольно часто можно получать 
эффективный способ генерирования распределения дискретной случайной величины 
с распределением вероятностей $\pi $ за время полиномиальное от логарифма 
числа компонент вектора $\pi $.

Для лучшего понимания происходящего в условиях задачи, отметим, что одним из 
самых универсальных способов получения асимптотически наилучших оценок 
неизвестных параметров по выборке является метод наибольшего правдоподобия 
(Ибрагимов--Хасьминский, В.Г. Спокойный). Напомним вкратце в чем он 
заключается. Пусть имеется выборка из распределения, зависящего от 
неизвестного параметра -- в нашем случае выборкой $ {x}$ из 10~000 
элементов будет письмо, а неизвестным ``параметром'' будет функция $f$. 
Далее считается вероятность (или плотность вероятности в случае непрерывных 
распределений) $L\left( { {x};f} \right)$ того что выпадет данный $\vec 
{x}$ при условии, что значение параметра $f$. Если посмотреть на 
распределение $L\left( { {x};f} \right)$, как на распределение в 
пространстве параметров ($ {x}$ -- зафиксирован), то при большом объеме 
выборки (размерности $ {x})$ при естественных условиях это распределение 
концентрируется в малой окрестности наиболее вероятного значения
\[
f\left( {{x}} \right)=\arg \mathop {\max }\limits_f L\left( { 
{x};f} \right),
\]
которое ``асимптотически'' совпадает с неизвестным истинным значением $\bar 
{f}$.

\end{ordre}

\begin{remark}
Для оценки mixing time нужно оценить спектральную щель 
стохастической матрицы переходных вероятностей, задающей исследуемую 
марковскую динамику, то есть нужно оценить расстояние от максимального 
собственного значения этой матрицы равного единицы (теорема 
Фробениуса--Перрона) до следующего по величине модуля. Именно это число 
определяет основание геометрической прогрессии, мажорирующей исследуемую 
последовательность норм разностей расстояний (по вариации) между 
распределением в данный момент времени и стационарным (финальным) 
распределением. Для оценки спектральной щели разработано довольно много 
методов, из которых мы упомянем лишь некоторые: неравенство Пуанкаре 
(канонический путь), изопериметрическое неравенство Чигера (проводимость), с 
помощью техники каплинга (получаются простые, но, как правило, довольно 
грубые оценки), с помощью каплинга Мертона, с помощью дискретной кривизны 
Риччи и теорем о концентрации меры (Мильмана--Громова). Приведем некоторые 
примеры применения MCMC: Тасование $n$ карт, разбиением приблизительно на 
две равные кучи и перемешиванием этих куч (mixing time$\sim \log _2 
n)$;\footnote{ Здесь контраст проявляется, пожалуй, наиболее ярко. Скажем 
для колоды из 52 карт пространство состояний марковской цепи будет иметь 
мощность 52! (если сложить времена жизней в наносекундах каждого человека, 
когда либо жившего на Земле, то это число на много порядков меньше 52!). В 
то время как такое тасование: взять сверху колоды карту, и случайно 
поместить ее во внутрь колоды, отвечающее определенному случайному 
блужданию, с очень хорошей точностью выйдет на равномерную меру, отвечающую 
перемешанной колоде, через каких-то 200--300 шагов. Если брать тасование 
разбиением на кучки, то и того меньше -- за 8--10 шагов.} Hit and Run 
(mixing time $\sim n^3)$; Модель Изинга -- $n$ спинов на отрезке, 
стационарное распределение = распределение Гиббса, Глауберова динамика 
(mixing time $\sim n^{{2\log _2 e} \mathord{\left/ {\vphantom {{2\log _2 e} 
T}} \right. \kern-\nulldelimiterspace} T}$, $0<T\ll 1$ (см. задачу 18)); Проблема поиска 
кратчайших гамильтоновых путей; Имитация отжига (см. задачу 20) для решения 
задач комбинаторной оптимизации, MCMC для решения задач перечислительной 
комбинаторики. Но, пожалуй, самым известным примером (Dyer--Frieze--Kannan) 
является полиномиальный вероятностный алгоритм (работающий быстрее известных 
``экспоненциальных'' детерминированных) приближенного поиска центра тяжести 
выпуклого множества и вычисления его объема. Одна из работ в этом 
направлении была удостоена премии Фалкерсона -- аналога Нобелевской премии в 
области Computer Science. Близкие идеи используются и при применении 
экспандеров в Computer Science. В 2010 году премия Неванлинны 
была вручена Д. Спилману, в частности, за сублинейное (по числу элементов 
матрицы, отличных от нуля) решение системы линейных уравнений с помощью 
экспандеров (см. часть 2). 

%Полезно сравнить эту задачу с задачей 7 и с задачей 19 раздела [ССЫЛКА].
\end{remark}

\begin{problem} (Одномерная модель Изинга \cite{44}). 
Рассмотрим конечный отрезок одномерной целочисленной решетки $\{0,1,\dots,n\}$ в каждой вершине $k$ которой находится спин, принимающий два значения $\sigma(k)=\pm1$. При этом считаем, что $\sigma(0)=\sigma(n)=1$. Опредеим Гамильтониан системы $H(\sigma) = \sum_{k=0}^{n-2}(1-\sigma(k)\sigma(k+1))/2$. Определим расстояние Больцмана--Гиббса по формуле $\pi(\sigma) = Z^{-1}\exp(-\beta H(\sigma))$, $\beta = T^{-1}>0$~--- величина, обратная <<температуре>>, а $Z$~--- нормирующий множитель (статсумма). Одним из стандартных способов построения однородной дискретной марковской цепи с заданным стационарным распределением $\pi(\sigma)$ является распределение Глаубера:
\begin{enumerate}
\item Выбираем $k\in\{1,\dots,n-1\}$ согласно равномерному распределению.
\item С вероятностью $$p=\exp(\beta H(\sigma_{k,+1}))/\left(\exp(\beta H(\sigma_{k,+1}))+\exp(\beta H(\sigma_{k,-1}))\right)$$ новым состоянием будет $\sigma_{k,+1}$, а с вероятностью $1-p$ состояние  $\sigma_{k,-1}$,
где $$\sigma_{k,+1} = \sigma_{k}(i),\,i\not=k,\,\sigma_{k,+1} = 1;$$
$$\sigma_{k,-1} = \sigma_{k}(i),\,i\not=k,\,\sigma_{k,-1} = -1.$$
Покажите, что характерное время выхода на стационарное распределение (mixing time) 
этой марковской цепи оценивается сверху как $n^{2\log_2(\beta\exp(1))}$ в предположении $\beta \gg 1$.
\end{enumerate}
\end{problem}


\begin{problem}\Star (Hit and Run).
Ряд задач, в которых используется метод Монте-Карло, предполагает возможность случайно равномерно набрасывать точки в некоторое наперед заданное множество (не обязательно выпуклое и связное). Например, при вычислении интеграла с помощью метода
Монте-Карло или при численном решении задач оптимизации, в которых нужно уметь
приближенно находить центр тяжести множества. Исходя из MCMC подхода (см. \cite{240}) обоснуйте,
аккуратно оговорив детали, следующий способ генерации точек.

Берем любую точку внутри множества и проводим случайно направление через эту точку, далее с помощью
граничного оракула случайно генерируем (с помощью равномерного распределения) на
этом направлении внутреннюю точку рассматриваемого множества. Через эту точку снова
проводим случайное направление и т.д. Хорошо ли будет работать Hit and Run для вытянутых множеств или для множеств, имеющих достаточно острые углы? Предложите модификацию (например, с помощью эллипсоидов Дикина) алгоритма Hit and Run для таких “плохих” множеств. Предложите другие способы случайно равномерно набрасывать точки в некоторое наперед заданное множество (например, Shake and Bake). Хорошо ли будет работать метода Shake and Bake для множеств в пространствах большой размерности? Как следует действовать, если рассматриваемое множество имеет простую структуру: $n$-мерный куб, $n$-мерный шар, $n$-мерный симплекс, многогранник?
\end{problem}

\begin{remark}
См. L. Lovasz, S. Vempala, Hit-and-run from a corner, Proceedings of STOC, 2004, pp. 310– 314.
\end{remark}

\begin{problem}\DStar(Глобальная оптимизация и монотонный симметричный 
марковский поиск; Некруткин--Тихомиров) 
Рассматривается задача глобальной оптимизации $f\left( x \right)\to \mathop {\min }\limits_{x\in 
{\rm R}^n} $. Считаем, что глобальный минимум достигается в единственной 
точке $x^\ast $ (причем для любых $\varepsilon >0$ выполняется 
условие\footnote{$B_\varepsilon ^c \left( {x } \right)$ -- дополнение 
шара $B_\varepsilon { \left( x \right)}$ радиуса $\varepsilon $ с 
центром в точке $x $} $\inf \left\{ {f\left( x \right):\;\;x\in B_\varepsilon^c \left( {x^* } \right)} \right\}>f\left( {x^* } \right))$, 
$f\left( x \right)$ -- непрерывная функция, дважды гладка в точке 
$x^\ast $, причем матрица Гессе $G$ функции $f\left( x \right)$ в этой точке 
положительно определена. Опишем алгоритм (с точностью до выбора функции 
плотности распределения $g\left( r \right)$, $r\in \left[ {0,\infty } 
\right))$.

\begin{enumerate}
\item (начальный шаг) Выбираем точку старта $x_0 =x$;
\item (шаг $k<N$) Независимо генерируем с.в. $\xi _k $ из центрально симметричного распределения с заданной плотностью $g\left( r \right)$. Если
	\begin{itemize}
	\item $f\left( {x_k +\xi _k } \right)\le f\left( {x_k } \right),$ то $x_{k+1} =x_k +\xi _k $,
	\item иначе $x_{k+1} =x_k $;
	\end{itemize}

\end{enumerate}
Введем обозначения
\[
M_r =\inf \left\{ {x\in B_r{\left( {x^* } \right)}:\;\;f\left( x 
\right)<f\left( y \right)\;\mbox{для всех }y\in B_r^{c} \left( {x^* } 
\right)} \right\},
\]
\[
\tau _\varepsilon =\min \left\{ {n\in {\rm N}:\quad x_n \in M_\varepsilon } 
\right\},
\quad
\delta \left( x \right)=\inf \left\{ {r\ge 0:\;\;x\in M_r } \right\},
\quad
\]
\[
\Gamma =\prod\limits_{i=1}^n {\left( {\frac{\lambda _i }{\lambda _{\min } }} 
\right)^{1 \mathord{\left/ {\vphantom {1 2}} \right. 
\kern-\nulldelimiterspace} 2}} ,
\]
где $\lambda _i $ \textbf{-- }собственные числа $G$. Покажите, что, как бы 
мы не выбирали функцию плотности $g\left( r \right)$, всегда при 
$\varepsilon <\rho \left( {x,x^\ast } \right)$ имеет место следующая оценка 
снизу:
\[
\Exp\left[ {\left. {\tau _\varepsilon } \right|x_0 =x} \right]\ge \ln \left( 
{{\rho \left( {x,x^\ast } \right)} \mathord{\left/ {\vphantom {{\rho \left( 
{x,x^\ast } \right)} \varepsilon }} \right. \kern-\nulldelimiterspace} 
\varepsilon } \right)+2.
\]
Покажите, что метод с плотностью (есть много других вариантов)
\[
g\left( r \right)=\nu \left( r \right)r^{-d},
\quad
\nu \left( r \right)=\frac{c}{\left( {e+n\left| {\ln r} \right|} \right)\ln 
^2\left( {e+n\left| {\ln r} \right|} \right)},
\]
где $c$ -- находится из условия нормировки, дает оценку ($\varepsilon 
<{\delta \left( x \right)} \mathord{\left/ {\vphantom {{\delta \left( x 
\right)} 2}} \right. \kern-\nulldelimiterspace} 2)$

\[\tag{*}
\Exp\left[ {\left. {\tau _\varepsilon } \right|x_0 =x} \right]\le b^n\Gamma \ln 
^2\left( \varepsilon \right)\ln ^2\left( {\ln \left( \varepsilon \right)} 
\right)\left| {\ln \left( {\delta \left( x \right)} \right)} \right|, 
\]
где $b\in \left( {2,3} \right)$ (для простоты восприятия мы привели здесь 
огрубленный вариант). 

\end{problem}

\begin{remark}
Если отказаться от гладкости и(или) положительной 
определенности матрицы $G$, то вместо $\Gamma $, которое в типичных 
ситуациях растет с размерностью пространства экспоненциально быстро, в (*) 
можно использовать $F_{\varepsilon ,x}^{-1} $, где
\[
F_{\varepsilon ,x}  = \mathop {\inf }\limits_{\varepsilon \le r<\delta 
\left( x \right)} \left\{ {{\mbox{vol}\left( {M_r } \right)} \mathord{\left/ 
{\vphantom {{\mbox{vol}\left( {M_r } \right)} {\mbox{vol}\left( {B_r \left( 
{x^\ast } \right)} \right)}}} \right. \kern-\nulldelimiterspace} 
{\mbox{vol}\left( {B_r \left( {x^\ast } \right)} \right)}} \right\}.
\]
Отметим, что все приведенные результаты сохраняются с небольшими поправками 
и для оценок вероятностей больших уклонений, т.е. для
\[
n\left( {x,\varepsilon ,\gamma } \right)=\min \left\{ {n:\;\;\PR\left( {x_n 
\in M_\varepsilon \left| {x_0 =x} \right.} \right)\ge \gamma } \right\}=
\]
\[=\min 
\left\{ {n:\;\;\PR\left( {\tau _\varepsilon \le n\left| {x_0 =x} \right.} 
\right)\ge \gamma } \right\}.
\]
Детали имеются в работах А.С. Тихомирова, опубликованных за последние 20 лет 
в ЖВМ и МФ. 

Изложенные в этой задаче результаты могут вызвать на первых порах удивление. 
И, действительно, как такое возможно, чтобы в задачах глобальной оптимизации 
зависимость числа итераций от точности была логарифмическая, в то время как 
известны нижние оценки, в которых эта точность входит в степени размерности 
пространства (в случае равномерной гладкости высокого порядка, степень можно 
понижать) в знаменателе, см., например, \textit{Zhigljavsky A., Zilinskas A.} Stochastic global optimization. 
Springer Optimization and Its Applications, 2008? Тут стоит отметить, что, 
во-первых, нижние оценки получаются для детерминированных методов, ну и 
самое главное, что ``проклятие размерности'' здесь также никуда не делось. 
Даже при самом благоприятном раскладе, в оценку (*) входит фактор $2^n$, 
экспоненциально растущий с ростом размерности пространства. В отличие от 
глобальной оптимизации, в выпуклой оптимизации такие проблемы можно решать (см. Часть 2).

\end{remark}

 \begin{problem}[Глобальная оптимизация и simulated annealing]
 \label{annealing}
 Пожалуй, самым популярным сейчас методом глобальной оптимизации (правда, с очень 
плохими на данный момент теоретическими оценками скорости сходимости) 
является simulated annealing (имитация затвердевания или отжига), 
представляющий собой дискретное приближение решения стохастического 
дифференциального уравнения\footnote{ Детально изученного в статье \textit{German S., Hwang C.P.} 
Diffusions for global optimization // SIAM J. Control and Optimization. 
1986. V. 24. no. 5. P. 1031--1043.}
\[
dx_t =-\nabla f\left( {x_t } \right)dt+\sqrt {2T\left( t \right)} dw_t ,
\]
где $w_t $ -- винеровский процесс. Покажите, что при неограничительных 
условиях и $T\left( t \right)\equiv T$ траектория $x_t $ имеет при $t\to 
\infty $ стационарное распределение с плотностью Гиббса
\[
\frac{\exp \left( {-{f\left( x \right)} \mathord{\left/ {\vphantom {{f\left( 
x \right)} T}} \right. \kern-\nulldelimiterspace} T} \right)}{\int {\exp 
\left( {-{f\left( z \right)} \mathord{\left/ {\vphantom {{f\left( z \right)} 
T}} \right. \kern-\nulldelimiterspace} T} \right)dz} },
\]
экспоненциально концентрирующееся в окрестности единственной точки 
глобального минимума $x^\ast $ дважды гладкой функции $f\left( x \right)$ 
при $T\to 0+$. Однако при $T\to 0+$ и время выхода на это стационарное 
распределение неограниченно возрастает, что создает проблемы для 
практического применения. Более правильно брать $T\left( t \right)=c 
\mathord{\left/ {\vphantom {c {\ln \left( {2+t} \right)}}} \right. 
\kern-\nulldelimiterspace} {\ln \left( {2+t} \right)}$, где $c$ -- 
достаточно большое число. Покажите, что тогда для любой начальной точки $x_0 
$ траектория процесса $x_t $ имеет в пределе $t\to \infty $ (который, 
фактически, с хорошей точностью проявляется уже на конечных временах) 
распределение, сосредоточенное в точке $x^\ast $. 

\end{problem}

\begin{remark} 
Детали и способы дискретизации можно почерпнуть из 
работы \textit{Kushner H.} Asymptotic global behavior for stochastic approximation and 
diffusion with slowly decreasing noise effects: global minimization via 
Monte Carlo // SIAM J. Appl. Math. 1987. V. 47. no. 1. P. 169--183.
\end{remark}

\begin{problem} [Multilevel Monte Carlo; M. Giles] 
Некоторый диффузионный процесс описывается стохастическим дифференциальным уравнением
\[
dS\left( t \right)=a\left( {S,t} \right)dt+b\left( {S,t} \right)dW\left( t 
\right),
\quad
0\le t\le T,
\quad
S\left( 0 \right)=S_0 ,
\]
где $W\left( t \right)$ -- винеровский процесс. Задана липшицева функция 
$f\left( S \right)$. Требуется предложить численный способ оценивания 
\[
Y=\Exp\left[ {f\left( {S\left( T \right)} \right)} \right].
\]

а)* Дискретизируем задачу по схеме Эйлера
\[
\hat {S}_{n+1} =\hat {S}_n +a\left( {\hat {S}_n ,t_n } \right)h+b\left( 
{\hat {S}_n ,t_n } \right)\Delta W_n ,
\]
возьмем $N$ независимых реализаций $\left\{ {\hat {S}_n^{\left( i \right)} } 
\right\}$, и положим
\[
\overline {Y}=\frac{1}{N}\sum\limits_{i=1}^N {f\left( {\hat {S}_{T 
\mathord{\left/ {\vphantom {T h}} \right. \kern-\nulldelimiterspace} 
h}^{\left( i \right)} } \right)} .
\]
Покажите, что найдутся такие $C_1 ,C_2 >0$, что
\[
\mbox{MSE}=E\left[ {\left( {\overline {Y}-Y} \right)^2} \right]\approx C_1 
N^{-1}+C_2 h^2.
\]
Покажите, что если от оценки требуется точность $\varepsilon $ ($\sqrt 
{\mbox{MSE}} ={\rm O}\left( \varepsilon \right))$, то оптимально (с точки 
зрения $\mbox{Total}\left( \varepsilon \right)$ -- общего числа 
арифметических операций / генерирования нормальных с.в. $\Delta W_n )$ 
выбирать
\[
h={\rm O}\left( \varepsilon \right),
\quad
N={\rm O}\left( {\varepsilon ^{-2}} \right),
\quad
\mbox{Total}\left( \varepsilon \right)={\rm O}\left( {\varepsilon ^{-3}} 
\right).
\]

б)** Предложим другой (более эффективный) способ оценивания. Для 
этого введем константу $M>1$ и положим
\[
h_l =M^{-l}T,
\quad
\overline {Y}_l =\frac{1}{N_l }\sum\limits_{i=1}^{N_l } {\left( {f\left( {\hat 
{S}_{T \mathord{\left/ {\vphantom {T {h_l }}} \right. 
\kern-\nulldelimiterspace} {h_l }}^{\left( i \right)} } \right)-f\left( 
{\hat {S}_{T \mathord{\left/ {\vphantom {T {h_{l-1} }}} \right. 
\kern-\nulldelimiterspace} {h_{l-1} }}^{\left( i \right)} } \right)} 
\right)} ,
\]
\[
\overline {Y}_0 =\frac{1}{N_0 }\sum\limits_{i=1}^{N_0 } {f\left( {\hat {S}_{T 
\mathord{\left/ {\vphantom {T {h_0 }}} \right. \kern-\nulldelimiterspace} 
{h_0 }}^{\left( i \right)} } \right)} ,
\quad
\overline {Y}=\sum\limits_{l=0}^L {\overline {Y}_l } .
\]
Покажите, что
\[
\mbox{Bias}=E\left[ {\overline {Y}-Y} \right]={\rm O}\left( {h_L } \right),
\]
\[
V_l =D\left[ {f\left( {\hat {S}_{T \mathord{\left/ {\vphantom {T {h_l }}} 
\right. \kern-\nulldelimiterspace} {h_l }}^{\left( i \right)} } 
\right)-f\left( {\hat {S}_{T \mathord{\left/ {\vphantom {T {h_{l-1} }}} 
\right. \kern-\nulldelimiterspace} {h_{l-1} }}^{\left( i \right)} } \right)} 
\right]={\rm O}\left( {h_l } \right),
\]
Мы хотим, чтобы
\[
\sqrt {\mbox{MSE}} =\sqrt {\mbox{Bias}^2+\Var\left[ {\overline {Y}} \right]} \le 
\mbox{Bias}+\sqrt {\Var\left[ {\overline {Y}} \right]} \sim \varepsilon ,
\]
что достигается, если положить
\[
L={\log \left( {\varepsilon ^{-1}} \right)} \mathord{\left/ {\vphantom 
{{\log \left( {\varepsilon ^{-1}} \right)} {\log M}}} \right. 
\kern-\nulldelimiterspace} {\log M}+{\rm O}\left( 1 \right),
\quad
\Var\left[ {\overline {Y}} \right]=\sum\limits_{l=0}^L {N_l^{-1} V_l } \sim 
\sum\limits_{l=0}^L {N_l^{-1} h_l } \sim \varepsilon ^2.
\]
Учитывая это, покажите, что решение задачи $$\mbox{Total}\left( \varepsilon 
\right)=\sum\limits_{l=0}^L {N_l h_l } \to \mathop {\min }\limits_{\left\{ 
{N_l } \right\}\ge 0}$$ при ограничении $\sum\limits_{l=0}^L {N_l^{-1} h_l } 
={\rm O}\left( {\varepsilon ^2} \right)$ имеет вид $N_l ={\rm O}\left( 
{\varepsilon ^{-2}Lh_l } \right)$. Таким образом, $\mbox{Total}\left( 
\varepsilon \right)={\rm O}\left( {\varepsilon ^{-2}\left( {\log \varepsilon 
} \right)^2} \right)$.

\end{problem}

\begin{remark}
Описанный в п. б) метод был предложен относительно 
недавно в контексте разработки эффективных численных методов оценки 
финансовых инструментов на рынке, поэтому он попал далеко не во все 
классические монографии на эту тему: \textit{Glasserman P.} Monte Carlo methods in financial 
engineering. Springer, 2005; \textit{Graham C., Talay D. }Mathematical foundation of stochastic 
simulation. Series ``Stochastic modelling and applied probability''. V. 68. 
2013. Тем не менее, мы рекомендуем эти книги для погружения в область 
численных методов финансовой математики.

\end{remark}