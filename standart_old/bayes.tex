\section{Байесовские методы}
\label{bayes}

\begin{problem}
Охранная система представлена в виде Байесовской модели, изображенной на Рис. \ref{Fig:bayes1.png}. 
\imgh{50mm}{bayes1.png}{Модель охранной системы.}
Пусть $t$ обозначает факт срабатывания тревоги, $v$ -- наличие вора $\in \text{Be}(s)$, $e$ -- было ли землетрясение, $r$ -- наличие радиосообщения. Величины $t$, $v$, $e$, $r$ имеют распределение Бернулли. Переменная $s$ -- статистика по криминогенной активности -- принимает значения из отрезка [0, 1]. Значения $\PR(t = 1|v,e)$, $\PR(r = 1|e)$, $\PR(e = 1)$ и $\PR(v = 1)$ заданы, причем $\PR(t = 1|v=0,e=0) = 0$, $\PR(r = 1|e = 0) = 0$.
Проведите расчет $\PR(v = 1|t = 1, s = s_0)$ и $\PR(v = 1 | t = 1, s=s_0, r = 1)$. 
\end{problem}

\begin{remark}
Генеративные вероятностные модели часто представляются в виде интуитивно понятных  \textit{графических моделей}. Графическая модель -- это граф, определяющий зависимости между случайными величинами. Вершинам соответствуют случайные величины, ребрам -- зависимости между ними.

Наблюдаемые величины (значения которых известны) закрашиваются, скрытые (значения которых надо найти) -- остаются незакрашенными. Стрелка из вершины $A$ в $B$  обозначает зависимость $B$ от $A$ (часто под такого рода зависимостью подразумевается, что $A$ является параметрами распределения с.в. $B$).

Для обозначения повторяющихся величин с одинаковым распределением используются прямоугольники, которые могут быть вложенными. В одном из углов прямоугольника обычно указывается количество повторений случайных величин, расположенных внутри него. 

\end{remark}

\begin{problem}
Радиоактивный источник излучает за одну секунду $n \in \Po(s)$ частиц, где $s$ -- неизвестная интенсивность излучения. Прибор, регистрирующий частицы, имеет погрешность $\theta = 0.9$, т.е. количество зарегистрированных частиц за одну секунду $c \in \mathrm{Bin}(\theta,n)$. Покажите, что 
\[
\PR(c|\theta,s) = \Po(s\theta),
\]
\[
\PR(n-c|c,\theta,s) = \Po(s(1-\theta)). 
\]
Предположим, что за первую секунду прибор зарегистрировал $c_1 = 10$ частиц, а за вторую -- $c_2 = 16$ частиц (см. Рис. \ref{Fig:bayes3.png}). Докажите формулы для условного распределения и математического ожидания $n_1$: 
\[
\PR(n_1 | c_1, c_2, \theta, s) = \int_{0}^{\infty} p(n_1|c_1,\theta,s) p(s|c_1,c_2,\theta) ds = 
\]
\[
= C_{n_1+c_2}^{c_1+c_2} \left(\frac{2\theta}{1+\theta}\right)^{c_1+c_2+1} \left(\frac{1-\theta}{1+\theta}\right)^{n_1 - c_1},  
\]
\[
\Exp(n_1 | c_1, c_2, \theta, s) = \frac{c_1 + c_2 + 1 - \theta}{2\theta} + \frac{c_1 - c_2}{2} = 131.5.
\]
Найдите 90\% доверительный интервал для $n_1 | c_1, c_2, \theta, s$, воспользовавшись соотношением
\[
\PR(|X - \Exp X| > t \sqrt{\Var X}) \leq \frac{1}{t^2}.
\]
\imgh{60mm}{bayes3.png}{Модель радиоактивного источника.}
\end{problem}


\begin{problem}[Распределение Дирихле]
\label{dir}

Ознакомьтесь с распределениями Дирихле, бета и гамма, приведенными в замечании. Покажите, что если $(X_1, \ldots, X_k)$, $X_i \in \text{Г}( \gamma_i, 1)$ -- гамма распределенные независимые с.в. и $S = \sum \limits_{i=1}^k X_i$, то 
\[\tag{1}
(X_1 / S , \ldots, X_k / S) \in \mathrm{Dir}(\gamma_1, \ldots, \gamma_{k}).
\] 
Предложите способ генерации вектора с распределением Дирихле ($\gamma_i \in \mathbb{N}$), имея в распоряжении генератор с.в. с равномерным распределением на отрезке $[0, 1]$. 

Покажите, что каждая компонента вектора с распределением Дирихле имеет бета-распределение $X_i/S \in \mathrm{Beta}(\gamma_i, \sum \limits_{j \neq i} \gamma_j)$. 

Положим, что вектор $(\theta_1,\ldots,\theta_k)$ априорно имеет распределение (1), выборка $Y_1,\ldots, Y_n$ сгенерирована из дискретного распределения $\mathrm{Cat}(\theta_1,\ldots,\theta_k)$, т.е. $\PR(Y_i = m | \theta_1,\ldots,\theta_k) = \theta_m$. Докажите справедливость формулы для плотности апостериорного распределения $\theta_1,\ldots,\theta_k$: 
\[
p(\theta_1,\ldots,\theta_k \vert Y_1,\ldots, Y_n, \gamma_1, \ldots, \gamma_{k}) = 
\]
\[
=\mathrm{Dir}\left(\gamma_1 + \sum_{i=1}^n \delta_1(Y_i), \ldots, \gamma_{k}+ \sum_{i=1}^n \delta_k(Y_i)\right),
\]
где $\delta_k(x) = [x = k]$.
Убедитесь, что данная функция принимает максимальное значение при 
\[\tag{2}
\theta_m \propto \gamma_m - 1 + \sum_{i=1}^n \delta_m(Y_i).
\]

\end{problem}

\begin{remark}

Говорят, что вектор $X$ принадлежит распределению \textit{Дирихле} $\mathrm{Dir}(\gamma_1, \ldots, \gamma_{k})$, если: 
\[
f_{X}(x_1, \ldots, x_k) = \frac{\text{Г}(\gamma_1 +  \ldots + \gamma_{k})}{\text{Г}(\gamma_1)  \ldots  \text{Г}(\gamma_{k})} x_1^{\gamma_1-1} \ldots x_k^{\gamma_k-1},
\] 
\noindent где  $(x_1, \ldots, x_k) \in \{x_i \geq 0, \; \sum x_i  = 1 \}$, $\gamma_i \geq 0$, $\text{Г}$ -- гамма функция. 

Говорят, что вектор $X$ принадлежит \textit{бета} распределению \\  $\mathrm{Beta}(\gamma_1, \gamma_2)$, если: 

\begin{center}
$f_{X}(x) = \frac{\text{Г}(\gamma_1 + \gamma_2)}{\text{Г}(\gamma_1) \text{Г}(\gamma_2)} x^{\gamma_1-1} (1 - x)^{\gamma_2-1}$, где  $x \in [0, 1]$.
\end{center} 


Допускается также обозначение следующего вида \\ $ X \in \mathrm{Dir}(\gamma_1, \ldots, \gamma_{k+1})$, если 
\[
f_{X}(x_1, \ldots, x_k) = \frac{\text{Г}(\gamma_1 +  \ldots + \gamma_{k+1})}{\text{Г}(\gamma_1)  \ldots  \text{Г}(\gamma_{k+1})} x_1^{\gamma_1-1} \ldots x_k^{\gamma_k-1} (1 - x_1 - \ldots - x_k)^{\gamma_{k+1}-1},
\] 


\noindent где  $(x_1, \ldots, x_k) \in \{x_i \geq 0, \; \sum x_i \leq 1 \}$.

Говорят, что с.в. $X$ принадлежит \textit{гамма} распределению
\\  $\text{Г}(\alpha, \lambda)$, если: 
\begin{center}
$f_{X}(x) = \frac{x^{\alpha-1} e^{-x/\lambda}}{\lambda^\alpha\text{Г}(\alpha)}$,  где  $x \geq 0$.
\end{center}

Распределение Дирихле $\mathrm{Dir}(\gamma_1, \ldots, \gamma_{k})$ может быть интерпретировано следующим образом. Его функция плотности возвращает вероятность того, что вероятности $A_1,\ldots,A_K$ -- несовместных  событий равны соответственно $x_1, \ldots, x_K$, $\sum_i x_i = 1$ при условии, что $i$-е событие наблюдалось $\gamma_i-1$ раз. 

Отметим два свойства распределения Дирихле, делающего его удобным для использования в качестве априорного, при оценке параметров дискретного распределения (см. выражение (2)). Первое, семейство распределений Дирихле является сравнительно большим среди всех возможных распределений на симплексе, что является своего рода страховкой от неправильного выбора типа априорного распределения. Второе, имея выборку, сгенерированную из дискретного распределения, апостериорное распределение допускает простое аналитическое вычисление. 

Обратите внимание, что в выражении (2)  при $\gamma_i < 1$, $i = \overline{1,k}$ априорное распределение способствует разреживанию (увеличению количества нулевых компонент) $\theta_1,\ldots,\theta_k$, при  $\gamma_i = 1$, $i = \overline{1,k}$ априорное распределение является равномерным, а при  $\gamma_i > 1$, $i = \overline{1,k}$ распределение способствует сглаживанию $\theta_1,\ldots,\theta_k$.

Распределение Дирихле приобрело широкое практическое применение в области  тематического моделирования текстов (см. задачу \ref{sec:them_modeling}), машинного перевода, построения моделей в экологии, байесовской проверки гипотез, точечного оценивания и оценки доверительных интервалов. 
\end{remark}

\begin{problem}[Упорядоченное распределение Дирихле]
Пусть $X_{(k)}$ является $k$-ой порядковой статистикой в наборе $X_1, \ldots, X_n$ (*) случайных величин с функцией распределения $F_X(x)$. Пусть $U_k = F_X(X_{(k)}).$ Покажите, что $U \in \mathrm{Dir^*}(1, \ldots, 1)$ (см. замечание).

Сравните доказанное утверждение с задачей \ref{sec:ordered_seq} раздела \ref{standart}. 
Покажите, что если $\mathbb{P}(X \in [X_{(i-1)}, X_{(i)}]) = W_i = U_i - U_{i-1}$, где $X_{(0)} = -\infty$, $X_{(n+1)} = +\infty$, то 
$W \in \mathrm{Dir}(1, \ldots, 1)$.
 
Имея в распоряжении набор векторов (*), где $X_i \in \mathbb{R}^m$, эмпирическая оценка плотности распределения в точке $x$ может быть найдена следующим образом:
\[
\widehat{f}_X(x) = \frac{\mathbb{E}U_k}{V_{r_k}},
\]    
где $r_k$ -- расстояние от $x$ до $X_{(k)}$, $V_{r_k}$ -- объем шара радиуса $r_k$ в пространстве $\mathbb{R}^m$. Докажите, что  
\[
U_k \in \mathrm{Beta}(k, n-k+1), 
\]
\[
\widehat{f}_X(x) \xrightarrow{p} f_X(x), \quad
n \to \infty.
\]    


\end{problem}


\begin{remark}

Говорят, что вектор $Y$ принадлежит \textit{упорядоченному распределению Дирихле} $\mathrm{Dir^*}(\gamma_1, \ldots, \gamma_{k+1})$, если: 
\[
f_{Y}(y_1, \ldots, y_k) = \]
\[ \frac{\text{Г}(\gamma_1 +  \ldots + \gamma_{k+1})}{\text{Г}(\gamma_1)  \ldots  \text{Г}(\gamma_{k+1})} y_1^{\gamma_1-1}(y_2 - y_1)^{\gamma_2-1} \ldots (y_k - y_{k-1})^{\gamma_k-1} (1 - y_k)^{\gamma_{k+1}-1},
\] 

\noindent где  $(y_1, \ldots, y_k) \in \{ 0 \leq y_1 \leq \ldots \leq  y_k \leq 1 \}$, $\gamma_i \geq 0$.

Причем, если $X \in \mathrm{Dir}(\gamma_1, \ldots, \gamma_{k+1})$ и $Y_i = \sum \limits_{j=1}^{i} X_j$, $i = \overline{1,k}$ то 
\[
Y \in \mathrm{Dir^*}(\gamma_1, \ldots, \gamma_{k+1}).
\]

\end{remark}


\begin{problem} 
\label{po_dir}
Распределение Пуассона--Дирихле получается из упорядоченного вектора с распределением Дирихле предельным переходом, описанным в замечании. Докажите, что величины $\theta_{(i)}$ с вероятностью 1 убывают с экспоненциальной скоростью:
\[
\theta_{(i)} \propto e^{- i/\lambda}.
\]
Такое распределение часто возникает при моделировании разделения большого числа элементов из $S$ на большое число видов. Предположим, что из $S$, была произведена выборка размера $n$. Докажите, что вероятность того, что все элементы выборки имеют один и тот же вид равна
\[
\frac{\lambda \text{Г}(\lambda) \text{Г}(n) }{\text{Г}(\lambda+n)}.
\]
\end{problem}

\begin{remark}
Пусть $\theta_1,\ldots,\theta_n \in \mathrm{Dir}(\gamma_1,\ldots,\gamma_n)$, при этом
выполнены условия
\[
\max_i\gamma_i \to 0,
\quad n \to \infty,
\] 
\[
\lambda_n = \sum_{i=1}^n \gamma_i \to \lambda. 
\]
Введем обозначения
\[
s_0 = 0, \quad s_j = \gamma_1 + \ldots + \gamma_j, \quad j = \overline{1,n}.
\]
Распределение Дирихле может быть сконструировано из $Y_i \in \text{Г}(\gamma_i, 1)$ по правилу $\theta_i = Y_i/\sum_j Y_j$. Известно, что $Y_i$ безгранично делимы, т.е. могут быть разбиты на сколь угодно большое число гамма распределенных случайных величин $Y_{i1},\ldots,Y_{ip}$. Частичные суммы величин $\sum_j Y_j$ приближаются в пределе к случайному процессу с независимыми приращениями $g$: 
\[
g(s_m) = \sum_{i=1}^{m} Y_i, \quad \theta_j = \frac{g(s_j) - g(s_{j-1})}{g(s_n)}. 
\]       
Из вида характеристической функции 
\[
\phi_{g(s)}(\mu) = \exp \left(  
-s \int_{0}^{\infty} \left(1 - e^{-\mu z} \right) \frac{e^{-z}}{z} dz
\right)
\]
\[
\gamma(dz) = \frac{e^{-z}}{z} dz
\]
заключаем, что $g(s)$ является субординатором на множестве $s \in [0, \lambda]$ (см. задачу \ref{subord} из раздела \ref{geom}). Величины скачков процесса $g(s)$ образуют пуассоновский процесс $J(t)$ на множестве $t \in (0, \infty)$ c переменной интенсивностью
\[
\lambda(z) = \lambda \frac{e^{-z}}{z}.
\]   
Обозначим за $J_{(1)} \geq J_{(2)} \geq J_{(3)} \geq \ldots$ упорядоченные величины скачков. Последовательность $\theta_{(1)} \geq \theta_{(2)} \geq \theta_{(3)} \geq \ldots$, где 
\[
\theta_{(i)}  = \frac{J_{(i)}}{g(\lambda)},
\]
имеет распределение \textit{Пуассона--Дирихле}.

\end{remark}

\begin{problem}[Сопряженные распределения]
\label{bayes_def}
Сформулируем задачу байесовского вывода. Пусть известно распределение $p(X|\theta)$ (\textit{правдоподобие выборки} $X$). Требуется найти: 
\begin{enumerate}
\item $p(\theta|X) = \frac{p(X|\theta)p(\theta)}{p(X)}$ -- апостериорное распределение;
\item $p(x'|X) = \int_\Theta p(x'|\theta)p(\theta|X) d\theta$ -- предсказание нового $x'$.
\end{enumerate} 

Ключевым моментом является выбор наиболее разумного $p(\theta)$ -- \textit{априорного распределения}. Выбор осуществляется, исходя из дополнительной информации о параметре $\theta$ и необходимости введения регуляризации (отдание предпочтения более простым функциям $p(X | \theta)$, которые могут быть построены более точно при ограниченном размере выборки $|X|$). 

Решение  задач Байесовского вывода существенно упрощается при использовании \textit{сопряженных семейств} распределений (см. М. Де Гроот. Оптимальные статистические решения. -- М.: Мир, 1974).  Семейство распределений $\lbrace p(\theta|\alpha), \; \alpha \in \Omega \rbrace$ называется \textit{сопряженным} к семейству правдоподобий $\lbrace p(X|\theta) , \; \theta \in \Theta \rbrace$, если $\forall 
\alpha \; \exists \alpha'$ такой, что апостериорное распределение $p(\theta|X, \alpha) = p(\theta|\alpha')$ (т.е.  апостериорное распределение имеет такой же вид, что и априорное, но с другими параметрами). Именно  сопряженное распределение разумно взять в качестве априорного, что, в частности, упростит вычисление нормировочного интеграла: 
\[
p(X) = \int \limits_{\theta} p(X|\theta)p(\theta) d \theta.
\] 
 

Рассмотрим несколько примеров сопряженных семейств. Покажите, что:

\begin{enumerate}
\item Сопряженным к распределению Бернулли является \textit{бета}  распределение (см. замечание к задаче \ref{dir}).
Если $x_1, \ldots, x_n$ -- реализация выборки из распределения Бернулли, 
априорное распределение есть $\mathrm{Beta}(\alpha, \beta)$, то апостериорное распределение будет иметь параметры:
\[
\alpha + \sum \limits_{i = 1}^n x_i,  \; \; \beta + n - \sum \limits_{i = 1}^n x_i.  
\]


\item Пусть $x_1, \ldots, x_n$ -- реализация одного элемента выборки из мультиномиального  распределения с неизвестным вектором параметров $\theta$ длины $n$. Допустим, что  априорное распределение  $\theta$ есть распределение Дирихле (см. замечание к задаче \ref{dir}) $\mathrm{Dir}(\alpha_1, \ldots, \alpha_n)$. Тогда апостериорное распределение есть  $\mathrm{Dir}(\alpha_1 + x_1, \ldots, \alpha_n + x_n)$.  

\item Если $x_1, \ldots, x_n$ -- реализация выборки из распределения Пуассона с неизвестным значением среднего $\theta$, 
априорное распределение есть гамма-распределение  $\text{Г}(\alpha, \beta)$, то апостериорное распределение будет иметь параметры:
\[
\alpha + \sum \limits_{i = 1}^n x_i,  \; \; \beta + n.  
\]

\item Если $x_1, \ldots, x_n$ -- реализация выборки из экспоненциального распределения  с неизвестным параметром $\theta$, 
априорное распределение есть гамма-распределение  $\text{Г}(\alpha, \beta)$, то апостериорное распределение будет иметь параметры:
\[
\alpha + n,  \; \; \beta + \sum \limits_{i = 1}^n x_i.  
\]


\item Если $x_1, \ldots, x_n$ -- реализация выборки из нормального распределения  с неизвестным  значением среднего $m$ и заданной мерой точности $r = 1/\sigma^2$, 
априорное распределение $m$ есть  $N(\mu, 1/\tau)$, то апостериорное распределение есть $N(\mu', 1/\tau')$, где:
\[
\mu' = \frac{\tau \mu + n r \overline{x}}{\tau + n r},  \; \; 
\tau' = \tau + n r.  
\]

\item Если $x_1, \ldots, x_n$ -- реализация выборки из нормального распределения  c заданным  значением среднего  $m$ и неизвестной мерой точности $r = 1/\sigma^2$. Пусть априорное распределение  $r$ есть гамма-распределение  $\text{Г}(\alpha, \beta)$. Тогда апостериорное распределение будет иметь параметры:
\[
\alpha + \frac{n}{2},  \; \; \beta + \frac{1}{2} \sum \limits_{i = 1}^n (x_i - m)^2.  
\]

\item Если $x_1, \ldots, x_n$ -- реализация выборки из нормального распределения  c неизвестным значением среднего  $m$ и неизвестной мерой точности $r = 1/\sigma^2$ (см. Рис. \ref{Fig:model.pdf}). Пусть условное априорное распределение $m$ при фиксированном $r$ есть  $N(\mu, 1/(\tau r) )$; априорное распределение  $r$ есть гамма-распределение  $\text{Г}(\alpha, \beta)$. Тогда апостериорное совместное распределение $m$ и $r$ имеет  следующий вид:
\[
m \vert r \in N \left( \frac{\tau \mu + n \overline{x}}{\tau + n}, (\tau + n) r  \right),  
\]
\[
r \in \text{Г} \left( \alpha + \frac{n}{2}, \beta + \frac{1}{2} \sum \limits_{i = 1}^n (x_i - \overline{x})^2 + \frac{1}{2} \frac{\tau n (\overline{x} - \mu)^2}{\tau + n} \right),
\]
\[
p(m, r | X, \mu, \tau, \alpha, \beta) = \const \cdot p(r | \alpha, \beta) p(m | r, \mu, \tau) p(X | m, r) =
\]
\[
p(m| r, X, \mu, \tau) p(r | X, \mu, \tau, \alpha, \beta).
\]

\imgh{80mm}{model.pdf}{Графическая модель генерации выборки из нормального распределения с неизвестными параметрами.}

\end{enumerate} 

\end{problem}



\begin{problem}[Экспоненциальное семейство распределений]
\label{EF}
Многие стандартные распределения принадлежат экспоненциальному семейству (см. замечание), например, нормальное, гамма, бета, Бернулли, Дирихле. 
Пусть с.в. $Y \in EF$. Докажите следующие свойства $EF$:

\[
\varphi_{T(x)}(t) = \Exp e^{\langle t, T(x) \rangle} = \frac{Z(\theta + t)}{Z(\theta)};
\]
\[
\nabla \log Z(\theta) = \Exp T(x);
\quad
\nabla^2 \log Z(\theta) = \cov (T(x), T(x));
\]
для выборки $X_1,\ldots,X_n \in p(x|\theta)$ оценка максимума правдоподобия может быть представлена как 
\[
\nabla \log Z(\widehat{\theta}_{ML}) = \frac{1}{n} \sum_{i=1}^{n} T(X_i);
\]
\[
\mathcal{KL}(\nu_2 \Vert \nu_1) = \nabla d(\nu_2) (\nu_2 - \nu_1) - [d(\nu_2) - d(\nu_1)];
\]
сопряженное распределение (см. задачу \ref{bayes_def}) на параметры $\theta$ может быть вычислено по формуле
\[
p(\theta | \alpha_1, \alpha_2) \propto e^{ \langle \theta, \alpha_1 \rangle - \alpha_2 \log Z(\theta)}.
\]

\end{problem}

\begin{remark}
C.в. $X \in EF$, если ее плотность распределения имеет следующий вид (canonical   parametrization):
\[
p(x | \nu) = h(x)e^{\langle x, \nu \rangle - d(\nu)}. 
\]
Возможно также другое представление:
\[
p(x | \theta) = \frac{h(x)}{Z(\theta)} e^{ \langle \theta, T(x) \rangle}, 
\]
где $T(x)$ -- \textit{достаточная статистика} распределения $p(x | \theta)$.
\end{remark}


\begin{problem}[Процесс Дирихле]
\label{DP}
Рассмотрим графическую модель разделения смеси распределений (mixture model). Предполагается, что каждый элемент выборки $X = \{x_1, \ldots, x_n\}$ имеет скрытую компоненту из вектора $Z = \{z_1, \ldots, z_n\}$, обозначающую компоненту смеси, к которой относится элемент. Процесс генерации выборки устроен таким образом (см. Рис. \ref{Fig:DP.pdf}):
генерируется априорное дискретное распределение компонент смеси $(\pi_1, \ldots, \pi_K) \in \mathrm{Dir}(\alpha/K, \ldots, \alpha/K)$; в таком случае $\PR(z_i = m) = \pi_m$; $j$-я ($j \in \overline{1,K}$) компонента смеси имеет параметры $\theta_j$, которые сгенерированы из произвольного распределения $F(\lambda)$; $x_i$ генерируется из распределения $i$-й компоненты с параметрами $\theta_i$. 

Как правило, в таких задачах требуется оценить скрытые переменные $Z$, $\pi$, $\theta$ по известной выборке $X = \{x_1, \ldots, x_n\}$ и заданных значениях параметров $\alpha$, $\lambda$. 

\imgh{60mm}{DP.pdf}{Dirichlet Process Mixture Model.}

В случае, когда $K$ -- число компонент дискретного распределения не задано, вместо распределения Дирихле для конечного $K$ уместно воспользоваться процессом Дирихле DP, где $K = \infty$ (см. замечание).    

Пусть $\theta_1, \ldots, \theta_{n+1}$ -- независимые случайные величины, у которых распределение является случайной мерой $G \in \mathrm{DP}(H, \alpha)$, $A \subset \Theta$. Докажите, что
\[
\PR(\theta_{n+1} \in A | \theta_1, \ldots, \theta_n) = \frac{1}{n+\alpha}\left(\alpha H(A) + \sum \limits_{i=1}^{n} \delta_{\theta_i}(A) \right),
\]
Пусть $m$ -- число уникальных величин среди $\theta_1, \ldots, \theta_n$. Докажите следующие выражения:
\[
\Exp(m | n) = \alpha (\psi(n + \alpha)  - \psi(\alpha)) \simeq \alpha \ln \left(1 + \frac{n}{\alpha} \right) \quad \text{при} \; \alpha, n \gg 0,
\]
\[
\Var(m | n) = \alpha (\psi(n + \alpha)  - \psi(\alpha)) + \alpha^2 (\psi'(n + \alpha)  - \psi'(\alpha)) \simeq \]\[ \alpha \ln \left(1 + \frac{n}{\alpha} \right) \quad \text{при} \; n > \alpha \gg 0,
\]
где $\psi(x) = \frac{\text{Г'(x)}}{\text{Г(x)}}$ -- digamma функция.
\end{problem}

\begin{remark}
Пусть $H$ -- некоторое распределение с носителем $\Theta$, $\alpha$ -- положительное число. $G$, случайная вероятностная мера (распределение), называется   \textit{процессом Дирихле} $\mathrm{DP}(H, \alpha)$ с базовым распределением $H$ и параметром концентрации $\alpha$, если для любого конечного измеримого разбиения пространства элементарных исходов $\Theta$: $A_1, \ldots, A_r$ выполнено
\[
(G(A_1), \ldots, G(A_r)) \in \mathrm{Dir}(\alpha H(A_1), \ldots, \alpha H(A_r)).
\]

Процесс Дирихле обладает следующими свойствами:
\begin{enumerate}
\item $\forall A \subset \Theta$: $\Exp \left[G(A)\right] = H(A)$;
\item $\forall A \subset \Theta$: $\Var \left[G(A)\right] = \frac{H(A)(1-H(A))}{\alpha+1}$;
\item Пусть $\theta_1, \ldots, \theta_n$ -- независимые случайные величины со случайным распределением $G \in \mathrm{DP}(H, \alpha)$, тогда
\[
\PR(G(A_1), \ldots, G(A_r) | \theta_1, \ldots, \theta_n) \in \mathrm{Dir}(\alpha H(A_1) + n_1, \ldots, \alpha H(A_r) + n_r),
\]
где $n_j = | i: \theta_i \in A_j |$. 
Что также можно записать в другом виде:
\[
G | \theta_1, \ldots, \theta_n \in \mathrm{DP} \left(\alpha + n, \frac{\alpha}{\alpha+n} H + \frac{n}{\alpha+n} \cdot \frac{\sum \delta_{\theta_i}}{n} \right),
\]
где $\frac{\sum \delta_{\theta_i}}{n}$ -- смесь распределений $\delta_{\theta_1}, \ldots, \delta_{\theta_n}$, в котором $\PR(\theta = \theta_i) = 1$ при $\theta \in \delta_{\theta_i}$.   
\end{enumerate}

\end{remark}

\begin{problem}[Stick-breaking construction]
\label{stick}

Рассмотрим альтернативное определение процесса Дирихле (см. замечание к задаче \ref{DP}). Пусть $J_k$ -- $k$-й по счету скачек гамма процесса $g(s)$, $s \in [0,\alpha]$ (см. замечание к задаче \ref{po_dir}), нормированное значение скачка обозначим как $\pi_k = J_k/g(\alpha)$. Введем также последовательность с.в. $v_k: \Theta \to \Theta$ с вероятностной мерой $H: \mathcal{B}(\Theta) \to [0,1]$, где $\mathcal{B}(\Theta)$ -- все измеримые подмножества $\Theta$. Докажите, что случайная мера $G: \mathcal{B}(\Theta) \to [0,1]$, такая что
\[
\forall A \in \mathcal{B}(\Theta): \; G(A) = \sum_{k = 1}^{\infty} \pi_k \delta_{v_k}(A),
\]
является процессом Дирихле $DP(H, \alpha)$.

Для программной реализации процесса Дирихле, как правило, используется генеративная модель ``ломания палки'':
\[
\beta_k \in \mathrm{Beta}(\alpha, 1),
\quad
\pi_k = \beta_k \prod \limits_{l=1}^{k-1} (1-\beta_l), 
\quad
\theta_k \in F(\lambda) = H,
\]
\[
G \in \sum \limits_{k = 1}^{\infty} \pi_k \delta_{\theta_k}.
\]
Докажите, что $G \in \mathrm{DP}(H, \alpha)$.

\end{problem}

\begin{ordre}
См. статью T. Ferguson. A Bayesian analysis of some nonparametric problems. 1973. 
\end{ordre}

\begin{problem}
Для процесса Дирихле $G \in DP(H, \alpha)$ (см. альтернативное определение в задаче \ref{stick}) и измеримой функции $Z: \Theta \to \mathbb{R}$ докажите справедливость следующего утверждения. Если $\int_{\Theta} |Z(\theta)| dH (\theta) < \infty$, то с вероятностью единица 
$\int_{\Theta} |Z(\theta)| dG (\theta) < \infty$ и 
\[
\Exp \int_{\Theta} Z(\theta) dG (\theta)  = \int_{\Theta} Z(\theta) d \Exp G (\theta) =  \int_{\Theta} Z(\theta) dH (\theta).
\]
\end{problem}

\begin{problem}
\label{Fx_bayes}
Требуется по выборке $X_1,\ldots,X_n \in \mathbb{R}$ оценить функцию распределения $F_X(t)$. Априорно предполагается, что $F_X$ соответствует вероятностной мере $H: \mathcal{\mathbb{R}} \to [0,1]$. Предлагается оценивать функцию $F_X$ как 
\[
\widehat{F}_X(t) = \Exp G((-\infty, t) | X_1,\ldots,X_n),
\]
где $G((-\infty, t) | X_1,\ldots,X_n)$ -- с.в., соответствующая условной случайной мере $G | X_1,\ldots,X_n$ (см. замечание к задаче \ref{DP}).
Покажите, что
\[
\widehat{F}_X(t) = \frac{\alpha}{\alpha+n} H((-\infty, t)) + \frac{n}{\alpha+n}\frac{1}{n} \sum_{i=1}^{n} [X_i < t].
\]
Аналогично, для оценки $\Exp X$ предлагается использовать оценку
\[
\widehat{m}_X = \Exp \int_{-\infty}^{+\infty} x dG(x).
\]
Покажите, что
\[
\widehat{m}_X = \frac{\alpha}{\alpha+n} \Exp_{H} X + \frac{n}{\alpha+n}\frac{1}{n} \sum_{i=1}^{n} X_i.
\]

\end{problem}


\begin{problem}
Требуется по выборкам $X_1,\ldots,X_n \in \mathbb{R}$ и $Y_1,\ldots,Y_m \in \mathbb{R}$ оценить вероятность
\[
\bigtriangleup = \PR(X_1 < Y_1) = \int_{-\infty}^{+\infty} F_X(t)dF_Y(t).
\]
Априорно предполагается, что $F_X$ соответствует вероятностной мере $H_X: \mathcal{\mathbb{R}} \to [0,1]$, $F_Y$ -- вероятностной мере $H_Y$. Предлагается оценивать $\bigtriangleup$ как (см. задачу \ref{Fx_bayes}) 
\[
\widehat{\bigtriangleup} = \int_{-\infty}^{+\infty} \widehat{F}_X(t)d\widehat{F}_Y(t).
\]
Покажите, что
\[
\widehat{\bigtriangleup} = p_x p_y \bigtriangleup_0 + p_x(1-p_y)\frac{1}{m} \sum_{i=1}^m H_X(Y_i) + (1-p_x)p_y\frac{1}{n} \sum_{i=1}^n H_Y(X_i) + 
\]
\[
+(1-p_x)(1-p_y) \frac{1}{m n} \sum_{i,j} [X_i < Y_i],
\]
где
\[
p_x = \frac{\alpha_x}{\alpha_x+n},
\quad
p_y = \frac{\alpha_y}{\alpha_y+m},
\quad
\bigtriangleup_0 = \int_{-\infty}^{+\infty} H_X(t)dH_Y(t).
\]
\end{problem}

\begin{problem}[Вариационный вывод для DP]
Для поиска максимума функции правдоподобия $\log p(X | Z, \theta, \pi, \alpha, \lambda)$ в задаче \ref{DP} можно воспользоваться вариационным выводом (см. задачу \ref{varinf} из раздела \ref{CS}), введя более простую параметрическую модель распределения скрытых переменных $q(Z, \theta, \pi)$ и приближая $q(Z, \theta, \pi)$ к $p(Z, \theta, \pi| X, \alpha, \lambda)$ в смысле расстояния $\KL(q \Vert p)$.   

Приведем генеративную модель из задачи \ref{DP}, в которой в качестве процесса Дирихле используется Stick-breaking construction (см. задачу \ref{stick}), а также конкретный пример распределения $F(\lambda) = \mathrm{Dir}(\lambda_1,\ldots,\lambda_M)$: 
\[
\beta_k \in \mathrm{Beta}(\alpha, 1),
\;
\pi_k = \beta_k \prod \limits_{l=1}^{k-1} (1 - \beta_l), 
\;
z_i \in \mathrm{Cat}(\pi),
\;
\theta_k \in \mathrm{Dir}(\lambda_1,\ldots,\lambda_M),
\]
\[
\beta = \Vert \beta_k \Vert_{K \times 1}, \quad
Z = \Vert z_i \Vert_{n \times 1}, \quad
\theta = \Vert \theta_{km} \Vert_{K \times M}.
\]


В качестве примера распределения $x$ в каждой из компонент смеси расмматривается дискретное распределение с параметрами $\theta_k$, т.е.
\[
x_i|z_i \in \mathrm{Cat}(\theta_{z_i}) \quad 
\Leftrightarrow 
\quad
\PR(x_i = m | z_i = k) = \theta_{k m},  
\]
\[
m = \overline{1,M}, \quad k = \overline{1,K}, \quad
i = \overline{1,n}.
\]
Будем искать $q(Z, \beta, \theta)$ в следующем факторизованном виде
\[
q(Z, \beta, \theta) = \prod \limits_{k=1}^K q(\beta_k | \gamma_k) \prod \limits_{k=1}^K q(\theta_k | \tau_k) \prod \limits_{i=1}^n q(z_i | \phi_i),
\]
$\gamma_k$ -- Beta параметры для распределений с.в. $\beta_k$, $\tau_k$ -- параметры распределения $F$ аналогичные $\lambda$, $\phi_i$ -- параметры дискретного распределения с.в. $z_i$, $K \sim \alpha \ln n$. 
Покажите, что параметры $q$, соответствующие минимуму $\KL(q \Vert p)$, удовлетворяет следующей системе уравнений:
\[
\gamma_{k1} = \alpha + \sum_{i = 1}^n q(z_i = k), \quad
\gamma_{k2} = 1 + \sum_{i = 1}^n q(z_i > k), 
\]
\[
\tau_{km} = \lambda_m + \sum_{i = 1}^n [x_i = m] q(z_i = k),
\]
\[
\phi_{ik} \propto \exp \left( \Exp_q \log (\beta_k) + \sum_{l < k} \Exp_q \log (1 - \beta_l) + \sum_{i = 1}^n \Exp_q \log (\theta_{kx_i}) 
\right),
\]
где
$$
q(z_i = k) = \phi_{i,k}, \quad q(z_i > k) = \sum \limits_{j = k+1}^K \phi_{i,j},
$$
$$
\Exp_q \log (\beta_k) = \psi(\gamma_{k1}) - \psi(\gamma_{k1} + \gamma_{k2}), 
$$
$$
\Exp_q \log (1- \beta_k) = \psi(\gamma_{k2}) - \psi(\gamma_{k1} + \gamma_{k2}), 
$$
$$
\Exp_q \log (\theta_{km}) = \psi(\tau_{km}) - \psi\left(\sum_{j}\tau_{kj}\right). 
$$

\end{problem}






\begin{problem}[Тематическое моделирование]
\label{sec:them_modeling}
Вероятностная тематическая модель представляет собой средство для выявления тем в коллекции текстовых документов, описывая каждую тему $z \in Z$ дискретным распределением на множестве терминов $W$, a каждый документ $d \in D$  — дискретным распределением на множестве тем. Предполагается, что порядок терминов в документах не важен, и коллекция является  выборкой из дискретного распределения $\PR(d, w, z)$, где  $\Omega = D \times W \times Z$,  $z$ -- скрытая переменная темы термина $w$ в документе $d$. Коллекция представляется матрицей частот $F  
 = \Vert \widehat{p}_{wd} \Vert_{W\times D}$. Предполагается также, что
\[
\PR(d, w, z) = \PR(w|d,z)\PR(z|d)\PR(d) = \PR(w|z)\PR(z|d)  \PR(d) 
= \phi_{wz} \theta_{zd} \; \PR(d), 
\]  
\[
 \PR(d, w) = \PR(d) \sum_z \phi_{wz} \theta_{zd} . 
\]
Обозначим за $n_{dw}$ число слов $w$ в документе $d$.
Поиск максимума правдоподобия модели для заданной выборки $X = \{(d,w, n_{dw}): d \in D, w \in W\}$  
\[
 L(\Phi, \Theta) = \log  \prod_{d,w} \bigg[\PR(d, w)\bigg]^{n_{dw}}
\] 
 равносилен минимизации расстояния $\mathcal{KL}(F \Vert \Phi \Theta)$, где стохастическое матричное разложение $\Phi \Theta$ не единственно и определено с точностью до невырожденного преобразования $\Phi S^{-1}  S \Theta$.
Ввиду наличия неопределенности в выборе решения, наряду с правдоподобием имеет смысл максимизировать одновременно набор критериев $R_i(\Phi, \Theta)$, выражающих априорные предположения на счет $\phi_z$ и $\theta_d$ как столбцов матриц $\Phi, \Theta^T$ и называемых \textit{регуляризаторами}. Предлагается в качестве решения задачи многокритериальной  оптимизации максимизировать
\[
L(\Phi, \Theta) + R(\Phi, \Theta) = L(\Phi, \Theta) + \sum_i \tau_i R_i(\Phi, \Theta).
\]
Покажите, что шагами EM-алгоритма (см. задачу \ref{em} в разделе \ref{CS}) для решения данной задачи будут

Expectation step: вычислить распределение по темам для каждого термина в документе  
\[
q_{dw}(z) = \frac{\phi_{wz} \theta_{zd}}{\sum_s \phi_{ws} \theta_{sd}};
\]
Maximization step:  найти оптимальные  $\Phi, \Theta$

\[
\phi_{wz} \propto \left( 
\sum_d n_{dw} q_{dw}(z)  + \phi_{wz}\frac{\partial R}{\partial \phi_{wz}}
\right)_{+}, \;
\theta_{zd} \propto \left( 
\sum_w n_{dw} q_{dw}(z)  + \theta_{zd}\frac{\partial R}{\partial \theta_{zd}}
\right)_{+}.
\]
Докажите, что регуляризатор вида
\[
R(\Theta) = - \sum_{d=1}^{D}\sum_{k=1}^{K} \alpha_k \log(\theta_{dk})
\]
соответствует следующему априорному предположению на счет распределения $\theta_d$
\[
\theta_d \in \mathrm{Dir}(\alpha_1 + 1,\ldots,\alpha_K + 1),
\quad
d \in \overline{1,D}, \; K = |T|.
\]
При каком значении $\alpha_1,\ldots,\alpha_K$ данный регуляризатор будет способствовать увеличению количества нулевых компонент $\theta_d$?
\end{problem}

\begin{problem}[PAC-Байесовские неравенства]
Ознакомьтесь с постановкой задачи классификации. 
Для правила (алгоритма) классификации $h \in H$ и набора классифицируемых объектов $X_1, \ldots, X_n$ пусть выборка $X_1^h, \ldots, X_n^h$ есть значения функции потерь  с $X_i^h \in [0,1]$ и $\Exp[X_1^h] = \mu^h$, $\Var[X_1^h]=(\sigma^h)^2$. 

Воспользовавшись неравенством Хефдинга и указанием к задаче, докажите следующее утверждение.
Для любого не зависящего от выборок распределения $p$ на $H$ ($h \sim p$), $\lambda \geq 0$, $\delta \in (0, 1)$ и произвольного априорного распределения $q$ на $H$ с вероятностью не меньше $1 - \delta$  справедливо:
\[\tag{1}
\Exp_q \left( 
\frac{1}{n} \sum_{i=1}^n X_i^h - \mu^h
\right) \leq
\frac{\mathcal{KL} (q \Vert p) + \ln \frac{1}{\delta}  }{n \lambda} + \frac{\lambda}{8}.
 \]
Воспользовавшись неравенством Бернштейна, докажите утверждение, аналогичное предыдущему.
 \[\tag{2}
\Exp_q \left( 
\frac{1}{n} \sum_{i=1}^n X_i^h - \mu^h
\right) \leq
\frac{\mathcal{KL} (q \Vert p) + \ln \frac{1}{\delta}  }{n \lambda} + \lambda (e-2) \Exp_p (\sigma^h)^2.
 \]


\end{problem}

\begin{ordre}
Докажите следующие утверждения.

Для любого измеримого отображения $f : H \to \mathbb{R}$ и любых пар распределений $q$ и $p$ на $H$ справедливо:
\[
\Exp_q f(h) \leq  \mathcal{KL} (q \Vert p)  + \ln \left( \Exp_p e^{f(h)} \right).
\]

Случайная величина $X$ определена на множестве $D$.  Рассмотрим c.в.  $h \sim p$, $h \in H$, не зависящую от $X$,   измеримое отображения $f : H \times D \to \mathbb{R}$. Тогда $\forall \delta \in (0, 1)$ и произвольного распределения $q$ на $H$   с вероятностью не меньше $1 - \delta$ справедливо следующее:
\[
\Exp_q f(h, X) \leq \mathcal{KL} (q \Vert p) + \ln \frac{1}{\delta} + \ln \left( \Exp_p \Exp_X e^{f(h,X)} \right).
\]

\end{ordre}

\begin{remark}
Минимизация по $\lambda$, фигурирующем в выражениях (1) и (2), приводит к зависимости вида $\lambda = \lambda(q)$. Для получения оценки, выполняющейся одновременно для всех $q$, рассмотрим конечный набор значений $\lambda_i$, и для каждого $q$ будем брать $i$, соответствующее минимуму в выражениях (1) и (2). Т.о., если определить $\lambda_i$ как $c^i \lambda_{\min}$, где $\lambda_{\min}$ соответствует $\mathcal{KL} (q \Vert p) = 0$, то получим следующие \textit{PAC-Байесовские неравенства} Хефдинга и Бернштейна.
\[
\Exp_q \left( 
\frac{1}{n} \sum_{i=1}^n X_i^h - \mu^h
\right) \leq
\frac{1+c}{2}
\sqrt{
\frac{\mathcal{KL} (q \Vert p) + \ln \frac{1}{\delta} + \varepsilon(q) }{2 n} 
},
\]
\[
\varepsilon(q) = \frac{\ln 2}{2 \ln c} \left(
1 + \frac{\mathcal{KL} (q \Vert p) }{\ln (1/\delta) }
\right);
\]
при условии
$\mathcal{KL} (q \Vert p) \leq n  (e-2) \Exp_p (\sigma^h)^2  -  \ln \frac{\nu}{\delta}$ выполнено неравенство
\[
\Exp_q \left( 
\frac{1}{n} \sum_{i=1}^n X_i^h - \mu^h
\right) \leq
(1 + c)
\sqrt{
\frac{ (e-2) \Exp_p (\sigma^h)^2 (\mathcal{KL} (q \Vert p) + \ln \frac{\nu}{\delta})  }{n} 
},
 \]
 \[
 \nu  = \left\lceil \frac{1}{\ln c} \ln \left( \frac{(e-2)n}{4\ln(1/\delta)} \right) \right\rceil + 1.
 \]   
   
Данные неравенства позволяют с хорошей точностью оценить отличие среднего риска от эмпирического риска, по сравнению с $VC$-оцениванием. Оптимизация  по $q$ позволяет настраивать гиперпараметры алгоритма классификации.     
\end{remark}



